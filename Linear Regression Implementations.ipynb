{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear Regression Implementations.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMOAKTfggtRmO5T3s+Razgv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7rbt6qO5tBQ2"},"source":["## Linear regression implementations"]},{"cell_type":"markdown","source":["This project aims to discuss linear models for regression tasks, where the outcome variable $y$ that should be predicted is continuous. Linear models refer to the linearity of parameters, in such a way that parameters and variables relates in a linear form, $y = f(\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$). Note that linear models do not imply in a linear relationship between $y$ and $X$.\n","\n","The classic approach to regression is the linear model fitted using **Ordinary Least Squares (OLS)**. Since the *linear regression model* defines values for coefficients of the hyperplane that is the closest to the training data, this can be translated into the following optimization problem:\n","\\begin{equation}\n","  \\displaystyle \\min_{b} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 = (y_i - xb)^2\n","\\end{equation}\n","\n","\\begin{equation}\n","  \\displaystyle \\min_{b} \\sum_{i=1}^N (y_i - (b_0 - b_1x_1 - b_2x_2 - ... - b_px_p))^2\n","\\end{equation}\n","\n","Where $x$ is $1xp$ and $b$ is $px1$. Supposing that $(X^TX)$ is non-singular, where $X$ is the training data $Nxp$, then the coefficients that minimize the sum of squared residuals above (and, then, a cost function as the mean squared error) are given by:\n","\\begin{equation}\n","  \\displaystyle \\hat{\\beta} = (X^TX)^{-1}X^Ty\n","\\end{equation}\n","Where $y$ $Nx1$ is the vector of outputs. Consequently, this algorithm to fit a linear regression model is called \"ordinary least squares\". Despite of its robustness, it can become very slow when the number of input variables $p$ gets too large.\n","\n","**Batch gradient descent** is an optimization algorithm that fits well to define values of parameters in the context of linear regression models. Differently from OLS, which has a closed-form solution for defining coefficients, gradient descent is an algorithm with iterations through which a candidate solution should converge to a definite solution. As with any other standard optimization algorithm, a random initialization defines first values for the coefficients $\\hat{\\beta}^0$.\n","\n","Its inner functioning considers that the best direction towards which the candidate solution should move along the cost function in order to minimize it is given by the negative of the *gradient* of the cost function, $-\\nabla_{\\beta} MSE(\\beta)$. This gradient consists of a vector $(p-1)x1$ of partial derivatives of the cost function with respect to each of the coefficients in the linear model (except from the intercept), $\\partial MSE(\\beta)/\\partial \\beta_j$. Consequently, given an iteration $\\hat{\\beta}^m$, the *update rule* follows:\n","\\begin{equation}\n","  \\displaystyle \\hat{\\beta}^{m+1} = \\hat{\\beta}^m - \\eta\\nabla_{\\beta}MSE(\\beta)\n","\\end{equation}\n","\n","Where $\\eta$ is the learning rate. Note that the gradient as defined above depends on the entire training data, since the derivatives of $MSE$ are then calculated using all training instances and averaging among them. An alternative that requires a smaller amount of data is **mini-batch (or stochastic) gradient descent**, which calculates derivatives using only a random subset of size $S$ of the training instances during each iteration.\n","\n","Above it was mentioned that linearity of these models only refers to the way how variables relate with their coefficients. So, one can take any *polynomial expansion* of the vector $x$ and use their values as new inputs to the model, which is still linear, but capturing non-linear relationships between $y$ and $X$. Since such **polynomial regression** has terms ($x_j, x_j^2, ..., x_jx_k, ...$) with very different scales, it is necessary to standard scale all inputs previous to fitting the model. Finally, model estimation makes use of any algorithm (OLS, gradient descent, and so on) to define parameters values, and also can apply regularization or not.\n","\n","When a large amount of features is available, which can even be a consequence of polynomial expansions, overfitting is likely to happen. In linear models, the most immediate way to handle this is the implementation of regularization by the constraint of the size of parameters. **Regularized regression** modifies the cost function by adding a *penalty term* to it, and there are two main alternatives of restriction over the size of parameters, both following the norm of the vector of coefficients $\\beta$: **L2 regularization**, which leads to *ridge regression*, and **L1 regularization**, which implies in *lasso regression*. The penalty term of L2 regularization is given by:\n","\\begin{equation}\n","  \\displaystyle L2 =  \\alpha\\frac{1}{2}\\sum_{j=1}^p\\beta_j^2\n","\\end{equation}\n","\n","While the penalty term of L1 regularization follows:\n","\\begin{equation}\n","  \\displaystyle L1 =  \\alpha\\sum_{j=1}^p|\\beta_j|\n","\\end{equation}\n","\n","Where $\\alpha$ is the regularization parameter that controls the amount of shrinkage imposed to the coefficients of the linear model. The main modification to the fitting procedure with regularization is an additional term to the gradient of the cost function. Besides of ridge and lasso regression, **Elastic Net** mixes L1 and L2 regularization:\n","\\begin{equation}\n","  \\displaystyle ElasticNet = r\\alpha\\sum_{j=1}^p|\\beta_j| + \\frac{1-r}{2}\\alpha\\sum_{j=1}^p\\beta_j^2\n","\\end{equation}\n","\n","When it comes to choosing among L1, L2 and Elastic Net, the learning task will dictate which is the best option. Even so, in sparse features spaces, when only a few variables are relevant, lasso or Elastic Net regression are preferable. Finally, regularization also applies for linear models constructed for classification tasks. In a similar, way the cost function should be extended to include a penalty term.\n","\n","This [article](https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076) gives all codes found here for implementing linear regression models. Its author developed 10 Python classes covering standard linear regression, polynomial transformation, standard scaling, and ridge, lasso, and elastic net regression.\n","\n","The first two classes to be mentioned are **PolynomialFeatures** and **StandardScaler**. In order to create polynomial expansions, the first class defines all products of input variables up to some predefined degree, which involves defining the maximum exponent $t$ ($x_j^t$ for $j \\in \\{1, 2, ..., p\\}$) of the polynomial expansion. For correcting the scale of variables, the second class calculates means and variances of all variables, and then normalizes all values of inputs.\n","\n","The most important class is **Regression**, which is the base upon which classes for linear regression (with no regularization), and ridge, lasso and, elastic net regression are created. This class has a *fit* method that iterates the batch gradient descent algorithm (coefficients, predictions, gradients, updates). The *predict* method simply implements the dot product between the input matrix and the vector of coefficients. The **LinearRegression** class inherits from Regression, but the *fit* method implements either OLS or batch gradient descent (available in the Regression class), depending the alternative chosen during the initialziation of LinearRegression. Since the Regression class works with attributes referring to regularization, LinearRegression defines functions for the regularization amount and the gradient of regularization term that return 0 for all coefficients values.\n","\n","The classes **l1_regularization**, **l2_regularization** and **l1_l2_regularization** differ one from the other with respect to: i) how the regularization is calculated when the class is called in addition to coefficients values; and ii) how the gradient of the regularization term is calculated also as a function of coefficients values. These classes are used, respectively, during the initialization of **Ridge**, **Lasso** and **ElasticNet** classes, which all inherit from Regression, to declare the regularization attribute."],"metadata":{"id":"HcIyXKV8dC3I"}},{"cell_type":"markdown","metadata":{"id":"tWi-Yyretplg"},"source":["**References**\n","<br>\n","[ML From Scratch: Linear, Polynomial, and Regularized Regression Models](https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076)."]},{"cell_type":"markdown","metadata":{"id":"YjtnwOcFtdtn"},"source":["----------------"]},{"cell_type":"markdown","metadata":{"id":"-xp0Wq1nAb73"},"source":["This notebook first imports all relevant libraries, and then presents implementations of linear regression estimation with OLS and gradient descent, besides of polynomial regression, ridge and lasso regularized models. All implementations follow from this [article](https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076) ([Github](https://github.com/lukenew2/mlscratch) page of reference)."]},{"cell_type":"markdown","metadata":{"id":"1jKqXHmYthkK"},"source":["**Summary:**\n","1. [Libraries](#libraries)<a href='#libraries'></a>.\n","2. [Ordinary least squares (OLS)](#ols)<a href='#ols'></a>.\n","3. [Batch gradient descent](#gradient_descent)<a href='#gradient_descent'></a>.\n","4. [Polynomial regression](#polynomial_regression)<a href='#polynomial_regression'></a>.\n","  * [Polynomial transformation](#polynomial_transf)<a href='#polynomial_transf'></a>.\n","  * [Data normalization](#data_normalization)<a href='#data_normalization'></a>.\n","\n","5. [Regularized models](#regularized_models)<a href='#regularized_models'></a>.\n","  * [Ridge regression](#ridge_regression)<a href='#ridge_regression'></a>.\n","  * [Lasso regression](#lasso_regression)<a href='#lasso_regression'></a>.\n","  * [Elastic Net regression](#elastic_net)<a href='#elastic_net'></a>."]},{"cell_type":"markdown","metadata":{"id":"NPwnjKEUtk3U"},"source":["<a id='libraries'></a>"]},{"cell_type":"markdown","metadata":{"id":"Kd3Ii0l1tmT1"},"source":["## Libraries"]},{"cell_type":"code","metadata":{"id":"-tNt5lArtoMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639247169665,"user_tz":180,"elapsed":23606,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"2f4a7b1a-0016-40ef-f6ce-b19f85980539"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bx0RX4AsUtKe","executionInfo":{"status":"ok","timestamp":1639247175246,"user_tz":180,"elapsed":659,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"fb909714-f17e-4ba9-855e-a144efdc502c"},"source":["cd \"/content/gdrive/MyDrive/Studies/tree_based/Codes\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Studies/tree_based/Codes\n"]}]},{"cell_type":"code","metadata":{"id":"xOnfCCeLtr2A"},"source":["import numpy as np\n","from itertools import combinations_with_replacement\n","from scipy.linalg import lstsq\n","from scipy.special import factorial\n","\n","# pip install mlscratch==0.0.1\n","from mlscratch.utils.metrics import mean_squared_error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojy3GLAotqii"},"source":["<a id='ols'></a>"]},{"cell_type":"markdown","metadata":{"id":"bg_iUVd-twyu"},"source":["## Ordinary least squares (OLS)"]},{"cell_type":"code","source":["class LinearRegression():\n","    \"\"\"\n","    Class representing a linear regression model.\n","    Models relationship between target variable and attributes by computing \n","    line that minimizes mean squared error.\n","    Parameters\n","    ----------   \n","    solver : {'lstsq'}, \n","        Optimization method used to minimize mean squared error in training.\n","        'lstsq' : \n","            Ordinary lease squares method using scipy.linalg.lstsq.\n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    Notes\n","    -----\n","    This class is capable of being trained using ordinary least squares method.\n","    See solver parameter above.\n","    \"\"\"\n","    def __init__(self, solver='lstsq'):\n","        self.solver = solver \n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit linear regression model.\n"," \n","        If solver='lstsq' model is trained using ordinary least squares.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data. Independent variables.\n","        y : array-like of shape (n_samples, 1)\n","            Target values. Dependent variable.\n","        Returns\n","        -------\n","        self : returns an instance of self.\n","        \"\"\"\n","        # If solver is 'lstsq' use ordinary least squares optimization method.\n","        if self.solver == 'lstsq':\n","            # Make sure inputs are numpy arrays.\n","            X = np.array(X)\n","            y = np.array(y)\n","            # Add x_0 = 1 to each instance for the bias term.\n","            X = np.c_[np.ones((X.shape[0], 1)), X]\n","            # Scipy implementation of least squares.\n","            self.coef_, residues, rank, singular = lstsq(X, y)\n","\n","            return self\n","          \n","    def predict(self, X):\n","        \"\"\"\n","        Estimate target values using the linear model.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Instances.\n","        Returns\n","        -------\n","        C : array of shape (n_samples, 1)\n","            Estimated targets per instance.\n","        \"\"\"\n","        # Make sure inputs are numpy arrays.\n","        X = np.array(X)\n","        # Add x_0 = 1 to each instance for the bias term.\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","\n","        return X.dot(self.coef_)"],"metadata":{"id":"rZrtMsTI50O0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='gradient_descent'></a>"],"metadata":{"id":"Gpk7HkNC1N6I"}},{"cell_type":"markdown","source":["## Batch gradient descent"],"metadata":{"id":"29bRN84j1Vaw"}},{"cell_type":"code","source":["class Regression():\n","    \"\"\"\n","    Class representing our base regression model.  \n","    \n","    Models relationship between a dependant scaler variable y and independent\n","    variables X by optimizing a cost function with batch gradient descent.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-1\n","        Learning rate determining the size of steps in batch gradient descent.\n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-1):\n","        self.n_iter = n_iter \n","        self.lr = lr \n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit linear model with batch gradient descent.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data. Independent variables.\n","        y : array-like of shape (n_samples, 1)\n","            Target values. Dependent variable.\n","        Returns\n","        -------\n","        self : returns an instance of self.\n","        \"\"\"\n","        # Make sure inputs are numpy arrays.\n","        X = np.array(X)\n","        y = np.array(y)\n","        # Add x_0 = 1 to each instance for the bias term.\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","        # Store number of samples and features in variables.\n","        n_samples, n_features = np.shape(X)\n","        self.training_errors = []\n","        # Initialize weights randomly from normal distribution.\n","        self.coef_ = np.random.randn(n_features, 1)\n","        # Batch gradient descent for number iterations = n_iter.\n","        for _ in range(self.n_iter):\n","            y_preds = X.dot(self.coef_)\n","            # Calculate mse.\n","            cost_function = mean_squared_error(y, y_preds) \n","            self.training_errors.append(cost_function) \n","            # Gradients of loss function.\n","            gradients = (2/n_samples) * X.T.dot(y_preds - y)\n","            gradients = gradients\n","            # Update the weights.\n","            self.coef_ -= self.lr * gradients \n","\n","        return self \n","\n","    def predict(self, X):\n","        \"\"\"\n","        Estimate target values using the linear model.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Instances.\n","        Returns\n","        -------\n","        C : array of shape (n_samples, 1)\n","            Estimated targets per instance.\n","        \"\"\"\n","        # Make sure inputs are numpy arrays.\n","        X = np.array(X)\n","        # Add x_0 = 1 to each instance for the bias term.\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","\n","        return X.dot(self.coef_)\n","  \n"," \n","class LinearRegression():\n","    \"\"\"\n","    Class representing a linear regression model.\n","    Models relationship between target variable and attributes by computing \n","    line that minimizes mean squared error.\n","    Parameters\n","    ----------   \n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-1\n","        Learning rate determining the size of steps in batch gradient descent.\n","    solver : {'bgd', 'lstsq'}, default='bgd'\n","        Optimization method used to minimize mean squared error in training.\n","        \n","        'bgd' : \n","            Batch gradient descent.\n","            \n","        'lstsq' :         \n","            Ordinary lease squares method using scipy.linalg.lstsq.\n","            \n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","        \n","    Notes\n","    -----\n","    This class is capable of being trained using ordinary least squares method\n","    or batch gradient descent.  See solver parameter above.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=e-1, solver='bgd'):\n","        self.solver = solver \n","        super(LinearRegression, self).__init__(n_iter=n_iter, lr=lr)\n","        \n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit linear regression model.\n"," \n","        If solver='lstsq' model is trained using ordinary least squares.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data. Independent variables.\n","        y : array-like of shape (n_samples, 1)\n","            Target values. Dependent variable.\n","            \n","        Returns\n","        -------\n","        self : returns an instance of self.\n","        \"\"\"\n","        # If solver is 'lstsq' use ordinary least squares optimization method.\n","        if self.solver == 'lstsq':\n","            # Make sure inputs are numpy arrays.\n","            X = np.array(X)\n","            y = np.array(y)\n","            # Add x_0 = 1 to each instance for the bias term.\n","            X = np.c_[np.ones((X.shape[0], 1)), X]\n","            # Scipy implementation of least squares.\n","            self.coef_, residues, rank, singular = lstsq(X, y)\n","            \n","            return self\n","        # If solver is 'bgd' use batch gradient descent.\n","        elif self.solver == 'bgd':\n","            super(LinearRegression, self).fit(X, y)"],"metadata":{"id":"Udl5qym11C-1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='polynomial_regression'></a>"],"metadata":{"id":"WFo6s8Ye2CHl"}},{"cell_type":"markdown","source":["## Polynomial regression"],"metadata":{"id":"YA0a7x7l2I1N"}},{"cell_type":"markdown","source":["<a id='polynomial_transf'></a>"],"metadata":{"id":"RZnsSLoy3DUi"}},{"cell_type":"markdown","source":["### Polynomial transformation"],"metadata":{"id":"_qZLZuN13ABw"}},{"cell_type":"code","source":["class PolynomialFeatures:\n","    \"\"\"\n","    Generate polynomial features. \n","    Generate a new matrix of features including all combinations of different\n","    polynomial features less than or equal to the specified degree. No bias \n","    term is calculated in this implementation because our regression models\n","    create a bias term when they are trained.\n","    Parameters\n","    ----------\n","    degree : int, default=2\n","        Degree of polynomial features to be created.\n","    Attributes\n","    ----------\n","    n_input_features : int \n","        The total number of input features.\n","    n_output_features : int\n","        The total number of output features computed by iterating over all\n","        possible polynomial combinations of input features.\n","    \"\"\"\n","    def __init__(self, degree=2):\n","        self.degree = degree\n","\n","    def fit(self, X):\n","        \"\"\"\n","        Compute the number of output features.\n","            n_output_features = (n+d)!/d!n!\n","        where n is the number of input features and d is the degree.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Feature matrix to be transformed into polynomial feature matrix.\n","        \"\"\"\n","        # Make sure input is numpy array.\n","        X = np.array(X)\n","        # Store number of input features in attribute.\n","        self.n_input_features = X.shape[1]\n","        # Calculate numerator and denominator of equation listed above.\n","        numerator = factorial(self.n_input_features + self.degree)\n","        denominator = factorial(self.degree) * factorial(self.n_input_features)\n","        # Calculate number of output features minus 1 to subtract bias term.\n","        self.n_output_features = int(numerator / denominator) - 1\n","\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transform data to polynomial feature matrix.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Feature matrix to be transformed into polynomial feature matrix.\n","        Returns\n","        -------\n","        X : array-like of shape (n_samples, n_output_features)\n","            Tranformed polynomial feature matrix where n_output_features is\n","            the number of output features after polynomial transformation.\n","        \"\"\"\n","        # Generate all combination of feature indices and store them in tuples.\n","        combos = [combinations_with_replacement(range(self.n_input_features),i)\n","                  for i in range(1, self.degree + 1)]\n","        # Create list of tuples containing feature index combinations.\n","        combinations = [item for sublist in combos for item in sublist]\n","        # Create new array of the desired output shape.\n","        X_new = np.empty((X.shape[0], self.n_output_features))\n","        # Multiply features for each combination tuple in combinations.\n","        for i, index_combos in enumerate(combinations):\n","            X_new[:, i] = np.prod(X[:, index_combos], axis=1)\n","\n","        return X_new\n","\n","    def fit_transform(self, X):\n","        \"\"\"\n","        Compute the number of output features and transform data to polynomial\n","        feature matrix.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Feature matrix to be transformed into polynomial feature matrix.\n","        Returns \n","        -------\n","        X : array-like of shape (n_samples, n_output_features)\n","            Transformed polynomial feature matrix where n_output_features is \n","            the number of output features after polynomial transformation.\n","        \"\"\"\n","        self.fit(X)\n","\n","        return self.transform(X)"],"metadata":{"id":"1Mp-3C513F9K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='data_normalization'></a>"],"metadata":{"id":"sHXK6NZX3Mkv"}},{"cell_type":"markdown","source":["### Data normalization"],"metadata":{"id":"vBKqPDJF3Y0J"}},{"cell_type":"code","source":["class StandardScaler:\n","    \"\"\"\n","    Standardize features by centering the mean to 0 and unit variance.\n","    The standard score of an instance is calculated by:\n","        z = (x - u) / s\n","    where u is the mean of the training data and s is the standard deviation.\n","    Standardizing data is often necessary before training many machine\n","    learning models to avoid problems like exploding/vanishing gradients and\n","    feature dominance.\n","    Attributes\n","    ----------\n","    mean_ : numpy array of shape (n_features, )\n","        The mean of each feature in the training set.\n","    var_ : numpy array of shape (n_features, )\n","        The variance of each feature in the training set.\n","    \"\"\"\n","    def fit(self, X):\n","        \"\"\"\n","        Calculate and store the mean and variance of each feature in the\n","        training set.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Data set to calculate mean and variance of.\n","        \"\"\"\n","        self.mean_ = np.mean(X, axis=0)\n","        self.var_ = np.var(X, axis=0)\n","\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Standardize data by subtracting out the mean and dividing by\n","        standard deviation calculated during fitting.\n","        Parameters \n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Data to be standardized\n","        Returns \n","        -------\n","        X_std : array-like of shape(n_samples, n_featuers)\n","            Standardized data.\n","        \"\"\"\n","        X_std = (X - self.mean_) / np.sqrt(self.var_)\n","\n","        return X_std\n","\n","    def inverse_transform(self, X_std):\n","        \"\"\"\n","        Transform data back into orginal state by multiplying by standard\n","        deviation and adding the mean back in.\n","        Inverse standard scaler:\n","            x = z * s + u\n","        where s is the standard deviation, and u is the mean.\n","        Parameters \n","        ----------\n","        X_std : array-like of shape (n_samples, n_features)\n","            Standardized data to convert into original state.\n","        Returns\n","        -------\n","        X : array-like of shape (n_samples, n_features)\n","            Transformed data.\n","        \"\"\"\n","        X = X_std * np.sqrt(self.var_) + self.mean_\n","\n","\n","        return X"],"metadata":{"id":"oyQKQP5o3acR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='regularized_models'></a>"],"metadata":{"id":"h2fRQJ-h3kBp"}},{"cell_type":"markdown","source":["## Regularized models"],"metadata":{"id":"WmRyQtda3qPb"}},{"cell_type":"markdown","source":["<a id='ridge_regression'></a>"],"metadata":{"id":"mdfDJeyD31Mn"}},{"cell_type":"markdown","source":["### Ridge regression"],"metadata":{"id":"jB3VPcRe3621"}},{"cell_type":"markdown","source":["#### L2 regularization"],"metadata":{"id":"di9lskwC4FsC"}},{"cell_type":"code","source":["class l2_regularization():\n","    \"\"\"\n","    Add l2 regularization penalty to linear models.\n","    Regularization term:\n","        alpha * 1/2 * ||w||^2\n","    Where w is the vector of feature weights and alpha is the hyperparameter\n","    controlling how much regularization is done to the model.\n","    Parameters\n","    ----------\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    Notes\n","    -----\n","    The bias term is not regularized and therefore should be omitted from the\n","    feature weights as input.  \n","    \"\"\"\n","    def __init__(self, alpha=1.0):\n","        self.alpha = alpha \n","\n","    def __call__(self, w):\n","        \"\"\"Calculate regularization term.\"\"\"\n","        return self.alpha * 0.5 * np.linalg.norm(w, 2)\n","    \n","    def grad(self, w):\n","        \"\"\"\n","        Calculate gradient descent regularization term.\n","            alpha * w\n","        where alpha is the factor determining the amount of regularization and\n","        w is the vector of feature weights.  \n","        \"\"\"\n","        gradient_penalty = np.asarray(self.alpha) * w\n","        # Insert 0 for bias term.\n","        return np.insert(gradient_penalty, 0, 0, axis=0)"],"metadata":{"id":"IdQ8xxsX3xUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Ridge regression"],"metadata":{"id":"EhXU7NmX4IHO"}},{"cell_type":"code","source":["class Regression():\n","    \"\"\"\n","    Class representing our base regression model.  \n","    \n","    Models relationship between a dependant scaler variable y and independent\n","    variables X by optimizing a cost function with batch gradient descent.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-1\n","        Learning rate determining the size of steps in batch gradient descent.\n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-1):\n","        self.n_iter = n_iter \n","        self.lr = lr \n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit linear model with batch gradient descent.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data. Independent variables.\n","        y : array-like of shape (n_samples, 1)\n","            Target values. Dependent variable.\n","        Returns\n","        -------\n","        self : returns an instance of self.\n","        \"\"\"\n","        # Make sure inputs are numpy arrays.\n","        X = np.array(X)\n","        y = np.array(y)\n","        # Add x_0 = 1 to each instance for the bias term.\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","        # Store number of samples and features in variables.\n","        n_samples, n_features = np.shape(X)\n","        self.training_errors = []\n","        # Initialize weights randomly from normal distribution.\n","        self.coef_ = np.random.randn(n_features, 1)\n","        # Batch gradient descent for number iterations = n_iter.\n","        for _ in range(self.n_iter):\n","            y_preds = X.dot(self.coef_)\n","            # Penalty term if regularized (don't include bias term).\n","            regularization = self.regularization(self.coef_[1:])\n","            # Calculate mse + penalty term if regularized.\n","            cost_function = mean_squared_error(y, y_preds) + regularization\n","            self.training_errors.append(cost_function) \n","            # Regularization term of gradients (don't include bias term).\n","            gradient_reg = self.regularization.grad(self.coef_[1:])\n","            # Gradients of loss function.\n","            gradients = (2/n_samples) * X.T.dot(y_preds - y)\n","            gradients = gradients + gradient_reg\n","            # Update the weights.\n","            self.coef_ -= self.lr * gradients \n","\n","        return self \n","\n","    def predict(self, X):\n","        \"\"\"\n","        Estimate target values using the linear model.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Instances.\n","        Returns\n","        -------\n","        C : array of shape (n_samples, 1)\n","            Estimated targets per instance.\n","        \"\"\"\n","        # Make sure inputs are numpy arrays.\n","        X = np.array(X)\n","        # Add x_0 = 1 to each instance for the bias term.\n","        X = np.c_[np.ones((X.shape[0], 1)), X]\n","\n","        return X.dot(self.coef_)\n","\n","\n","class LinearRegression(Regression):\n","    \"\"\"\n","    Class representing a linear regression model.\n","    Models relationship between target variable and attributes by computing \n","    line that minimizes mean squared error.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-1\n","        Learning rate determining the size of steps in batch gradient descent.     \n","    solver : {'bgd', 'lstsq'}, default=\"bgd\"\n","        Optimization method used to minimize mean squared error in training.\n","        'bgd' : \n","            Batch gradient descent.\n","        'lstsq' : \n","            Ordinary lease squares method using scipy.linalg.lstsq.\n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    Notes\n","    -----\n","    This class is capable of being trained using ordinary least squares method\n","    or batch gradient descent.  See solver parameter above.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-1, solver='bgd'):\n","        self.solver = solver \n","        # No regularization.\n","        self.regularization = lambda x: 0\n","        self.regularization.grad = lambda x: 0\n","        super(LinearRegression, self).__init__(n_iter=n_iter, lr=lr)\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit linear regression model.\n","        If solver='bgd', model is trained using batch gradient descent. \n","        If solver='lstsq' model is trained using ordinary least squares.\n","        Parameters\n","        ----------\n","        X : array-like of shape (n_samples, n_features)\n","            Training data. Independent variables.\n","        y : array-like of shape (n_samples, 1)\n","            Target values. Dependent variable.\n","        Returns\n","        -------\n","        self : returns an instance of self.\n","        \"\"\"\n","        # If solver is 'lstsq' use ordinary least squares optimization method.\n","        if self.solver == 'lstsq':\n","            # Make sure inputs are numpy arrays.\n","            X = np.array(X)\n","            y = np.array(y)\n","            # Add x_0 = 1 to each instance for the bias term.\n","            X = np.c_[np.ones((X.shape[0], 1)), X]\n","            # Scipy implementation of least squares.\n","            self.coef_, residues, rank, singular = lstsq(X, y)\n","\n","            return self\n","\n","        elif self.solver == 'bgd': \n","            super(LinearRegression, self).fit(X, y)\n","\n","\n","class Ridge(Regression):\n","    \"\"\"\n","    Class representing a linear regression model with l2 regularization.\n","    Minimizes the cost fuction:\n","        J(w) = MSE(w) + alpha * 1/2 * ||w||^2\n","    where w is the vector of feature weights and alpha is the hyperparameter\n","    controlling how much regularization is done to the model.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-1\n","        Learning rate determining the size of steps in batch gradient descent.\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    Attributes \n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    Notes\n","    -----\n","    This class is capable of being trained using batch gradient descent at\n","    current version.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-1, alpha=1.0):\n","        self.alpha = alpha\n","        self.regularization = l2_regularization(alpha=self.alpha)\n","        super(Ridge, self).__init__(n_iter=n_iter, lr=lr)"],"metadata":{"id":"k0Hgt7zI4QGi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='lasso_regression'></a>"],"metadata":{"id":"htKZjTAt4fh_"}},{"cell_type":"markdown","source":["### Lasso regression"],"metadata":{"id":"bMMdUI3b4jS0"}},{"cell_type":"markdown","source":["#### L1 regularization"],"metadata":{"id":"RxZHSJQe4sdt"}},{"cell_type":"code","source":["class l1_regularization():\n","    \"\"\"\n","    Add l1 regularization penalty to linear models.\n","    Regularization term:\n","        alpha * ||w||\n","    where w is the vector of feature weights and alpha is the hyperparameter\n","    controlling how much regularization is done to the model.\n","    Parameters\n","    ----------\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    Notes\n","    -----\n","    The bias term is not regularized and therefore should be omitted from the\n","    feature weights as input.  \n","    \"\"\"\n","    def __init__(self, alpha=1.0):\n","        self.alpha = alpha \n","\n","    def __call__(self, w):\n","        \"Calculate l1 regularization term.\"\n","        return self.alpha * np.linalg.norm(w, 1)\n","\n","    def grad(self, w):\n","        \"\"\"Calculate subgradient vector of l1 regularization penalty.\n","        \n","                      -1 if w_i < 0\n","            sign(w) =  0 if w_i = 0\n","                       1 if w_i > 0\n","        where w is the vector of feature weights.\n","        \"\"\"\n","        subgradient = self.alpha * np.sign(w)\n","        # Insert 0 for bias term.\n","        return np.insert(subgradient, 0, 0, axis=0)"],"metadata":{"id":"0ixENkL_4ngI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Lasso regression"],"metadata":{"id":"-FR8a3SZ4zHM"}},{"cell_type":"code","source":["class Lasso(Regression):\n","    \"\"\"\n","    Class representing a linear regression model with l1 regularization.\n","    Minimizes the cost fuction:\n","        J(w) = MSE(w) + alpha * ||w||\n","    where w is the vector of feature weights and alpha is the hyperparameter\n","    controlling how much regularization is done to the model.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-2\n","        Learning rate determining the size of steps in batch gradient descent.\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    Attributes\n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    Notes\n","    -----\n","    This class is capable of being trained using batch gradient descent at\n","    current version.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-2, alpha=1.0):\n","        self.alpha = alpha\n","        self.regularization = l1_regularization(alpha=self.alpha)\n","        super(Lasso, self).__init__(n_iter=n_iter, lr=lr)"],"metadata":{"id":"v6gvfW-C40zY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='elastic_net'></a>"],"metadata":{"id":"dmRHDDdQ44n-"}},{"cell_type":"markdown","source":["### Elastic Net regression"],"metadata":{"id":"NCOUSZf347GQ"}},{"cell_type":"markdown","source":["#### L1-L2 regularization"],"metadata":{"id":"Tf_k0MOR5LVg"}},{"cell_type":"code","source":["class l1_l2_regularization():\n","    \"\"\"\n","    Add a mix of l1 and l2 regularization penalty to linear models.\n","    Regularization term:\n","        r * alpha * ||w|| + (1 - r) / 2 * alpha * ||w||^2\n","    where r is the mix ratio, alpha is the factor determining the amount\n","    of regularization and w is the vector of feature weights.\n","    Parameters\n","    ----------\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    r : float, default=0.5\n","        Mix ratio determining the amount of l1 vs l2 regularization to add.  \n","        A value of 0 is equivalent to l2 regularization and a value of 1 is\n","        equivalent to l1 regularization.\n","    Notes\n","    -----\n","    The bias term is not regularized and therefore should be omitted from the\n","    feature weights as input.  \n","    \"\"\"\n","    def __init__(self, alpha=1.0, r=0.5):\n","        self.alpha = alpha\n","        self.r = r \n","\n","    def __call__(self, w):\n","        \"\"\"Calculate elastic net regularization penalty.\"\"\"\n","        l1_term = self.alpha * np.linalg.norm(w, 1)\n","        l2_term = self.alpha * 0.5 * np.linalg.norm(w, 2)\n","\n","        return self.r * l1_term + (1 - self.r) * l2_term\n","\n","    def grad(self, w):\n","        \"\"\"\n","        Calculate gradient descent regularization penalty.\n","            alpha * (r * sign(w) + (1 - r) * w)\n","        where r is the mix ratio, alpha is the factor determining the amount\n","        of regularization and w is the vector of feature weights.\n","        \"\"\"\n","        l1_grad = self.r * np.sign(w)\n","        l2_grad = np.asarray(1 - self.r) * w \n","\n","        gradient_penalty = self.alpha * (l1_grad + l2_grad)\n","        # Insert 0 for bias term.\n","        return np.insert(gradient_penalty, 0, 0, axis=0)"],"metadata":{"id":"Lq3pzAVq5H3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Elastic Net regression"],"metadata":{"id":"3wn0WDUi5Mxc"}},{"cell_type":"code","source":["class ElasticNet(Regression):\n","    \"\"\"\n","    Class representing a linear regression model with a mix of l1 and l2 \n","    regularization.\n","    Minimizes the cost function:\n","        J(w) = MSE(w) + r * alpha * ||w|| + (1 - r) * alpha * 1/2 * ||w||^2\n","    where w is the vector of feature weights, r is the mix ratio, and alpha\n","    is the hyperparameter controlling how much regularization is done.\n","    Parameters\n","    ----------\n","    n_iter : float, default=1000\n","        Maximum number of iterations to be used by batch gradient descent.\n","    lr : float, default=1e-2\n","        Learning rate determining the size of steps in batch gradient descent.\n","    alpha : float, default=1.0\n","        Factor determining the amount of regularization to be performed on\n","        the model.\n","    r : float, default=0.5\n","        Mix ratio determining the amount of l1 vs l2 regularization to add.  \n","        A value of 0 is equivalent to l2 regularization and a value of 1 is\n","        equivalent to l1 regularization.\n","    Attributes\n","    ----------\n","    coef_ : array of shape (n_features, 1)\n","        Estimated coefficients for the regression problem.\n","    Notes\n","    -----\n","    This class is capable of being trained using batch gradient descent at\n","    current version.\n","    \"\"\"\n","    def __init__(self, n_iter=1000, lr=1e-2, alpha=1.0, r=0.5):\n","        self.alpha = alpha\n","        self.r = r \n","        self.regularization = l1_l2_regularization(alpha=self.alpha, r=self.r)\n","        super(ElasticNet, self).__init__(n_iter=n_iter, lr=lr)"],"metadata":{"id":"hOxktzw75Vhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DQ0vbGRy5YUL"},"execution_count":null,"outputs":[]}]}