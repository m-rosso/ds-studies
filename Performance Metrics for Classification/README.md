## Performance Metrics for Classification

This folder contains codes and discussion over performance metrics for classification tasks. All of these are presented together in the same Jupyter notebook. An additional Jupyter notebook compares, for a large collection of distinct datasets, two different, but highly similar statistics: precision-recall AUC and average precision score. The expected high correlation between them is briefly assessed and discussed in that notebook.
<br>
The codes for model estimations that conducted to results used for the analysis of "Performance Metrics for Classification" are the same for the folder named "GBM Hyper-parameters".
