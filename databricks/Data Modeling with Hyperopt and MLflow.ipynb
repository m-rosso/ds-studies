{"cells":[{"cell_type":"markdown","source":["## Data Modeling with Hyperopt and MLflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c69b9d2a-57bb-4d13-91cc-257e16a33631"}}},{"cell_type":"markdown","source":["This notebook applies broader functionalities from Databricks that can be implemented without distributed computing (even though this is also useful here). Consequently, pandas APIs are used for handling data, instead of PySpark or even Koalas, as done in other notebooks. The main objective of this notebook is to use Hyperopt and MLflow for a binary classification task that otherwise would make use of standard approaches, such as the solely use of libraries as scikit-learn and XGBoost, besides of a handmade API for model tracking.\n\nAs mentioned, the data in handled through pandas, given that its volume is relatively small and codes for data pre-processing were constructed upon that Python framework. The data pre-processing procedures follow from a single function whose components can be found in the notebook of module-kind \"FunctionsClasses\". The response variable is binary and moderately imbalanced to the positive class. Concerning the learning task, two different fitting methods will be implemented: a logistic regression model (from scikit-learn) and a gradient boosted model (from XGBoost).\n\n[MLflow](https://www.mlflow.org/docs/latest/index.html) is an API that allows, together with Databricks UI, a comprehensive and user-friendly framework for model tracking and registry. Within an MLflow context, several elements of a model training (from parameters and metrics to the fitted models) can be promptly logged, and later recovered back using the MLflow API. This helps exploring different models and retrieving the most appropriate one in order to register it and then deploy it into production.\n\n[Hyperopt](http://hyperopt.github.io/hyperopt/) is an efficient library for optimizing hyper-parameters of machine learning models, although also the optimized choice of models themselves is available. Its basic usage is pretty simple: a function that takes values of hyper-parameters as arguments and returns the value of an objective function should be created, together with the search space and other elements for the optimization (such as the algorithm, maximum number of evaluations, and so on). Even that no distributed computing is necessary, once the notebook is attached to a Spark cluster, the search for the best hyper-parameters values can be distributed for faster results. The sinergy between MLflow and Hyperopt is huge: the first can keep track of all tested models fittted during the search conducted by the second.\n\nBelow, we find codes that import and pre-process data and a large section with data modeling. First, default models are fitted using fixed values of hyper-parameters and applying MLflow to log different elements of the model training. Then, Hyperopt is used to search for the best values of hyper-parameters for each learning algorithm (logistic regression and XGBoost). K-folds CV is the validation strategy, instead of a train-validation-test split. Finally, using those best values, final models are fitted, and codes illustrate how to register them into Databricks DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e88fd2c6-b294-4c65-a609-4bbef695bbca"}}},{"cell_type":"markdown","source":["## Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b129ff9b-e9cc-44f5-b6fe-dafdbbdb022c"}}},{"cell_type":"code","source":["%run \"./FunctionsClasses\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8db26f79-9a75-484d-9e61-4465c7705d50"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import __version__ as sk_version\n\nimport xgboost as xgb\nfrom xgboost import __version__ as xgb_version\n\nimport mlflow\nfrom mlflow.models.signature import infer_signature\nfrom mlflow.utils.environment import _mlflow_conda_env\n\nimport hyperopt as hp\nfrom hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK, space_eval\nfrom hyperopt.pyll import scope\n\nimport cloudpickle"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ace570d-412c-4d07-9f21-54df3cb73b98"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Functions and classes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f92117c-8a6c-428b-9b40-ea39304a9304"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e11973d-2274-4d83-93e3-c92b868ff421"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Settings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e592ae7-1b66-4363-b653-0daff69ca8fa"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a27ee40-00dc-455c-b949-bd0f119b4348"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting unidecode\n  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\nInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.2\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting unidecode\n  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\nInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.2\nPython interpreter will be restarted.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Importing data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4895b52a-7fc6-449c-89f7-e93842db8e98"}}},{"cell_type":"code","source":["df = spark.read.format('csv').\\\n           options(header='true', delimiter = ',', inferSchema='true').\\\n           load(\"/FileStore/shared_uploads/matheusf.rosso@gmail.com/fraud_data_sample.csv\")\ndf = df.toPandas().sort_values('epoch', ascending=True)\n\n# Convertendo epoch em datetime:\ndf['date'] = df['epoch'].apply(lambda x: epoch_to_date(float(x)))\n\nprint(f'Type of df: {type(df)}.')\nprint(f'Shape of df: {df.shape}.')\nprint(f'Number of unique orders: {df.order_id.nunique()}.')\nprint('Time interval: from {0} to {1}.'.format(str(df.date.apply(lambda x: x.date()).min()),\n                                               str(df.date.apply(lambda x: x.date()).max())))\n\n# Support variables:\ndrop_vars = ['y', 'order_amount', 'store_id', 'order_id', 'status', 'epoch', 'date', 'weight']\n\n# df.head(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e88fa0d8-3dcb-4871-817f-12957985004f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of df: <class 'pandas.core.frame.DataFrame'>.\nShape of df: (8361, 1429).\nNumber of unique orders: 8361.\nTime interval: from 2021-05-17 to 2021-06-27.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of df: <class 'pandas.core.frame.DataFrame'>.\nShape of df: (8361, 1429).\nNumber of unique orders: 8361.\nTime interval: from 2021-05-17 to 2021-06-27.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Train-test split"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e47443f-935d-4e81-85ee-372c304a7397"}}},{"cell_type":"code","source":["df_train, df_test = train_test_split(df, preserve_date=True, date_var='date', test_ratio=0.25, shuffle=False, seed=None)\n\n# Intervalo de tempo dos dados de treinamento e de teste:\nfirst_date_train = str(df_train['date'].min().date())\nlast_date_train = str(df_train['date'].max().date())\nfirst_date_test = str(df_test['date'].min().date())\nlast_date_test = str(df_test['date'].max().date())\n\nprint(f'Shape of df_train: {df_train.shape}.')\nprint(f'Number of unique instances: {df_train.order_id.nunique()}.')\nprint(f'Time interval: ({first_date_train}, {last_date_train}).\\n')\n\nprint(f'Shape of df_test: {df_test.shape}.')\nprint(f'Number of unique instances: {df_test.order_id.nunique()}.')\nprint(f'Time interval: ({first_date_test}, {last_date_test}).')\n\n# df_train.head(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20684f6b-bf6d-4003-92e2-718358237ccc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Shape of df_train: (6313, 1429).\nNumber of unique instances: 6313.\nTime interval: (2021-05-17, 2021-06-09).\n\nShape of df_test: (2048, 1429).\nNumber of unique instances: 2048.\nTime interval: (2021-06-10, 2021-06-27).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Shape of df_train: (6313, 1429).\nNumber of unique instances: 6313.\nTime interval: (2021-05-17, 2021-06-09).\n\nShape of df_test: (2048, 1429).\nNumber of unique instances: 2048.\nTime interval: (2021-06-10, 2021-06-27).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Data pre-processing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed8d6bfc-b740-474b-ba34-1eb31f83c18e"}}},{"cell_type":"code","source":["df_train, df_test, df_train_scaled, df_test_scaled = pre_process(training_data=df_train, test_data=df_test,\n                                                                 vars_to_drop=drop_vars,\n                                                                 log_transform=True, standardize=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"576d5a1b-d7d0-4cd5-9c9d-754f81fa610d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"---------------------------------------------------------------------------------------------------------\n\u001B[1mCLASSIFYING FEATURES AND EARLY SELECTION\u001B[0m\n\n\nInitial number of features: 1421.\n3 features were dropped for excessive number of missings!\n360 features were dropped for having no variance!\n1058 remaining features.\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mASSESSING MISSING VALUES\u001B[0m\n\n\n\u001B[1mTraining data:\u001B[0m\n\u001B[1mNumber of features with missings:\u001B[0m 271 out of 1066 features (25.42%).\n\u001B[1mAverage number of missings:\u001B[0m 443 out of 6313 observations (7.02%).\n\n\u001B[1mTest data:\u001B[0m\n\u001B[1mNumber of features with missings:\u001B[0m 158 out of 1066 features (14.82%).\n\u001B[1mAverage number of missings:\u001B[0m 128 out of 2048 observations (6.25%).\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mAPPLYING LOGARITHMIC TRANSFORMATION OVER NUMERICAL DATA\u001B[0m\n\n\n\u001B[1mTraining data:\u001B[0m\n\u001B[1mNumber of numerical variables log-transformed:\u001B[0m 1006.\n\u001B[1mTest data:\u001B[0m\n\u001B[1mNumber of numerical variables log-transformed:\u001B[0m 1006.\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mAPPLYING STANDARD SCALE TRANSFORMATION OVER NUMERICAL DATA\u001B[0m\n\n\n\u001B[1mStandard scaling training data...\u001B[0m\n\u001B[1mStandard scaling test data...\u001B[0m\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mTREATING MISSING VALUES\u001B[0m\n\n\n\u001B[1mTreating missing values of training data...\u001B[0m\n\u001B[1mTreating missing values of test data...\u001B[0m\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mTRANSFORMING CATEGORICAL FEATURES\u001B[0m\n\n\n\u001B[1mNumber of categorical features:\u001B[0m 10\n\u001B[1mNumber of overall selected dummies:\u001B[0m 54.\n\u001B[1mShape of df_train_scaled:\u001B[0m (6313, 1378).\n\u001B[1mShape of df_test_scaled:\u001B[0m (2048, 1265).\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mFINAL ASSESSMENT OF MISSINGS AND CHECKING DATASETS CONSISTENCY\u001B[0m\n\n\nTraining and test data are consistent with each other.\n---------------------------------------------------------------------------------------------------------\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["---------------------------------------------------------------------------------------------------------\n\u001B[1mCLASSIFYING FEATURES AND EARLY SELECTION\u001B[0m\n\n\nInitial number of features: 1421.\n3 features were dropped for excessive number of missings!\n360 features were dropped for having no variance!\n1058 remaining features.\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mASSESSING MISSING VALUES\u001B[0m\n\n\n\u001B[1mTraining data:\u001B[0m\n\u001B[1mNumber of features with missings:\u001B[0m 271 out of 1066 features (25.42%).\n\u001B[1mAverage number of missings:\u001B[0m 443 out of 6313 observations (7.02%).\n\n\u001B[1mTest data:\u001B[0m\n\u001B[1mNumber of features with missings:\u001B[0m 158 out of 1066 features (14.82%).\n\u001B[1mAverage number of missings:\u001B[0m 128 out of 2048 observations (6.25%).\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mAPPLYING LOGARITHMIC TRANSFORMATION OVER NUMERICAL DATA\u001B[0m\n\n\n\u001B[1mTraining data:\u001B[0m\n\u001B[1mNumber of numerical variables log-transformed:\u001B[0m 1006.\n\u001B[1mTest data:\u001B[0m\n\u001B[1mNumber of numerical variables log-transformed:\u001B[0m 1006.\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mAPPLYING STANDARD SCALE TRANSFORMATION OVER NUMERICAL DATA\u001B[0m\n\n\n\u001B[1mStandard scaling training data...\u001B[0m\n\u001B[1mStandard scaling test data...\u001B[0m\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mTREATING MISSING VALUES\u001B[0m\n\n\n\u001B[1mTreating missing values of training data...\u001B[0m\n\u001B[1mTreating missing values of test data...\u001B[0m\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mTRANSFORMING CATEGORICAL FEATURES\u001B[0m\n\n\n\u001B[1mNumber of categorical features:\u001B[0m 10\n\u001B[1mNumber of overall selected dummies:\u001B[0m 54.\n\u001B[1mShape of df_train_scaled:\u001B[0m (6313, 1378).\n\u001B[1mShape of df_test_scaled:\u001B[0m (2048, 1265).\n\n\n---------------------------------------------------------------------------------------------------------\n\n\n---------------------------------------------------------------------------------------------------------\n\u001B[1mFINAL ASSESSMENT OF MISSINGS AND CHECKING DATASETS CONSISTENCY\u001B[0m\n\n\nTraining and test data are consistent with each other.\n---------------------------------------------------------------------------------------------------------\n\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Data modeling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3e61b00-c6d3-4f46-9ef2-6f0866e886f7"}}},{"cell_type":"code","source":["# Training and test data:\nX_train, y_train = (df_train_scaled.drop(drop_vars, axis=1), df_train_scaled['y'])\nX_test, y_test = (df_test_scaled.drop(drop_vars, axis=1), df_test_scaled['y'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3823308a-815d-4ac0-b4be-c15a398e3d9a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Default models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1fb8fb4a-bfae-44e9-b722-de7df76ddfaf"}}},{"cell_type":"markdown","source":["[MLflow API](https://www.mlflow.org/docs/latest/python_api/index.html) is the main documentation for its implementation. Below, only some of its main functions and classes are used, such as [autologging](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging), that logs standard elements of model training, [log](https://mlflow.org/docs/0.4.2/python_api/mlflow.html) of user-defined elements, and the log of elements conditional on the model object, here [sklearn](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html) and [XGBoost](https://www.mlflow.org/docs/latest/python_api/mlflow.xgboost.html) models."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f199e15e-9de9-4e95-a1f5-30ac7e641334"}}},{"cell_type":"markdown","source":["#### Logistic regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48cbda83-f263-4dda-9b90-bb87aac79e54"}}},{"cell_type":"code","source":["# Initializing the autologging of model parameters and metrics:\nmlflow.autolog()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f885dce7-19f7-477e-b7bb-8dd842096733"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2021/10/24 18:50:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2021/10/24 18:50:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2021/10/24 18:50:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python_shell/dbruntime/MlflowAutologging.py:170: UserWarning: Since MLflow PySpark ML autologging is now enabled, the conflicting MLflow Tracking integration with MLLib (https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/mllib-mlflow-integration.html) has been disabled.\"\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2021/10/24 18:50:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2021/10/24 18:50:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2021/10/24 18:50:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/databricks/python_shell/dbruntime/MlflowAutologging.py:170: UserWarning: Since MLflow PySpark ML autologging is now enabled, the conflicting MLflow Tracking integration with MLLib (https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/mllib-mlflow-integration.html) has been disabled.\"\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Creating the MLflow context for logging additional information about the model estimation:\nwith mlflow.start_run(run_name='lr_default') as lr_run:\n  # Creating model estimator object:\n  lr_model = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', warm_start=True)\n\n  # Training the model and converting the estimator into a transformer:\n  lr_model.fit(X_train, y_train)\n\n  # Predictions and ROC-AUC evaluated on test data:\n  pred_test = [p[1] for p in lr_model.predict_proba(X_test)]\n  test_roc_auc = roc_auc_score(y_test, pred_test)\n  \n  # Logging the model artifact:\n  mlflow.sklearn.log_model(artifact_path='lr_default_model', sk_model=lr_model)\n  \n  # Logging the test ROC-AUC:\n  mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n  print(f'\\nTest ROC-AUC : {test_roc_auc:.4f}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f81c776d-8c9b-4f22-8ccc-c189b2f3a6b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2021/10/24 18:51:37 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 18:51:38 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/48a321324fb3457caefe70de27a230d2/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC : 0.8706.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2021/10/24 18:51:37 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 18:51:38 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/48a321324fb3457caefe70de27a230d2/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC : 0.8706.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Loading the logged model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e530a7a9-86ba-4bd0-be2a-422a44e20f6b"}}},{"cell_type":"code","source":["# Loaded model:\nlr_model_loaded = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=lr_run.info.run_id\n  )\n)\n\n# Predictions on test data:\npred_test_loaded = lr_model_loaded.predict(X_test)\npred_test_loaded[0:10]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67057ab6-0135-491a-b47b-cf70f49ca8fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[30]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[30]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### XGBoost"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67189314-bb92-44f4-af15-30a92ec4a4f4"}}},{"cell_type":"code","source":["# Creating the MLflow context for logging additional information about the model estimation:\nwith mlflow.start_run(run_name='xgboost_default') as xgb_run:\n  # Creating the objects containing training and test data (inputs and labels):\n  train = xgb.DMatrix(data=X_train, label=y_train)\n  test = xgb.DMatrix(data=X_test, label=y_test)\n\n  # Creating and training the model:\n  xgb_model = xgb.train(params={'subsample': 0.75, 'eta': 0.1, 'max_depth': 3, 'objective': 'binary:logistic'},\n                        dtrain=train, num_boost_round=500, evals=[(test, \"test\")], early_stopping_rounds=50)\n\n  # Predictions and ROC-AUC evaluated on test data:\n  pred_test = xgb_model.predict(test)\n  test_roc_auc = roc_auc_score(y_test, pred_test)\n  \n  # Logging the model artifact:\n  mlflow.xgboost.log_model(artifact_path='xgboost_default_model', xgb_model=xgb_model)\n  \n  # Logging the test ROC-AUC:\n  mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n  print(f'\\nTest ROC-AUC : {test_roc_auc:.4f}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4404b1c-1e84-45be-afa4-a49ce06f15a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[18:53:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0]\ttest-logloss:0.60759\n[1]\ttest-logloss:0.53743\n[2]\ttest-logloss:0.47931\n[3]\ttest-logloss:0.42967\n[4]\ttest-logloss:0.38732\n[5]\ttest-logloss:0.35132\n[6]\ttest-logloss:0.31941\n[7]\ttest-logloss:0.29195\n[8]\ttest-logloss:0.26799\n[9]\ttest-logloss:0.24653\n[10]\ttest-logloss:0.22833\n[11]\ttest-logloss:0.21176\n[12]\ttest-logloss:0.19719\n[13]\ttest-logloss:0.18390\n[14]\ttest-logloss:0.17216\n[15]\ttest-logloss:0.16178\n[16]\ttest-logloss:0.15233\n[17]\ttest-logloss:0.14418\n[18]\ttest-logloss:0.13714\n[19]\ttest-logloss:0.13026\n[20]\ttest-logloss:0.12360\n[21]\ttest-logloss:0.11815\n[22]\ttest-logloss:0.11388\n[23]\ttest-logloss:0.11004\n[24]\ttest-logloss:0.10622\n[25]\ttest-logloss:0.10287\n[26]\ttest-logloss:0.10091\n[27]\ttest-logloss:0.09778\n[28]\ttest-logloss:0.09561\n[29]\ttest-logloss:0.09301\n[30]\ttest-logloss:0.09118\n[31]\ttest-logloss:0.08929\n[32]\ttest-logloss:0.08763\n[33]\ttest-logloss:0.08626\n[34]\ttest-logloss:0.08485\n[35]\ttest-logloss:0.08314\n[36]\ttest-logloss:0.08193\n[37]\ttest-logloss:0.08038\n[38]\ttest-logloss:0.07935\n[39]\ttest-logloss:0.07869\n[40]\ttest-logloss:0.07822\n[41]\ttest-logloss:0.07731\n[42]\ttest-logloss:0.07652\n[43]\ttest-logloss:0.07574\n[44]\ttest-logloss:0.07472\n[45]\ttest-logloss:0.07465\n[46]\ttest-logloss:0.07387\n[47]\ttest-logloss:0.07354\n[48]\ttest-logloss:0.07347\n[49]\ttest-logloss:0.07300\n[50]\ttest-logloss:0.07266\n[51]\ttest-logloss:0.07369\n[52]\ttest-logloss:0.07298\n[53]\ttest-logloss:0.07300\n[54]\ttest-logloss:0.07286\n[55]\ttest-logloss:0.07248\n[56]\ttest-logloss:0.07220\n[57]\ttest-logloss:0.07233\n[58]\ttest-logloss:0.07197\n[59]\ttest-logloss:0.07210\n[60]\ttest-logloss:0.07224\n[61]\ttest-logloss:0.07191\n[62]\ttest-logloss:0.07160\n[63]\ttest-logloss:0.07117\n[64]\ttest-logloss:0.07106\n[65]\ttest-logloss:0.07141\n[66]\ttest-logloss:0.07147\n[67]\ttest-logloss:0.07167\n[68]\ttest-logloss:0.07147\n[69]\ttest-logloss:0.07131\n[70]\ttest-logloss:0.07125\n[71]\ttest-logloss:0.07121\n[72]\ttest-logloss:0.07127\n[73]\ttest-logloss:0.07139\n[74]\ttest-logloss:0.07119\n[75]\ttest-logloss:0.07145\n[76]\ttest-logloss:0.07169\n[77]\ttest-logloss:0.07133\n[78]\ttest-logloss:0.07118\n[79]\ttest-logloss:0.07087\n[80]\ttest-logloss:0.07078\n[81]\ttest-logloss:0.07030\n[82]\ttest-logloss:0.07017\n[83]\ttest-logloss:0.07057\n[84]\ttest-logloss:0.07059\n[85]\ttest-logloss:0.07057\n[86]\ttest-logloss:0.07061\n[87]\ttest-logloss:0.07044\n[88]\ttest-logloss:0.07048\n[89]\ttest-logloss:0.07039\n[90]\ttest-logloss:0.07054\n[91]\ttest-logloss:0.07031\n[92]\ttest-logloss:0.07035\n[93]\ttest-logloss:0.07088\n[94]\ttest-logloss:0.07104\n[95]\ttest-logloss:0.07094\n[96]\ttest-logloss:0.07104\n[97]\ttest-logloss:0.07084\n[98]\ttest-logloss:0.07134\n[99]\ttest-logloss:0.07132\n[100]\ttest-logloss:0.07153\n[101]\ttest-logloss:0.07153\n[102]\ttest-logloss:0.07179\n[103]\ttest-logloss:0.07173\n[104]\ttest-logloss:0.07176\n[105]\ttest-logloss:0.07165\n[106]\ttest-logloss:0.07198\n[107]\ttest-logloss:0.07160\n[108]\ttest-logloss:0.07164\n[109]\ttest-logloss:0.07158\n[110]\ttest-logloss:0.07150\n[111]\ttest-logloss:0.07160\n[112]\ttest-logloss:0.07185\n[113]\ttest-logloss:0.07172\n[114]\ttest-logloss:0.07125\n[115]\ttest-logloss:0.07140\n[116]\ttest-logloss:0.07193\n[117]\ttest-logloss:0.07210\n[118]\ttest-logloss:0.07220\n[119]\ttest-logloss:0.07199\n[120]\ttest-logloss:0.07198\n[121]\ttest-logloss:0.07180\n[122]\ttest-logloss:0.07192\n[123]\ttest-logloss:0.07200\n[124]\ttest-logloss:0.07195\n[125]\ttest-logloss:0.07211\n[126]\ttest-logloss:0.07213\n[127]\ttest-logloss:0.07192\n[128]\ttest-logloss:0.07198\n[129]\ttest-logloss:0.07208\n[130]\ttest-logloss:0.07209\n[131]\ttest-logloss:0.07202\n2021/10/24 18:54:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 18:54:07 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/aaafa3e88cd141009f4f84048c20181e/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC : 0.9065.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[18:53:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[0]\ttest-logloss:0.60759\n[1]\ttest-logloss:0.53743\n[2]\ttest-logloss:0.47931\n[3]\ttest-logloss:0.42967\n[4]\ttest-logloss:0.38732\n[5]\ttest-logloss:0.35132\n[6]\ttest-logloss:0.31941\n[7]\ttest-logloss:0.29195\n[8]\ttest-logloss:0.26799\n[9]\ttest-logloss:0.24653\n[10]\ttest-logloss:0.22833\n[11]\ttest-logloss:0.21176\n[12]\ttest-logloss:0.19719\n[13]\ttest-logloss:0.18390\n[14]\ttest-logloss:0.17216\n[15]\ttest-logloss:0.16178\n[16]\ttest-logloss:0.15233\n[17]\ttest-logloss:0.14418\n[18]\ttest-logloss:0.13714\n[19]\ttest-logloss:0.13026\n[20]\ttest-logloss:0.12360\n[21]\ttest-logloss:0.11815\n[22]\ttest-logloss:0.11388\n[23]\ttest-logloss:0.11004\n[24]\ttest-logloss:0.10622\n[25]\ttest-logloss:0.10287\n[26]\ttest-logloss:0.10091\n[27]\ttest-logloss:0.09778\n[28]\ttest-logloss:0.09561\n[29]\ttest-logloss:0.09301\n[30]\ttest-logloss:0.09118\n[31]\ttest-logloss:0.08929\n[32]\ttest-logloss:0.08763\n[33]\ttest-logloss:0.08626\n[34]\ttest-logloss:0.08485\n[35]\ttest-logloss:0.08314\n[36]\ttest-logloss:0.08193\n[37]\ttest-logloss:0.08038\n[38]\ttest-logloss:0.07935\n[39]\ttest-logloss:0.07869\n[40]\ttest-logloss:0.07822\n[41]\ttest-logloss:0.07731\n[42]\ttest-logloss:0.07652\n[43]\ttest-logloss:0.07574\n[44]\ttest-logloss:0.07472\n[45]\ttest-logloss:0.07465\n[46]\ttest-logloss:0.07387\n[47]\ttest-logloss:0.07354\n[48]\ttest-logloss:0.07347\n[49]\ttest-logloss:0.07300\n[50]\ttest-logloss:0.07266\n[51]\ttest-logloss:0.07369\n[52]\ttest-logloss:0.07298\n[53]\ttest-logloss:0.07300\n[54]\ttest-logloss:0.07286\n[55]\ttest-logloss:0.07248\n[56]\ttest-logloss:0.07220\n[57]\ttest-logloss:0.07233\n[58]\ttest-logloss:0.07197\n[59]\ttest-logloss:0.07210\n[60]\ttest-logloss:0.07224\n[61]\ttest-logloss:0.07191\n[62]\ttest-logloss:0.07160\n[63]\ttest-logloss:0.07117\n[64]\ttest-logloss:0.07106\n[65]\ttest-logloss:0.07141\n[66]\ttest-logloss:0.07147\n[67]\ttest-logloss:0.07167\n[68]\ttest-logloss:0.07147\n[69]\ttest-logloss:0.07131\n[70]\ttest-logloss:0.07125\n[71]\ttest-logloss:0.07121\n[72]\ttest-logloss:0.07127\n[73]\ttest-logloss:0.07139\n[74]\ttest-logloss:0.07119\n[75]\ttest-logloss:0.07145\n[76]\ttest-logloss:0.07169\n[77]\ttest-logloss:0.07133\n[78]\ttest-logloss:0.07118\n[79]\ttest-logloss:0.07087\n[80]\ttest-logloss:0.07078\n[81]\ttest-logloss:0.07030\n[82]\ttest-logloss:0.07017\n[83]\ttest-logloss:0.07057\n[84]\ttest-logloss:0.07059\n[85]\ttest-logloss:0.07057\n[86]\ttest-logloss:0.07061\n[87]\ttest-logloss:0.07044\n[88]\ttest-logloss:0.07048\n[89]\ttest-logloss:0.07039\n[90]\ttest-logloss:0.07054\n[91]\ttest-logloss:0.07031\n[92]\ttest-logloss:0.07035\n[93]\ttest-logloss:0.07088\n[94]\ttest-logloss:0.07104\n[95]\ttest-logloss:0.07094\n[96]\ttest-logloss:0.07104\n[97]\ttest-logloss:0.07084\n[98]\ttest-logloss:0.07134\n[99]\ttest-logloss:0.07132\n[100]\ttest-logloss:0.07153\n[101]\ttest-logloss:0.07153\n[102]\ttest-logloss:0.07179\n[103]\ttest-logloss:0.07173\n[104]\ttest-logloss:0.07176\n[105]\ttest-logloss:0.07165\n[106]\ttest-logloss:0.07198\n[107]\ttest-logloss:0.07160\n[108]\ttest-logloss:0.07164\n[109]\ttest-logloss:0.07158\n[110]\ttest-logloss:0.07150\n[111]\ttest-logloss:0.07160\n[112]\ttest-logloss:0.07185\n[113]\ttest-logloss:0.07172\n[114]\ttest-logloss:0.07125\n[115]\ttest-logloss:0.07140\n[116]\ttest-logloss:0.07193\n[117]\ttest-logloss:0.07210\n[118]\ttest-logloss:0.07220\n[119]\ttest-logloss:0.07199\n[120]\ttest-logloss:0.07198\n[121]\ttest-logloss:0.07180\n[122]\ttest-logloss:0.07192\n[123]\ttest-logloss:0.07200\n[124]\ttest-logloss:0.07195\n[125]\ttest-logloss:0.07211\n[126]\ttest-logloss:0.07213\n[127]\ttest-logloss:0.07192\n[128]\ttest-logloss:0.07198\n[129]\ttest-logloss:0.07208\n[130]\ttest-logloss:0.07209\n[131]\ttest-logloss:0.07202\n2021/10/24 18:54:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 18:54:07 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/aaafa3e88cd141009f4f84048c20181e/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC : 0.9065.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Loading the logged model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a19d364-6943-4c90-ae31-c83932add79c"}}},{"cell_type":"code","source":["# Loaded model:\nxgb_model_loaded = mlflow.xgboost.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=xgb_run.info.run_id\n  )\n)\n\n# Predictions on test data:\npred_test_loaded = xgb_model_loaded.predict(test)\npred_test_loaded[0:10]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb6a16e8-0843-4dce-bf70-fc1622927f95"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[32]: array([0.00524007, 0.00260701, 0.00251006, 0.04042081, 0.00893994,\n       0.00692396, 0.00344518, 0.00198182, 0.00233956, 0.01562605],\n      dtype=float32)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[32]: array([0.00524007, 0.00260701, 0.00251006, 0.04042081, 0.00893994,\n       0.00692396, 0.00344518, 0.00198182, 0.00233956, 0.01562605],\n      dtype=float32)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Optimizing hyper-parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"405eed2d-d2cf-4030-875e-e0ab6a281c54"}}},{"cell_type":"markdown","source":["Here, [Hyperopt API](http://hyperopt.github.io/hyperopt/#documentation) is used for optimizing hyper-parameters of machine learning models. First, the [search space](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/) is declared, and then optimization takes place inside an MLflow context. Using an object from [SparkTrials class](http://hyperopt.github.io/hyperopt/scaleout/spark/), the search is conducted according to a distributed plan of computing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a08bc6d-3e05-477a-8aa2-8eeba487767a"}}},{"cell_type":"markdown","source":["#### Logistic regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc8233c1-e5f8-481a-a47c-2e4a98514c67"}}},{"cell_type":"code","source":["# Search space for the grid search:\nlr_search_space = {\n  'C': hp.choice('C', [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.5, 0.75, 1.0, 10.0])\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1c83124-1aaf-4f30-a93c-22d2b8c5fd5b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that takes hyper-parameter values as arguments and returns the objective value for minimization:\ndef train_model(params):\n  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n  mlflow.autolog()\n  \n  # Creating the MLflow context for logging additional information about the model estimation:\n  with mlflow.start_run(nested=True, run_name='opt_lr'):\n    val_roc_auc = []\n\n    # Loop over folds of data:\n    for train, val in KFold(3).split(X_train):\n        # Creating model estimator object:\n        model = LogisticRegression(penalty='l1', C=float(params['C']), solver='liblinear', warm_start=True)\n\n        # Training the model and converting the estimator into a transformer:\n        model.fit(X_train.iloc[train, :], y_train.iloc[train])\n\n        # Validation data:\n        X_val, y_val = X_train.iloc[val, :], y_train.iloc[val]\n\n        # Predictions and ROC-AUC evaluated on validation data:\n        pred_val = [p[1] for p in model.predict_proba(X_val)]\n        val_roc_auc.append(roc_auc_score(y_val, pred_val))\n\n    # ROC-AUC calculated through K-folds CV:\n    val_roc_auc = np.nanmean(val_roc_auc)\n\n    # Logging K-folds CV and test ROC-AUC:\n    mlflow.log_metric('val_roc_auc', val_roc_auc)\n    \n    # Returning the objective function for minimization:\n    return {'status': STATUS_OK, 'loss': -1*val_roc_auc}\n  \n# Defining the strategy of distributed computing:\nspark_trials = SparkTrials(parallelism=10)\n \n# MLflow context for tracking hyper-parameters tuning:\nwith mlflow.start_run(run_name='opt_lr'):\n  best_params = fmin(\n    fn=train_model, \n    space=lr_search_space, \n    algo=tpe.suggest, \n    max_evals=96,\n    trials=spark_trials\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65b90e13-00c9-4ab5-9116-3d8d89449ece"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/96 [00:00<?, ?trial/s, best loss=?]\r  1%|          | 1/96 [00:56<1:28:53, 56.14s/trial, best loss: -0.8554495819187787]\r  2%|▏         | 2/96 [01:01<1:04:12, 40.99s/trial, best loss: -0.9179919962611008]\r  4%|▍         | 4/96 [01:05<44:52, 29.26s/trial, best loss: -0.9275053222316195]  \r  5%|▌         | 5/96 [01:51<52:07, 34.37s/trial, best loss: -0.9275053222316195]\r  6%|▋         | 6/96 [01:56<37:58, 25.31s/trial, best loss: -0.9275053222316195]\r  7%|▋         | 7/96 [01:58<27:17, 18.40s/trial, best loss: -0.930579546540648] \r  8%|▊         | 8/96 [01:59<19:20, 13.18s/trial, best loss: -0.930579546540648]\r  9%|▉         | 9/96 [02:01<14:15,  9.83s/trial, best loss: -0.930579546540648]\r 10%|█         | 10/96 [02:08<12:52,  8.99s/trial, best loss: -0.930579546540648]\r 11%|█▏        | 11/96 [02:25<16:10, 11.42s/trial, best loss: -0.930579546540648]\r 12%|█▎        | 12/96 [02:47<20:30, 14.64s/trial, best loss: -0.930579546540648]\r 15%|█▍        | 14/96 [02:51<14:51, 10.87s/trial, best loss: -0.930579546540648]\r 16%|█▌        | 15/96 [02:57<12:46,  9.46s/trial, best loss: -0.930579546540648]\r 17%|█▋        | 16/96 [02:59<09:38,  7.24s/trial, best loss: -0.930579546540648]\r 18%|█▊        | 17/96 [03:03<07:59,  6.07s/trial, best loss: -0.930579546540648]\r 19%|█▉        | 18/96 [03:48<23:06, 17.77s/trial, best loss: -0.930579546540648]\r 20%|█▉        | 19/96 [03:55<18:40, 14.55s/trial, best loss: -0.930579546540648]\r 21%|██        | 20/96 [04:08<17:41, 13.97s/trial, best loss: -0.9352057988968743]\r 22%|██▏       | 21/96 [04:21<17:13, 13.78s/trial, best loss: -0.9352057988968743]\r 23%|██▎       | 22/96 [04:42<19:41, 15.97s/trial, best loss: -0.9352057988968743]\r 24%|██▍       | 23/96 [04:45<14:42, 12.08s/trial, best loss: -0.9352057988968743]\r 25%|██▌       | 24/96 [05:17<21:42, 18.08s/trial, best loss: -0.9352057988968743]\r 26%|██▌       | 25/96 [05:38<22:27, 18.97s/trial, best loss: -0.9352057988968743]\r 27%|██▋       | 26/96 [05:50<19:42, 16.89s/trial, best loss: -0.9352057988968743]\r 28%|██▊       | 27/96 [05:59<16:42, 14.53s/trial, best loss: -0.9352057988968743]\r 29%|██▉       | 28/96 [06:01<12:13, 10.78s/trial, best loss: -0.9352057988968743]\r 30%|███       | 29/96 [06:14<12:48, 11.48s/trial, best loss: -0.9352057988968743]\r 31%|███▏      | 30/96 [06:15<09:10,  8.34s/trial, best loss: -0.9352057988968743]\r 32%|███▏      | 31/96 [06:32<11:51, 10.94s/trial, best loss: -0.9352057988968743]\r 33%|███▎      | 32/96 [06:36<09:27,  8.87s/trial, best loss: -0.9352057988968743]\r 34%|███▍      | 33/96 [07:03<15:02, 14.33s/trial, best loss: -0.9352057988968743]\r 35%|███▌      | 34/96 [07:15<14:06, 13.65s/trial, best loss: -0.9352057988968743]\r 36%|███▋      | 35/96 [07:30<14:00, 13.78s/trial, best loss: -0.9352057988968743]\r 38%|███▊      | 36/96 [07:46<14:28, 14.47s/trial, best loss: -0.9352057988968743]\r 39%|███▊      | 37/96 [07:47<10:15, 10.44s/trial, best loss: -0.9352057988968743]\r 40%|███▉      | 38/96 [07:51<08:13,  8.51s/trial, best loss: -0.9352057988968743]\r 41%|████      | 39/96 [08:18<13:22, 14.07s/trial, best loss: -0.9352057988968743]\r 42%|████▏     | 40/96 [08:39<15:06, 16.18s/trial, best loss: -0.9352057988968743]\r 43%|████▎     | 41/96 [08:41<10:58, 11.98s/trial, best loss: -0.9352057988968743]\r 44%|████▍     | 42/96 [09:00<12:42, 14.13s/trial, best loss: -0.9352057988968743]\r 45%|████▍     | 43/96 [09:12<11:56, 13.51s/trial, best loss: -0.9352057988968743]\r 46%|████▌     | 44/96 [09:25<11:35, 13.37s/trial, best loss: -0.9352057988968743]\r 47%|████▋     | 45/96 [09:34<10:15, 12.07s/trial, best loss: -0.9352057988968743]\r 48%|████▊     | 46/96 [09:47<10:18, 12.37s/trial, best loss: -0.9352057988968743]\r 49%|████▉     | 47/96 [09:53<08:32, 10.46s/trial, best loss: -0.9352057988968743]\r 50%|█████     | 48/96 [10:19<12:06, 15.13s/trial, best loss: -0.9352057988968743]\r 51%|█████     | 49/96 [10:25<09:43, 12.41s/trial, best loss: -0.9352057988968743]\r 52%|█████▏    | 50/96 [10:41<10:07, 13.21s/trial, best loss: -0.9352057988968743]\r 53%|█████▎    | 51/96 [11:08<13:03, 17.41s/trial, best loss: -0.9352057988968743]\r 54%|█████▍    | 52/96 [11:21<11:48, 16.11s/trial, best loss: -0.9352057988968743]\r 55%|█████▌    | 53/96 [11:36<11:19, 15.79s/trial, best loss: -0.9352057988968743]\r 56%|█████▋    | 54/96 [11:38<08:10, 11.67s/trial, best loss: -0.9352057988968743]\r 57%|█████▋    | 55/96 [11:41<06:13,  9.10s/trial, best loss: -0.9352057988968743]\r 58%|█████▊    | 56/96 [11:51<06:16,  9.41s/trial, best loss: -0.9352057988968743]\r 59%|█████▉    | 57/96 [12:04<06:49, 10.50s/trial, best loss: -0.9352057988968743]\r 60%|██████    | 58/96 [12:05<04:50,  7.65s/trial, best loss: -0.9352057988968743]\r 61%|██████▏   | 59/96 [12:32<08:19, 13.49s/trial, best loss: -0.9352057988968743]\r 62%|██████▎   | 60/96 [12:45<08:00, 13.36s/trial, best loss: -0.9352057988968743]\r 64%|██████▎   | 61/96 [12:55<07:12, 12.37s/trial, best loss: -0.9352057988968743]\r 65%|██████▍   | 62/96 [13:19<08:49, 15.59s/trial, best loss: -0.9352057988968743]\r 66%|██████▌   | 63/96 [13:47<10:37, 19.33s/trial, best loss: -0.9352057988968743]\r 67%|██████▋   | 64/96 [13:53<08:10, 15.34s/trial, best loss: -0.9352057988968743]\r 68%|██████▊   | 65/96 [14:01<06:47, 13.14s/trial, best loss: -0.9352057988968743]\r 69%|██████▉   | 66/96 [14:04<05:03, 10.10s/trial, best loss: -0.9352057988968743]\r 70%|██████▉   | 67/96 [14:11<04:26,  9.19s/trial, best loss: -0.9352057988968743]\r 71%|███████   | 68/96 [14:12<03:08,  6.74s/trial, best loss: -0.9352057988968743]\r 72%|███████▏  | 69/96 [14:36<05:24, 12.03s/trial, best loss: -0.9352057988968743]\r 73%|███████▎  | 70/96 [14:37<03:47,  8.73s/trial, best loss: -0.9352057988968743]\r 74%|███████▍  | 71/96 [14:50<04:10, 10.04s/trial, best loss: -0.9352057988968743]\r 75%|███████▌  | 72/96 [15:03<04:22, 10.93s/trial, best loss: -0.9352057988968743]\r 76%|███████▌  | 73/96 [15:05<03:09,  8.26s/trial, best loss: -0.9352057988968743]\r 77%|███████▋  | 74/96 [15:18<03:33,  9.70s/trial, best loss: -0.9352057988968743]\r 78%|███████▊  | 75/96 [15:45<05:13, 14.91s/trial, best loss: -0.9352057988968743]\r 79%|███████▉  | 76/96 [15:51<04:04, 12.25s/trial, best loss: -0.9352057988968743]\r 80%|████████  | 77/96 [16:17<05:11, 16.39s/trial, best loss: -0.9352057988968743]\r 81%|████████▏ | 78/96 [16:33<04:47, 16.00s/trial, best loss: -0.9352057988968743]\r 82%|████████▏ | 79/96 [16:42<03:57, 13.95s/trial, best loss: -0.9352057988968743]\r 83%|████████▎ | 80/96 [16:53<03:29, 13.07s/trial, best loss: -0.9352057988968743]\r 84%|████████▍ | 81/96 [17:07<03:20, 13.36s/trial, best loss: -0.9352057988968743]\r 85%|████████▌ | 82/96 [17:13<02:36, 11.19s/trial, best loss: -0.9352057988968743]\r 86%|████████▋ | 83/96 [17:15<01:49,  8.44s/trial, best loss: -0.9352057988968743]\r 88%|████████▊ | 84/96 [17:35<02:23, 11.98s/trial, best loss: -0.9352057988968743]\r 89%|████████▊ | 85/96 [17:47<02:12, 12.01s/trial, best loss: -0.9352057988968743]\r 90%|████████▉ | 86/96 [18:01<02:06, 12.61s/trial, best loss: -0.9352057988968743]\r 91%|█████████ | 87/96 [18:09<01:41, 11.23s/trial, best loss: -0.9352057988968743]\r 92%|█████████▏| 88/96 [18:16<01:19,  9.97s/trial, best loss: -0.9352057988968743]\r 93%|█████████▎| 89/96 [18:50<02:00, 17.19s/trial, best loss: -0.9352057988968743]\r 94%|█████████▍| 90/96 [19:01<01:32, 15.34s/trial, best loss: -0.9352057988968743]\r 95%|█████████▍| 91/96 [19:04<00:58, 11.64s/trial, best loss: -0.9352057988968743]\r 96%|█████████▌| 92/96 [19:18<00:49, 12.35s/trial, best loss: -0.9352057988968743]\r 97%|█████████▋| 93/96 [19:20<00:27,  9.25s/trial, best loss: -0.9352057988968743]\r 98%|█████████▊| 94/96 [19:31<00:19,  9.78s/trial, best loss: -0.9352057988968743]\r 99%|█████████▉| 95/96 [19:32<00:07,  7.15s/trial, best loss: -0.9352057988968743]\r100%|██████████| 96/96 [19:38<00:00,  6.81s/trial, best loss: -0.9352057988968743]\r100%|██████████| 96/96 [19:38<00:00, 12.28s/trial, best loss: -0.9352057988968743]\nTotal Trials: 96: 96 succeeded, 0 failed, 0 cancelled.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/96 [00:00<?, ?trial/s, best loss=?]\r  1%|          | 1/96 [00:56<1:28:53, 56.14s/trial, best loss: -0.8554495819187787]\r  2%|▏         | 2/96 [01:01<1:04:12, 40.99s/trial, best loss: -0.9179919962611008]\r  4%|▍         | 4/96 [01:05<44:52, 29.26s/trial, best loss: -0.9275053222316195]  \r  5%|▌         | 5/96 [01:51<52:07, 34.37s/trial, best loss: -0.9275053222316195]\r  6%|▋         | 6/96 [01:56<37:58, 25.31s/trial, best loss: -0.9275053222316195]\r  7%|▋         | 7/96 [01:58<27:17, 18.40s/trial, best loss: -0.930579546540648] \r  8%|▊         | 8/96 [01:59<19:20, 13.18s/trial, best loss: -0.930579546540648]\r  9%|▉         | 9/96 [02:01<14:15,  9.83s/trial, best loss: -0.930579546540648]\r 10%|█         | 10/96 [02:08<12:52,  8.99s/trial, best loss: -0.930579546540648]\r 11%|█▏        | 11/96 [02:25<16:10, 11.42s/trial, best loss: -0.930579546540648]\r 12%|█▎        | 12/96 [02:47<20:30, 14.64s/trial, best loss: -0.930579546540648]\r 15%|█▍        | 14/96 [02:51<14:51, 10.87s/trial, best loss: -0.930579546540648]\r 16%|█▌        | 15/96 [02:57<12:46,  9.46s/trial, best loss: -0.930579546540648]\r 17%|█▋        | 16/96 [02:59<09:38,  7.24s/trial, best loss: -0.930579546540648]\r 18%|█▊        | 17/96 [03:03<07:59,  6.07s/trial, best loss: -0.930579546540648]\r 19%|█▉        | 18/96 [03:48<23:06, 17.77s/trial, best loss: -0.930579546540648]\r 20%|█▉        | 19/96 [03:55<18:40, 14.55s/trial, best loss: -0.930579546540648]\r 21%|██        | 20/96 [04:08<17:41, 13.97s/trial, best loss: -0.9352057988968743]\r 22%|██▏       | 21/96 [04:21<17:13, 13.78s/trial, best loss: -0.9352057988968743]\r 23%|██▎       | 22/96 [04:42<19:41, 15.97s/trial, best loss: -0.9352057988968743]\r 24%|██▍       | 23/96 [04:45<14:42, 12.08s/trial, best loss: -0.9352057988968743]\r 25%|██▌       | 24/96 [05:17<21:42, 18.08s/trial, best loss: -0.9352057988968743]\r 26%|██▌       | 25/96 [05:38<22:27, 18.97s/trial, best loss: -0.9352057988968743]\r 27%|██▋       | 26/96 [05:50<19:42, 16.89s/trial, best loss: -0.9352057988968743]\r 28%|██▊       | 27/96 [05:59<16:42, 14.53s/trial, best loss: -0.9352057988968743]\r 29%|██▉       | 28/96 [06:01<12:13, 10.78s/trial, best loss: -0.9352057988968743]\r 30%|███       | 29/96 [06:14<12:48, 11.48s/trial, best loss: -0.9352057988968743]\r 31%|███▏      | 30/96 [06:15<09:10,  8.34s/trial, best loss: -0.9352057988968743]\r 32%|███▏      | 31/96 [06:32<11:51, 10.94s/trial, best loss: -0.9352057988968743]\r 33%|███▎      | 32/96 [06:36<09:27,  8.87s/trial, best loss: -0.9352057988968743]\r 34%|███▍      | 33/96 [07:03<15:02, 14.33s/trial, best loss: -0.9352057988968743]\r 35%|███▌      | 34/96 [07:15<14:06, 13.65s/trial, best loss: -0.9352057988968743]\r 36%|███▋      | 35/96 [07:30<14:00, 13.78s/trial, best loss: -0.9352057988968743]\r 38%|███▊      | 36/96 [07:46<14:28, 14.47s/trial, best loss: -0.9352057988968743]\r 39%|███▊      | 37/96 [07:47<10:15, 10.44s/trial, best loss: -0.9352057988968743]\r 40%|███▉      | 38/96 [07:51<08:13,  8.51s/trial, best loss: -0.9352057988968743]\r 41%|████      | 39/96 [08:18<13:22, 14.07s/trial, best loss: -0.9352057988968743]\r 42%|████▏     | 40/96 [08:39<15:06, 16.18s/trial, best loss: -0.9352057988968743]\r 43%|████▎     | 41/96 [08:41<10:58, 11.98s/trial, best loss: -0.9352057988968743]\r 44%|████▍     | 42/96 [09:00<12:42, 14.13s/trial, best loss: -0.9352057988968743]\r 45%|████▍     | 43/96 [09:12<11:56, 13.51s/trial, best loss: -0.9352057988968743]\r 46%|████▌     | 44/96 [09:25<11:35, 13.37s/trial, best loss: -0.9352057988968743]\r 47%|████▋     | 45/96 [09:34<10:15, 12.07s/trial, best loss: -0.9352057988968743]\r 48%|████▊     | 46/96 [09:47<10:18, 12.37s/trial, best loss: -0.9352057988968743]\r 49%|████▉     | 47/96 [09:53<08:32, 10.46s/trial, best loss: -0.9352057988968743]\r 50%|█████     | 48/96 [10:19<12:06, 15.13s/trial, best loss: -0.9352057988968743]\r 51%|█████     | 49/96 [10:25<09:43, 12.41s/trial, best loss: -0.9352057988968743]\r 52%|█████▏    | 50/96 [10:41<10:07, 13.21s/trial, best loss: -0.9352057988968743]\r 53%|█████▎    | 51/96 [11:08<13:03, 17.41s/trial, best loss: -0.9352057988968743]\r 54%|█████▍    | 52/96 [11:21<11:48, 16.11s/trial, best loss: -0.9352057988968743]\r 55%|█████▌    | 53/96 [11:36<11:19, 15.79s/trial, best loss: -0.9352057988968743]\r 56%|█████▋    | 54/96 [11:38<08:10, 11.67s/trial, best loss: -0.9352057988968743]\r 57%|█████▋    | 55/96 [11:41<06:13,  9.10s/trial, best loss: -0.9352057988968743]\r 58%|█████▊    | 56/96 [11:51<06:16,  9.41s/trial, best loss: -0.9352057988968743]\r 59%|█████▉    | 57/96 [12:04<06:49, 10.50s/trial, best loss: -0.9352057988968743]\r 60%|██████    | 58/96 [12:05<04:50,  7.65s/trial, best loss: -0.9352057988968743]\r 61%|██████▏   | 59/96 [12:32<08:19, 13.49s/trial, best loss: -0.9352057988968743]\r 62%|██████▎   | 60/96 [12:45<08:00, 13.36s/trial, best loss: -0.9352057988968743]\r 64%|██████▎   | 61/96 [12:55<07:12, 12.37s/trial, best loss: -0.9352057988968743]\r 65%|██████▍   | 62/96 [13:19<08:49, 15.59s/trial, best loss: -0.9352057988968743]\r 66%|██████▌   | 63/96 [13:47<10:37, 19.33s/trial, best loss: -0.9352057988968743]\r 67%|██████▋   | 64/96 [13:53<08:10, 15.34s/trial, best loss: -0.9352057988968743]\r 68%|██████▊   | 65/96 [14:01<06:47, 13.14s/trial, best loss: -0.9352057988968743]\r 69%|██████▉   | 66/96 [14:04<05:03, 10.10s/trial, best loss: -0.9352057988968743]\r 70%|██████▉   | 67/96 [14:11<04:26,  9.19s/trial, best loss: -0.9352057988968743]\r 71%|███████   | 68/96 [14:12<03:08,  6.74s/trial, best loss: -0.9352057988968743]\r 72%|███████▏  | 69/96 [14:36<05:24, 12.03s/trial, best loss: -0.9352057988968743]\r 73%|███████▎  | 70/96 [14:37<03:47,  8.73s/trial, best loss: -0.9352057988968743]\r 74%|███████▍  | 71/96 [14:50<04:10, 10.04s/trial, best loss: -0.9352057988968743]\r 75%|███████▌  | 72/96 [15:03<04:22, 10.93s/trial, best loss: -0.9352057988968743]\r 76%|███████▌  | 73/96 [15:05<03:09,  8.26s/trial, best loss: -0.9352057988968743]\r 77%|███████▋  | 74/96 [15:18<03:33,  9.70s/trial, best loss: -0.9352057988968743]\r 78%|███████▊  | 75/96 [15:45<05:13, 14.91s/trial, best loss: -0.9352057988968743]\r 79%|███████▉  | 76/96 [15:51<04:04, 12.25s/trial, best loss: -0.9352057988968743]\r 80%|████████  | 77/96 [16:17<05:11, 16.39s/trial, best loss: -0.9352057988968743]\r 81%|████████▏ | 78/96 [16:33<04:47, 16.00s/trial, best loss: -0.9352057988968743]\r 82%|████████▏ | 79/96 [16:42<03:57, 13.95s/trial, best loss: -0.9352057988968743]\r 83%|████████▎ | 80/96 [16:53<03:29, 13.07s/trial, best loss: -0.9352057988968743]\r 84%|████████▍ | 81/96 [17:07<03:20, 13.36s/trial, best loss: -0.9352057988968743]\r 85%|████████▌ | 82/96 [17:13<02:36, 11.19s/trial, best loss: -0.9352057988968743]\r 86%|████████▋ | 83/96 [17:15<01:49,  8.44s/trial, best loss: -0.9352057988968743]\r 88%|████████▊ | 84/96 [17:35<02:23, 11.98s/trial, best loss: -0.9352057988968743]\r 89%|████████▊ | 85/96 [17:47<02:12, 12.01s/trial, best loss: -0.9352057988968743]\r 90%|████████▉ | 86/96 [18:01<02:06, 12.61s/trial, best loss: -0.9352057988968743]\r 91%|█████████ | 87/96 [18:09<01:41, 11.23s/trial, best loss: -0.9352057988968743]\r 92%|█████████▏| 88/96 [18:16<01:19,  9.97s/trial, best loss: -0.9352057988968743]\r 93%|█████████▎| 89/96 [18:50<02:00, 17.19s/trial, best loss: -0.9352057988968743]\r 94%|█████████▍| 90/96 [19:01<01:32, 15.34s/trial, best loss: -0.9352057988968743]\r 95%|█████████▍| 91/96 [19:04<00:58, 11.64s/trial, best loss: -0.9352057988968743]\r 96%|█████████▌| 92/96 [19:18<00:49, 12.35s/trial, best loss: -0.9352057988968743]\r 97%|█████████▋| 93/96 [19:20<00:27,  9.25s/trial, best loss: -0.9352057988968743]\r 98%|█████████▊| 94/96 [19:31<00:19,  9.78s/trial, best loss: -0.9352057988968743]\r 99%|█████████▉| 95/96 [19:32<00:07,  7.15s/trial, best loss: -0.9352057988968743]\r100%|██████████| 96/96 [19:38<00:00,  6.81s/trial, best loss: -0.9352057988968743]\r100%|██████████| 96/96 [19:38<00:00, 12.28s/trial, best loss: -0.9352057988968743]\nTotal Trials: 96: 96 succeeded, 0 failed, 0 cancelled.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Assessing the optimization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a7dac95-3a6c-421b-83f7-0bc31d4bbdd4"}}},{"cell_type":"code","source":["# Best model according to the ROC-AUC of K-folds CV:\nbest_run_lr = mlflow.search_runs(order_by=['metrics.val_roc_auc DESC']).iloc[0]\n\nprint(f'ROC-AUC from K-folds CV of the best run: {best_run_lr[\"metrics.val_roc_auc\"]:.4f}.')\n\n# Best hyper-parameter values:\nlr_opt_params = space_eval(lr_search_space, best_params)\nprint(f'Best hyper-parameters: {lr_opt_params}.')\n\n# Best models:\nmlflow.search_runs(order_by=['metrics.val_roc_auc DESC'])[['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time', 'end_time',\n                                                           'metrics.loss', 'metrics.val_roc_auc', 'params.C',\n                                                           'tags.mlflow.runName']].head(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9bd810fc-d5d6-49b2-bdd6-57c2a173f529"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ROC-AUC from K-folds CV of the best run: 0.9352.\nBest hyper-parameters: {'C': 0.1}.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ROC-AUC from K-folds CV of the best run: 0.9352.\nBest hyper-parameters: {'C': 0.1}.\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_id</th>\n      <th>experiment_id</th>\n      <th>status</th>\n      <th>artifact_uri</th>\n      <th>start_time</th>\n      <th>end_time</th>\n      <th>metrics.loss</th>\n      <th>metrics.val_roc_auc</th>\n      <th>params.C</th>\n      <th>tags.mlflow.runName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c55358779d1544a8bf8b15dc32e8781a</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:14:37.989000+00:00</td>\n      <td>2021-10-24 19:15:54.967000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6461e742461e4d0fa30c2e702cc208ec</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:13:49.577000+00:00</td>\n      <td>2021-10-24 19:15:27.555000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5b03728c07f6454bb7463ea12efde2fd</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:12:28.148000+00:00</td>\n      <td>2021-10-24 19:14:24.429000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>run_id</th>\n      <th>experiment_id</th>\n      <th>status</th>\n      <th>artifact_uri</th>\n      <th>start_time</th>\n      <th>end_time</th>\n      <th>metrics.loss</th>\n      <th>metrics.val_roc_auc</th>\n      <th>params.C</th>\n      <th>tags.mlflow.runName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c55358779d1544a8bf8b15dc32e8781a</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:14:37.989000+00:00</td>\n      <td>2021-10-24 19:15:54.967000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6461e742461e4d0fa30c2e702cc208ec</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:13:49.577000+00:00</td>\n      <td>2021-10-24 19:15:27.555000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5b03728c07f6454bb7463ea12efde2fd</td>\n      <td>721477946143311</td>\n      <td>FINISHED</td>\n      <td>dbfs:/databricks/mlflow-tracking/7214779461433...</td>\n      <td>2021-10-24 19:12:28.148000+00:00</td>\n      <td>2021-10-24 19:14:24.429000+00:00</td>\n      <td>-0.935206</td>\n      <td>0.935206</td>\n      <td>4</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Training the final model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e88c50e-8d70-4bfb-91fa-ac01aeaa384f"}}},{"cell_type":"code","source":["# Class to reconcile the prediction method with the trained sklearn classification model:\nclass SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n  def __init__(self, model):\n    self.model = model\n    \n  def predict(self, context, model_input):\n    return self.model.predict_proba(model_input)[:,1]\n\n# Creating the MLflow context for logging additional information about the model estimation:\nwith mlflow.start_run(run_name='lr_model_final') as lr_run_final:\n  # Creating model estimator object:\n  lr_model = LogisticRegression(penalty='l1', C=float(lr_opt_params['C']), solver='liblinear', warm_start=True)\n  \n  # Training the model and converting the estimator into a transformer:\n  lr_model.fit(X_train, y_train)\n\n  # Predictions and ROC-AUC evaluated on test data:\n  pred_test = [p[1] for p in lr_model.predict_proba(X_test)]\n  test_roc_auc = roc_auc_score(y_test, pred_test)\n  \n  # Python object of the trained model with predict method:\n  wrappedModel = SklearnModelWrapper(lr_model)\n\n  # Signature that defines the schema of the model's inputs and outputs in order to validate inputs after deployment:\n  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))\n\n  # Defining the conda environment for model serving:\n  conda_env = _mlflow_conda_env(\n        additional_conda_deps=None,\n        additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"scikit-learn=={}\".format(sk_version)],\n        additional_conda_channels=None,\n    )\n\n  # Logging the model artifact:\n  mlflow.pyfunc.log_model(\n    \"lr_model_final\",\n    python_model=wrappedModel,\n    conda_env=conda_env,\n    signature=signature\n  )\n  \n  # Logging the test ROC-AUC:\n  mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n  print(f\"\\nTest ROC-AUC: {test_roc_auc:.4f}.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b868d56-6eff-4500-8256-c2ea70bae067"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2021/10/24 19:16:21 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 19:16:22 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/f29c1a20c41c49618160d0a6a29bf58b/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  inputs = _infer_schema(model_input)\n2021/10/24 19:16:23 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/f29c1a20c41c49618160d0a6a29bf58b/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC: 0.9120.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2021/10/24 19:16:21 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2021/10/24 19:16:22 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/f29c1a20c41c49618160d0a6a29bf58b/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-b2c406c5-c86f-44d3-870b-c17324361535/lib/python3.8/site-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  inputs = _infer_schema(model_input)\n2021/10/24 19:16:23 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under dbfs:/databricks/mlflow-tracking/721477946143311/f29c1a20c41c49618160d0a6a29bf58b/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n\nTest ROC-AUC: 0.9120.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Loading the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c35270b3-e20c-4b37-806d-c591d2a28115"}}},{"cell_type":"code","source":["# Loaded model:\nlr_model_loaded = mlflow.pyfunc.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=lr_run_final.info.run_id\n  )\n)\n\n# Predictions on test data:\npred_test_loaded = lr_model_loaded.predict(X_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d67b5ec-1f90-49e1-b7e5-ff0769aaa415"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Model registry"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89d38c79-6f07-4b04-b73c-43617ae09835"}}},{"cell_type":"code","source":["# # Path to the model inside the DBFS:\n# lr_uri = lr_run_final.info.artifact_uri\n# lr_model_name = 'lr_model'\n\n# # Registering the model:\n# lr_model_version = mlflow.register_model(f'{lr_uri}/lr_model_final', lr_model_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26df0ecf-3a59-4ee0-bbff-90cea5e2c245"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### XGBoost"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d634a20-6be8-4a75-94bb-2bda139d130e"}}},{"cell_type":"code","source":["# Search space for the grid search:\nxgb_search_space = {\n  'subsample': hp.uniform('subsample', 0.5, 1.0),\n  'max_depth': hp.choice('max_depth', [i+1 for i in range(10)]),\n  'eta': hp.uniform('eta', 0.0001, 0.1)\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a46ae19-34f7-4e8f-8096-6ea8568a407b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that takes hyper-parameter values as arguments and returns the objective value for minimization:\ndef train_model(params):\n  # With MLflow autologging, hyperparameters and the trained model are automatically logged to MLflow.\n  mlflow.xgboost.autolog()\n  \n  # Creating the MLflow context for logging additional information about the model estimation:\n  with mlflow.start_run(nested=True, run_name='opt_xgb'):\n    val_roc_auc = []\n\n    # Loop over folds of data:\n    for train, val in KFold(3).split(X_train):\n      # Creating the objects containing training and validation data (inputs and labels):\n      train_data = xgb.DMatrix(data=X_train.iloc[train, :], label=y_train.iloc[train])\n      val_data = xgb.DMatrix(data=X_train.iloc[val, :], label=y_train.iloc[val])\n\n      # Creating and training the model:\n      model = xgb.train(params={'subsample': params['subsample'], 'eta': params['eta'], 'max_depth': params['max_depth'],\n                                'objective': 'binary:logistic'},\n                        dtrain=train_data, num_boost_round=500, evals=[(val_data, \"val\")], early_stopping_rounds=50)\n      \n      # Predictions and ROC-AUC evaluated on validation data:\n      pred_val = model.predict(val_data)\n      val_roc_auc.append(roc_auc_score(y_train.iloc[val], pred_val))\n\n    # ROC-AUC calculated through K-folds CV:\n    val_roc_auc = np.nanmean(val_roc_auc)\n\n    # Logging K-folds CV and test ROC-AUC:\n    mlflow.log_metric('val_roc_auc', val_roc_auc)\n    \n    # Returning the objective function for minimization:\n    return {'status': STATUS_OK, 'loss': -1*val_roc_auc}\n  \n# Defining the strategy of distributed computing:\nspark_trials = SparkTrials(parallelism=10)\n \n# MLflow context for tracking hyper-parameters tuning:\nwith mlflow.start_run(run_name='opt_xgb'):\n  best_params = fmin(\n    fn=train_model, \n    space=xgb_search_space, \n    algo=tpe.suggest, \n    max_evals=36,\n    trials=spark_trials\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16a4dfe7-5538-4a87-939b-7de98c820cf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Assessing the optimization"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6040bf48-1e34-4e5d-8ba1-3d54a88d54cf"}}},{"cell_type":"code","source":["# Best model according to the ROC-AUC of K-folds CV:\nbest_run_xgb = mlflow.search_runs(order_by=['metrics.val_roc_auc DESC']).iloc[0]\n\nprint(f'ROC-AUC from K-folds CV of the best run: {best_run_xgb[\"metrics.val_roc_auc\"]:.4f}.')\n\n# Best hyper-parameter values:\nopt_params_xgb = space_eval(xgb_search_space, best_params)\nprint(f'Best hyper-parameters: {opt_params_xgb}.')\n\n# Best models:\nmlflow.search_runs(order_by=['metrics.val_roc_auc DESC'])[['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time', 'end_time',\n                                                           'metrics.loss', 'metrics.val_roc_auc', 'params.C',\n                                                           'tags.mlflow.runName']].head(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80921566-97ff-484c-8f7d-bf562755143c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Training the final model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78822eeb-0f9e-4097-a1c1-371d7833e377"}}},{"cell_type":"code","source":["# Creating the MLflow context for logging additional information about the model estimation:\nwith mlflow.start_run(run_name='xgboost_final') as xgb_run_final:\n  # Creating the objects containing training and test data (inputs and labels):\n  train = xgb.DMatrix(data=X_train, label=y_train)\n  test = xgb.DMatrix(data=X_test, label=y_test)\n\n  # Creating and training the model:\n  xgb_model = xgb.train(params={'subsample': opt_params_xgb['subsample'], 'eta': opt_params_xgb['eta'],\n                                'max_depth': opt_params_xgb['max_depth'],\n                                'objective': 'binary:logistic'},\n                        dtrain=train, num_boost_round=500, evals=[(test, \"test\")], early_stopping_rounds=50)\n  \n  # Predictions and ROC-AUC evaluated on test data:\n  pred_test = xgb_model.predict(test)\n  test_roc_auc = roc_auc_score(y_test, pred_test)\n\n  # Signature that defines the schema of the model's inputs and outputs in order to validate inputs after deployment:\n  signature = infer_signature(X_train, xgb_model.predict(train))\n\n  # Defining the conda environment for model serving:\n  conda_env = _mlflow_conda_env(\n        additional_conda_deps=None,\n        additional_pip_deps=[\"cloudpickle=={}\".format(cloudpickle.__version__), \"xgboost=={}\".format(xgb_version)],\n        additional_conda_channels=None,\n    )\n\n  # Logging the model artifact:\n  mlflow.xgboost.log_model(artifact_path='xgboost_default_model', xgb_model=xgb_model, conda_env=conda_env, signature=signature)\n  \n  # Logging the test ROC-AUC:\n  mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n  print(f\"\\nTest ROC-AUC: {test_roc_auc:.4f}.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fe48953-70cc-4c53-a9a3-50609c6e0d58"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Loading the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e0b3903-e056-4802-a65e-85f3ca4e97f8"}}},{"cell_type":"code","source":["# Loaded model:\nxgb_model_loaded = mlflow.xgboost.load_model(\n  'runs:/{run_id}/model'.format(\n    run_id=xgb_run_final.info.run_id\n  )\n)\n\n# Predictions on test data:\npred_test_loaded = xgb_model_loaded.predict(test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f09673ec-6583-4235-aa5a-bb2b2293d8bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Model registry"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dac798cd-3ca5-429c-9901-5e57d64d4d4d"}}},{"cell_type":"code","source":["# # Path to the model inside the DBFS:\n# lr_uri = xgb_run_final.info.artifact_uri\n# xgb_model_name = 'xgb_model'\n\n# # Registering the model:\n# xgb_model_version = mlflow.register_model(f'{lr_uri}/xgb_model_final', xgb_model_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"826a693c-5cba-4790-80ad-7058b7cdd4d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n","errorSummary":"Internal error, sorry. Attach your notebook to a different cluster or restart the current cluster.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:232)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1400(ManagedSelector.java:62)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:401)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:360)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:184)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\tat java.base/java.lang.Thread.run(Thread.java:834)"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b4e9d29-b7ec-46f6-8bcc-a52b8188e7b1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Modeling with Hyperopt and MLflow","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":721477946143311}},"nbformat":4,"nbformat_minor":0}
