{"cells":[{"cell_type":"code","source":["####################################################################################################################################\n####################################################################################################################################\n#############################################################LIBRARIES##############################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17596fe1-9183-4731-833f-d1eb76b7dd6e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%pip install unidecode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78dcd5e8-0a7e-48b2-a04f-2cdddaf27c7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting unidecode\n  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\nInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.2\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting unidecode\n  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\nInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.2\nPython interpreter will be restarted.\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport time\nfrom copy import deepcopy\n\nimport re\nfrom unidecode import unidecode\n\nfrom sklearn.preprocessing import StandardScaler"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd5c67d7-ee49-456f-bcbc-2aaaf9c7b06a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["####################################################################################################################################\n####################################################################################################################################\n#######################################################FUNCTIONS AND CLASSES########################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f087809-7cfa-4352-821f-cc30fe690429"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["####################################################################################################################################\n#############################################################UTILS##################################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"435d65d0-b17b-4124-960b-1fb911206fd1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that converts epoch into date:\ndef epoch_to_date(x):\n    if np.isnan(x):\n        return np.NaN\n\n    else:\n        str_datetime = time.strftime('%d %b %Y %H:%M:%S', time.localtime(x/1000))\n        dt = datetime.strptime(str_datetime, '%d %b %Y %H:%M:%S')\n        return dt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26521016-dd39-493b-b088-6301ceb92f67"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that splits data into train and test set:\ndef train_test_split(dataframe, preserve_date=False, date_var='date', test_ratio=0.5, shuffle=False, seed=None):\n    \"\"\"\n    Function that splits data into train and test set.\n    \n    :param dataframe: complete set of data.\n    :type dataframe: dataframe.\n    :param preserve_date: indicates whether to perform split based on volume of data, but not mingling instances\n    from the same date.\n    :type preserve_date: boolean.\n    :param date_var: name of the date variable to consider during the split.\n    :type date_var: string.\n    :param seed: seed for shuffle.\n    :type seed: integer.\n    :param test_ratio: proportion of data to be allocated into test set.\n    :type test_ratio: float.\n    :param shuffle: indicates whether to shuffle data previously to the split.\n    :type shuffle: boolean.\n    \n    :return: training and test dataframes.\n    :rtype: tuple.\n    \"\"\"\n    df = dataframe.copy()\n    df.reset_index(drop=True, inplace=True)\n    \n    if shuffle:\n        df = df.sample(len(df), random_state=seed)\n    \n    if preserve_date:\n        # Number of instances by date:\n        orders_by_date = pd.DataFrame(data={\n            'date': df.date.apply(lambda x: x.date()).value_counts().index,\n            'freq': df.date.apply(lambda x: x.date()).value_counts().values}).sort_values('date')\n\n        # Accumulated number of instances by date:\n        orders_by_date['acum'] = np.cumsum(orders_by_date.freq)\n        orders_by_date['acum_share'] = [a/orders_by_date['acum'].max() for a in orders_by_date['acum']]\n\n        # Date gathering 1 - test_ratio of data:\n        last_date_train = orders_by_date.iloc[np.argmin(abs(orders_by_date['acum_share'] - (1 - test_ratio)))]['date']\n\n        # Train-test split:\n        df_test = df[df.date.apply(lambda x: x.date()) > last_date_train]\n        df_train = df[df.date.apply(lambda x: x.date()) <= last_date_train]\n    \n    else:\n        # Indexes for training and test data:\n        test_indexes = [True if i > int(len(df)*(1 - test_ratio)) else False for i in range(len(df))]\n        train_indexes = [True if i==False else False for i in test_indexes]\n\n        # Train-test split:\n        df_train = df.iloc[train_indexes, :]\n        df_test = df.iloc[test_indexes, :]\n    \n    return (df_train, df_test)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65e8d015-ce50-4092-b437-9ffe531270a8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that produces a dataframe with frequency of features by class and returns lists with features names by class:\ndef classify_variables(dataframe, vars_to_drop=[], drop_excessive_miss=True, excessive_miss=0.95,\n                       drop_no_var=True, validation_data=None, test_data=None):\n    \"\"\"\n    Function that produces a dataframe with frequency of features by class and returns lists with features names by class.\n\n    :param dataframe: reference data.\n    :type dataframe: dataframe.\n\n    :param vars_to_drop: list of support columns.\n    :type vars_to_drop: list.\n\n    :param drop_excessive_miss: flag indicating whether columns with excessive missings should be dropped out.\n    :type drop_excessive_miss: boolean.\n\n    :param excessive_miss: share of missings above which columns are dropped from the dataframes.\n    :type excessive_miss: float.\n\n    :param drop_no_var: flag indicating whether columns with no variance should be dropped out.\n    :type drop_no_var: boolean.\n\n    :param validation_data: additional data.\n    :type validation_data: dataframe.\n\n    :param test_data: additional data.\n    :type test_data: dataframe.\n\n    :return: dataframe and lists with features by class.\n    :rtype: dictionary.\n    \"\"\"\n    print(f'Initial number of features: {dataframe.drop(vars_to_drop, axis=1).shape[1]}.')\n\n    if drop_excessive_miss:\n        # Dropping features with more than 95% of missings in the training data:\n        excessive_miss_train = [c for c in dataframe.drop(vars_to_drop, axis=1) if\n                                sum(dataframe[c].isnull())/len(dataframe) > excessive_miss]\n\n        if len(excessive_miss_train) > 0:\n            dataframe.drop(excessive_miss_train, axis=1, inplace=True)\n\n            if validation_data is not None:\n                validation_data.drop(excessive_miss_train, axis=1, inplace=True)\n                \n            if test_data is not None:\n                test_data.drop(excessive_miss_train, axis=1, inplace=True)\n\n        print(f'{len(excessive_miss_train)} features were dropped for excessive number of missings!')\n        \n    # Data type of each variable:\n    type_vars = dict(zip(dataframe.drop(vars_to_drop, axis=1).dtypes.index,\n                         dataframe.drop(vars_to_drop, axis=1).dtypes.values))\n    \n    # Classifying features:\n    cat_vars = []\n    binary_vars = []\n    cont_vars = []\n\n    # Loop over variables:\n    for v in type_vars.keys():\n        # Categorical features:\n        if type_vars[v] == object:\n            cat_vars.append(v)\n\n        # Numerical features:\n        else:\n            # Binary variables:\n            if (dataframe[v].nunique() == 2) & (sorted(dataframe[v].unique()) == [0, 1]):\n                binary_vars.append(v)\n\n            # Continuous variables:\n            else:\n                cont_vars.append(v)\n        \n    if drop_no_var:\n        # Dropping features with no variance in the training data:\n        no_variance = [c for c in dataframe.drop(vars_to_drop, axis=1).drop(cat_vars,\n                                                                         axis=1) if np.nanvar(dataframe[c])==0]\n\n        if len(no_variance) > 0:\n            dataframe.drop(no_variance, axis=1, inplace=True)\n            if validation_data is not None:\n                validation_data.drop(no_variance, axis=1, inplace=True)\n                \n            if test_data is not None:\n                test_data.drop(no_variance, axis=1, inplace=True)\n\n        print(f'{len(no_variance)} features were dropped for having no variance!')\n        \n    print(f'{dataframe.drop(vars_to_drop, axis=1).shape[1]} remaining features.')\n    print('\\n')\n    \n    # Dataframe presenting the frequency of features by class:\n    feats_assess = pd.DataFrame(data={\n        'class': ['cat_vars', 'binary_vars', 'cont_vars', 'vars_to_drop'],\n        'frequency': [len(cat_vars), len(binary_vars), len(cont_vars), len(vars_to_drop)]\n    })\n    feats_assess.sort_values('frequency', ascending=False, inplace=True)\n    \n    # Dictionary with outputs from the function:\n    feats_assess_dict = {\n        'feats_assess': feats_assess,\n        'cat_vars': cat_vars,\n        'binary_vars': binary_vars,\n        'cont_vars': cont_vars\n    }\n    \n    if drop_excessive_miss:\n        feats_assess_dict['excessive_miss_train'] = excessive_miss_train\n\n    if drop_no_var:\n        feats_assess_dict['no_variance'] = no_variance\n    \n    return feats_assess_dict"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e566d335-4df8-4e13-9c53-65026be349bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that produces an assessment of the occurrence of missing values:\ndef assessing_missings(dataframe):\n    \"\"\"\n    Function that produces an assessment of the occurrence of missing values.\n\n    :param dataframe: reference data.\n    :type dataframe: dataframe.\n\n    :return: dataframe with frequency and share of missings by feature.\n    :rtype: dataframe.\n    \"\"\"\n    # Dataframe with the number of missings by feature:\n    missings_dict = dataframe.isnull().sum().sort_values(ascending=False).to_dict()\n\n    missings_df = pd.DataFrame(data={\n        'feature': list(missings_dict.keys()),\n        'missings': list(missings_dict.values()),\n        'share': [m/len(dataframe) for m in list(missings_dict.values())]\n    })\n\n    print('\\033[1mNumber of features with missings:\\033[0m {}'.format(sum(missings_df.missings > 0)) +\n          ' out of {} features'.format(len(missings_df)) +\n          ' ({}%).'.format(round((sum(missings_df.missings > 0)/len(missings_df))*100, 2)))\n    print('\\033[1mAverage number of missings:\\033[0m {}'.format(int(missings_df.missings.mean())) +\n          ' out of {} observations'.format(len(dataframe)) +\n          ' ({}%).'.format(round((int(missings_df.missings.mean())/len(dataframe))*100,2)))\n    \n    return missings_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8815cc4a-a769-47fc-b243-7963abdf1c6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that assess the number of missings in a dataframe:\ndef missings_detection(dataframe, name='df', var=None):\n    \"\"\"\"\n    Function that assess the number of missings in a dataframe\n\n    :param dataframe: dataframe for which missings should be detected.\n    :type dataframe: dataframe.\n    \n    :param name: name of dataframe.\n    :type name: string.\n    \n    :param var: name of variable whose missings should be detected (optional).\n    :type var: string.\n\n    :return: prints the number of missings when there is a positive amount of them.\n    \"\"\"\n\n    if var:\n        num_miss = dataframe[var].isnull().sum()\n        if num_miss > 0:\n            print(f'Problem - There are {num_miss} missings for \"{var}\" in dataframe {name}.')\n\n    else:\n        num_miss = dataframe.isnull().sum().sum()\n        if num_miss > 0:\n            print(f'Problem - Number of overall missings detected in dataframe {name}: {num_miss}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f94e7b76-70f4-4387-a8a1-f244c54d3488"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that forces consistency between reference (training) and additional (validation, test) data:\ndef data_consistency(dataframe, *args, **kwargs):\n    \"\"\"\n    Function that forces consistency between reference (training) and additional (validation, test) data:\n\n    The keyword arguments are expected to be dataframes whose argument names indicate the nature of the passed data. For instance,\n    'test_data=df_test' would be a dataframe with test instances.\n\n    :param dataframe: reference data.\n    :type dataframe: dataframe.\n\n    :return: dataframes with consistent data.\n    :rtype: dictionary.\n    \"\"\"\n    consistent_data = {}\n    \n    for d in kwargs.keys():\n        consistent_data[d] = kwargs[d].copy()\n        \n        # Columns absent in reference data:\n        absent_train = [c for c in kwargs[d].columns if c not in dataframe.columns]\n        \n        # Columns absent in additional data:\n        absent_test = [c for c in dataframe.columns if c not in kwargs[d].columns]\n        \n        # Creating absent columns:\n        for c in absent_test:\n            consistent_data[d][c] = 0\n    \n        # Preserving consistency between reference and additional data:\n        consistent_data[d] = consistent_data[d][dataframe.columns]\n        \n        # Checking consistency:\n        if sum([1 for r, a in zip(dataframe.columns, consistent_data[d].columns) if r != a]):\n            print('Problem - Reference and additional datasets are inconsistent!')\n        else:\n            print(f'Training and {d.replace(\"_\", \" \")} are consistent with each other.')\n    \n    return consistent_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"063939ad-2ca6-42c4-90eb-95c12b10788a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function for cleaning texts:\ndef text_clean(text, lower=True):\n    if pd.isnull(text):\n        return text\n    \n    else:\n        text = str(text)\n\n        # Removing accent:\n        text_cleaned = unidecode(text)\n        # try:\n        #     text_cleaned = unidecode(text)\n        # except AttributeError as error:\n        #     print(f'Error: {error}.')\n\n        # Removing extra spaces:\n        text_cleaned = re.sub(' +', ' ', text_cleaned)\n        \n        # Removing spaces before and after text:\n        text_cleaned = str.strip(text_cleaned)\n        \n        # Replacing spaces:\n        text_cleaned = text_cleaned.replace(' ', '_')\n        \n        # Deleting signs:\n        for m in '.,;+-!@#$%¨&*()[]{}\\\\/|':\n            if m in text_cleaned:\n                text_cleaned = text_cleaned.replace(m, '')\n\n        # Setting text to lower case:\n        if lower:\n            text_cleaned = text_cleaned.lower()\n\n        return text_cleaned"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63d04fb4-13f4-44a5-96ed-5ba1f6fdf289"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["####################################################################################################################################\n######################################################TRANSFORMATIONS###############################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"323373a5-a65c-40ff-9922-754d389b25c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class log_transformation(object):\n    \"\"\"Applies function to log-transform all variables in a dataframe except for those\n    explicitly declared. Returns the dataframe with selected variables log-transformed\n    and their respective names changed to 'L#PREVIOUS_NAME()'.\"\"\"\n\n    def __init__(self, not_log):\n        self.not_log = not_log\n        \n    def transform(self, data):\n        # Function that applies natural logarithm to numerical variables:\n        def log_func(x):\n            \"\"\"Since numerical features are not expected to assume negative values here, and since, after a sample\n            assessment, only a few negative values were identified for just a few variables, suggesting the occurrence of\n            technical issues for such observations, any negative values will be truncated to zero when performing\n            log-transformation.\"\"\"\n            if x < 0:\n                new_value = 0\n            else:\n                new_value = x\n\n            transf_value = np.log(new_value + 0.0001)\n\n            return transf_value\n        \n        # Redefining names of columns:\n        new_col = []\n        log_vars = []\n        \n        self.log_transformed = data\n        \n        # Applying logarithmic transformation to selected variables:\n        for f in list(data.columns):\n            if f in self.not_log:\n                new_col.append(f)\n            else:\n                new_col.append('L#' + f)\n                log_vars.append('L#' + f)\n                self.log_transformed[f] = data[f].apply(log_func)\n\n        self.log_transformed.columns = new_col\n        \n        print('\\033[1mNumber of numerical variables log-transformed:\\033[0m ' + str(len(log_vars)) + '.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26056406-bdf5-4a12-a181-ec15af4eeb7f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class standard_scale(object):\n    \"\"\"Fits and transforms all variables in a dataframe, except for those explicitly defined to not scale.\n    Uses 'StandardScaler' from sklearn and returns not only scaled data, but also in its dataframe original\n    format. If test data is provided, then their values will be standardized using means and variances from\n    train data.\"\"\"\n    \n    def __init__(self, not_stand):\n        self.not_stand = not_stand\n    \n    def scale(self, train, test=None):\n        # Creating standardizing object:\n        scaler = StandardScaler()\n        \n        # Calculating means and variances:\n        scaler.fit(train.drop(self.not_stand, axis=1))\n        \n        # Standardizing selected variables:\n        self.train_scaled = scaler.transform(train.drop(self.not_stand, axis=1))\n        \n        # Transforming data into dataframe and concatenating selected and non-selected variables:\n        self.train_scaled = pd.DataFrame(data=self.train_scaled,\n                                         columns=train.drop(self.not_stand, axis=1).columns)\n        self.train_scaled.index = train.index\n        self.train_scaled = pd.concat([train[self.not_stand], self.train_scaled], axis=1)\n        \n        # Test data:\n        if test is not None:\n            # # Standardizing selected variables:\n            self.test_scaled = scaler.transform(test.drop(self.not_stand, axis=1))\n            \n            # Transforming data into dataframe and concatenating selected and non-selected variables:\n            self.test_scaled = pd.DataFrame(data=self.test_scaled,\n                                            columns=test.drop(self.not_stand, axis=1).columns)\n            self.test_scaled.index = test.index\n            self.test_scaled = pd.concat([test[self.not_stand], self.test_scaled], axis=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0af3027-6fa5-40b6-9f06-956fbf354ab9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class one_hot_encoding(object):\n    \"\"\"\n    Arguments for initialization:\n        'features': list of categorical features whose categories should be selected.\n        'variance_param': parameter for selection based on the variance of a given dummy variable.\n    Methods:\n        'create_dummies': for a given training data ('categorical_train'), performs selection of dummies based on variance criterium.\n        Then, creates the same set of dummy variables for test data ('categorical_test').\n    Output objects:\n        'self.categorical_features': list of categorical features whose categories should be selected.\n        'self.variance_param': parameter for selection based on the variance of a given dummy variable.\n        'self.dummies_train': dataframe with selected dummies for training data.\n        'self.dummies_test': dataframe for test data with dummies selected from training data.\n        'self.categories_assessment': dictionary with number of overall categories, number of selected categories, and selected\n        categories for each categorical feature.\n    \"\"\"\n    def __init__(self, categorical_features,  variance_param = 0.01):  \n        self.categorical_features = categorical_features\n        self.variance_param = variance_param\n\n    def create_dummies(self, categorical_train, categorical_test = None):\n        self.dummies_train = pd.DataFrame(data=[])\n        self.dummies_test = pd.DataFrame(data=[])\n        self.categories_assessment = {}\n        \n        # Loop over categorical features:\n        for f in self.categorical_features:\n            # Training data:\n            # Creating dummy variables:\n            dummies_cat = pd.get_dummies(categorical_train[f]) \n            dummies_cat.columns = ['C#' + f + '#' + str.upper(str(c)) for c in dummies_cat.columns]\n\n            # Selecting dummies_cat depending on their variance:\n            selected_cat = [d for d in dummies_cat.columns if dummies_cat[d].var() > self.variance_param]\n\n            # Dataframe with dummy variables for all categorical features (training data):\n            self.dummies_train = pd.concat([self.dummies_train, dummies_cat[selected_cat]], axis=1)\n            \n            # Assessing categories:\n            self.categories_assessment[f] = {\n                \"num_categories\": len(dummies_cat.columns),\n                \"num_selected_categories\": len(selected_cat),\n                \"selected_categories\": selected_cat\n            }\n\n            if categorical_test is not None:\n                # Test data:\n                dummies_cat = pd.get_dummies(categorical_test[f])\n                dummies_cat.columns = ['C#' + f + '#' + str.upper(str(c)) for c in dummies_cat.columns]\n\n                # Checking if all categories selected from training data also exist for test data:\n                for c in selected_cat:\n                    if c not in dummies_cat.columns:\n                        dummies_cat[c] = [0 for i in range(len(dummies_cat))]\n\n                # Dataframe with dummy variables for all categorical features (test data):\n                self.dummies_test = pd.concat([self.dummies_test, dummies_cat[selected_cat]], axis=1)\n\n                # Preserving columns order as the same for training data:\n                self.dummies_test = self.dummies_test[list(self.dummies_train.columns)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49e3b54f-a42c-41c9-8afd-1f237eb29131"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that recriates original missing values from dummy variable of missing value status:\ndef recreate_missings(var, missing_var):\n    \"\"\"\n    Arguments:\n        'var': variable (series, array, or list) to impute missing values.\n        'missing_var': variable (series, array, or list) that indicates missing data.\n        Attention: both arguments should have the same lenght and should have the same index.\n    Outputs:\n        A list with missing values recreated (if any exists in 'missing_var').\n    \"\"\"\n    var_list = list(var)\n    missing_var_list = list(missing_var)\n    new_values = []\n    \n    # Loop over observations:\n    for i in range(len(var_list)):\n        if missing_var_list[i] == 1:\n            new_values.append(np.NaN)\n        else:\n            new_values.append(var_list[i])\n    \n    return new_values"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2644686-4ad2-4ed7-9d78-45a714303766"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that treats missing values by imputing 0 whenever they are found:\ndef impute_missing(var):\n    \"\"\"\n    Arguments:\n        'var': variable (series, array, or list) whose missing values should be replaced by 0.\n    Outputs:\n        A dictionary containing a list of values for the variable after missing values treatment, and a list of\n        missing value status.\n    \"\"\"\n    var_list = list(var)\n    new_values = []\n    missing_var = []\n    \n    # Loop over observations:\n    for value in var_list:\n        if np.isnan(value):\n            new_values.append(0)\n            missing_var.append(1)\n        else:\n            new_values.append(value)\n            missing_var.append(0)\n    \n    return {'var': new_values, 'missing_var': missing_var}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"588fc5c7-9b2e-4110-a7a0-32e3643a6837"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that applies log-transformation:\ndef applying_log_transf(dataframe, not_log):\n    \"\"\"\n    Function that applies log-transformation based upon the 'log_transformation' class.\n\n    :param dataframe: reference data.\n    :type dataframe: dataframe.\n\n    :param not_log: list with features names that should not be log-transformed.\n    :type not_log: list.\n\n    :return: dataframe with numerical variables log-transformed.\n    :rtype: dataframe.\n    \"\"\"\n    log_dataframe = dataframe.copy()\n    \n    # Assessing missing values (before logarithmic transformation):\n    num_miss = log_dataframe.isnull().sum().sum()\n    \n    # Applying the log-transformation:\n    log_transf = log_transformation(not_log=not_log)\n    log_transf.transform(log_dataframe)\n    log_dataframe = log_transf.log_transformed\n\n    # Assessing missing values (after logarithmic transformation):\n    num_miss_log = log_dataframe.isnull().sum().sum()\n\n    # Checking consistency in the number of missings:\n    if num_miss_log != num_miss:\n        print('Problem - Inconsistent number of overall missings!')\n        \n    # Assessing consistency of dimensions:\n    if not log_dataframe.shape == dataframe.shape:\n        print('Problem - Inconsistent dimensions!')\n        print(f'Shape before scaling: {log_dataframe.shape}.\\nShape after scaling: {dataframe.shape}.')\n    \n    return log_dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae026091-df5e-4c1d-b0f0-c88fe11aa52b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that applies the standard scaling transformation:\ndef applying_standard_scale(training_data, not_stand, *args, **kwargs):\n    \"\"\"\n    Function that applies the standard scaling transformation based upon the 'standard_scale' class.\n\n    The keyword arguments are expected to be dataframes whose argument names indicate the nature of the passed data. For instance,\n    'test_data=df_test' would be a dataframe with test instances.\n\n    :param training_data: reference data.\n    :type training_data: dataframe.\n\n    :param not_stand: list with features names that should not be standardized.\n    :type not_stand: list.\n\n    :return: dataframes with numerical variables standardized.\n    :rtype: dictionary.\n    \"\"\"\n    scaled_data = {}\n    \n    print('\\033[1mStandard scaling training data...\\033[0m')\n\n    stand_scale = standard_scale(not_stand = not_stand)\n    stand_scale.scale(train = training_data, test = None)\n\n    scaled_data['training_data'] = stand_scale.train_scaled\n\n    # Assessing consistency of dimensions:\n    if not scaled_data['training_data'].shape == training_data.shape:\n        print('Problem - Inconsistent dimensions!')\n        print(f'Shape before scaling: {training_data.shape}.\\nShape after scaling: {scaled_data[\"training_data\"].shape}.')\n\n    # Assessing consistency of missing values:\n    num_miss = training_data.isnull().sum().sum()\n    num_miss_scaled = scaled_data['training_data'].isnull().sum().sum()\n\n    if num_miss_scaled != num_miss:\n        print('Problem - Inconsistent number of overall missings!')\n\n    # Loop over additional data:\n    for d in kwargs.keys():\n        print(f'\\033[1mStandard scaling {d.replace(\"_\", \" \")}...\\033[0m')\n        stand_scale = standard_scale(not_stand = not_stand)\n        stand_scale.scale(train = training_data, test = kwargs[d])\n\n        scaled_data[d] = stand_scale.test_scaled\n\n        # Assessing consistency of dimensions:\n        if not scaled_data[d].shape == kwargs[d].shape:\n            print('Problem - Inconsistent dimensions!')\n            print(f'Shape before scaling: {training_data.shape}.\\nShape after scaling: {scaled_data[d].shape}.')\n\n        # Assessing consistency of missing values:\n        num_miss = kwargs[d].isnull().sum().sum()\n        num_miss_scaled = scaled_data[d].isnull().sum().sum()\n\n        if num_miss != num_miss_scaled:\n            print('Problem - Inconsistent number of overall missings!')\n    \n    return scaled_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85e3cc5b-eea7-4b0c-9ddf-d12b79bb4e4b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that treats missing values:\ndef treating_missings(dataframe, cat_vars, drop_vars):\n    \"\"\"\n    Function that treats missing values both from categorical and numerical features. The last set of features have their missings\n    treated using the function 'impute_missing'.\n\n    :param dataframe: reference data.\n    :type dataframe: dataframe.\n\n    :param cat_vars: list with categorical variables.\n    :type cat_vars: list.\n\n    :param drop_vars: list with support variables.\n    :type drop_vars: list.\n\n    :return: dataframe with treated missing values.\n    :rtype: dataframe.\n    \"\"\"\n    treated_dataframe = dataframe.copy()\n    \n    num_miss = treated_dataframe.isnull().sum().sum()\n    \n    # Loop over categorical features:\n    num_miss_cat_treat = 0\n    for f in cat_vars:\n        treated_dataframe[f] = ['missing_value' if pd.isnull(x) else x for x in treated_dataframe[f]]\n        num_miss_cat_treat += sum([1 for x in treated_dataframe[f] if x == 'missing_value'])\n    \n    # Loop over non-categorical features:\n    for f in treated_dataframe.drop(drop_vars, axis=1).drop(cat_vars, axis=1):\n        # Checking if there is missing values for a given feature:\n        if treated_dataframe[f].isnull().sum() > 0:\n            check_missing = impute_missing(treated_dataframe[f])\n            treated_dataframe[f] = check_missing['var']\n            treated_dataframe['NA#' + f.replace('L#', '')] = check_missing['missing_var']\n\n    num_miss_treat = int(sum([sum(treated_dataframe[f]) for f in treated_dataframe.columns if 'NA#' in f]))\n    num_miss_treat = num_miss_treat + num_miss_cat_treat\n\n    if num_miss_treat != num_miss:\n        print('Problem - Inconsistent number of overall missings!')\n        print(f'Number of missings before treatment: {num_miss}.')\n        print(f'Number of missings after treatment: {num_miss_treat}.')\n\n    if treated_dataframe.isnull().sum().sum() > 0:\n        print('Problem - Number of overall missings detected: ' +\n              str(treated_dataframe.isnull().sum().sum()) + '.')\n    \n    return treated_dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60b749fd-6a41-4db3-b090-297d3865ec56"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that applies one-hot encoding transformation over categorical variables:\ndef applying_one_hot(training_data, cat_vars, variance_param=0.01, *args, **kwargs):\n    \"\"\"\n    Function that applies one-hot encoding transformation over categorical variables based upon the 'one_hot_encoding' class.\n\n    The keyword arguments are expected to be dataframes whose argument names indicate the nature of the passed data. For instance,\n    'test_data=df_test' would be a dataframe with test instances.\n\n    :param training_data: reference data.\n    :type training_data: dataframe.\n    \n    :param cat_vars: list with categorical variables.\n    :type cat_vars: list.\n\n    :return: dataframes with categorical variables transformed into dummy variables.\n    :rtype: dictionary.\n    \"\"\"\n    dummies_df = {}\n    transf_data = {}\n    \n    # Create object for one-hot encoding:\n    categorical_transf = one_hot_encoding(categorical_features = cat_vars, variance_param = variance_param)\n\n    # Treating texts:\n    for f in cat_vars:\n        training_data[f] = training_data[f].apply(text_clean)\n\n    if kwargs:\n        for d in kwargs:\n            # Treating texts:\n            for f in cat_vars:\n                kwargs[d][f] = kwargs[d][f].apply(text_clean)\n\n            # Creating dummies:\n            categorical_transf.create_dummies(categorical_train = training_data[cat_vars],\n                                              categorical_test = kwargs[d][cat_vars])\n\n            # Additional data:\n            dummies_df[d] = categorical_transf.dummies_test\n            dummies_df[d].index = kwargs[d].index\n\n            # Concatenating dummy variables with remaining columns and dropping out original categorical features:\n            transf_data[d] = pd.concat([kwargs[d], dummies_df[d]], axis=1)\n            transf_data[d].drop(cat_vars, axis=1, inplace=True)\n\n    else:\n        # Creating dummies:\n        categorical_transf.create_dummies(categorical_train = training_data[cat_vars])\n        \n    # Training data:\n    dummies_df['training_data'] = categorical_transf.dummies_train\n    dummies_df['training_data'].index = training_data.index\n\n    # Concatenating dummy variables with remaining columns and dropping out original categorical features:\n    transf_data['training_data'] = pd.concat([training_data, dummies_df['training_data']], axis=1)\n    transf_data['training_data'].drop(cat_vars, axis=1, inplace=True)\n    \n    print(f'\\033[1mNumber of categorical features:\\033[0m {len(cat_vars)}')\n    print(f'\\033[1mNumber of overall selected dummies:\\033[0m {dummies_df[\"training_data\"].shape[1]}.')\n    \n    return transf_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847b91e5-3382-49e4-b53c-f24955d940c4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["####################################################################################################################################\n########################################################PRE-PROCESS#################################################################"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63aae9db-6b57-4629-8448-8c0a9b3f48c7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that applies distinct functions and classes in order to pre-process training and test data:\ndef pre_process(training_data, test_data, vars_to_drop, log_transform=True, standardize=True):\n    \"\"\"\n    Function that applies distinct functions and classes in order to pre-process training and test data.\n    \n    The implemented procedures are log-transformation and standard scaling of numerical variables, missing values\n    treatment, and one-hot encoding for transforming categorical variables.\n    \n    :param training_data: training data.\n    :type training_data: dataframe.\n    :param test_data: test data.\n    :type test_data: dataframe.\n    :param vars_to_drop: collection of variables that should not be considered during data pre-processing.\n    :type vars_to_drop: list.\n    :param log_transform: indicates whether to log-transform numerical variables.\n    :type log_transform: boolean.\n    :param standardize: indicates whether to standard scale numerical variables.\n    :type standardize: boolean.\n    \n    :return: training and test data pre-processed.\n    :rtype: tuple.\n    \"\"\"\n    \n    df_train = training_data.copy()\n    df_test = test_data.copy()\n    drop_vars = deepcopy(vars_to_drop)\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mCLASSIFYING FEATURES AND EARLY SELECTION\\033[0m')\n    print('\\n')\n    \n    class_variables = classify_variables(dataframe=df_train, vars_to_drop=drop_vars, test_data=df_test)\n    \n    # Lists of variables:\n    cat_vars = class_variables['cat_vars']\n    binary_vars = class_variables['binary_vars']\n    cont_vars = class_variables['cont_vars']\n\n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mASSESSING MISSING VALUES\\033[0m')\n    print('\\n')\n    \n    # Assessing missing values:\n    print('\\033[1mTraining data:\\033[0m')\n    missings_train = assessing_missings(dataframe=df_train)\n    print('\\n\\033[1mTest data:\\033[0m')\n    missings_test = assessing_missings(dataframe=df_test)\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mAPPLYING LOGARITHMIC TRANSFORMATION OVER NUMERICAL DATA\\033[0m')\n    print('\\n')\n\n    # Variables that should not be log-transformed:\n    not_log = [c for c in df_train.columns if c not in cont_vars]\n\n    if log_transform:\n        print('\\033[1mTraining data:\\033[0m')\n        df_train = applying_log_transf(dataframe=df_train, not_log=not_log)\n\n        print('\\033[1mTest data:\\033[0m')\n        df_test = applying_log_transf(dataframe=df_test, not_log=not_log)\n        print('\\n')\n\n\n    else:\n        print('\\033[1mNo transformation performed!\\033[0m')\n        print('\\n')\n\n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mAPPLYING STANDARD SCALE TRANSFORMATION OVER NUMERICAL DATA\\033[0m')\n    print('\\n')\n\n    # Inputs that should not be standardized:\n    not_stand = [c for c in df_train.columns if c.replace('L#', '') not in cont_vars]\n\n    if standardize:\n        scaled_data = applying_standard_scale(training_data=df_train, not_stand=not_stand,\n                                              test_data=df_test)\n        df_train_scaled = scaled_data['training_data']\n        df_test_scaled = scaled_data['test_data']\n\n    else:\n        df_train_scaled = df_train.copy()\n        df_test_scaled = df_test.copy()\n\n        print('\\033[1mNo transformation performed!\\033[0m')\n\n    print('\\n')\n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mTREATING MISSING VALUES\\033[0m')\n    print('\\n')\n\n    print('\\033[1mTreating missing values of training data...\\033[0m')\n    df_train_scaled = treating_missings(dataframe=df_train_scaled, cat_vars=cat_vars,\n                                        drop_vars=drop_vars)\n\n    print('\\033[1mTreating missing values of test data...\\033[0m')\n    df_test_scaled = treating_missings(dataframe=df_test_scaled, cat_vars=cat_vars,\n                                       drop_vars=drop_vars)\n\n    print('\\n')\n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mTRANSFORMING CATEGORICAL FEATURES\\033[0m')\n    print('\\n')\n    \n    transf_data = applying_one_hot(df_train_scaled, cat_vars, test_data=df_test_scaled)\n    df_train_scaled = transf_data['training_data']\n    df_test_scaled = transf_data['test_data']\n\n    print(f'\\033[1mShape of df_train_scaled:\\033[0m {df_train_scaled.shape}.')\n    print(f'\\033[1mShape of df_test_scaled:\\033[0m {df_test_scaled.shape}.')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\033[1mFINAL ASSESSMENT OF MISSINGS AND CHECKING DATASETS CONSISTENCY\\033[0m')\n    print('\\n')\n    \n    # Assessing missing values (training data):\n    missings_detection(df_train_scaled, name=f'df_train_scaled')\n\n    # Assessing missing values (test data):\n    missings_detection(df_test_scaled, name=f'df_test_scaled')\n    \n    # Checking datasets structure:\n    df_test_scaled = data_consistency(dataframe=df_train_scaled,\n                                      test_data=df_test_scaled)['test_data']\n    \n    print('---------------------------------------------------------------------------------------------------------')\n    print('\\n')\n    \n    return df_train, df_test, df_train_scaled, df_test_scaled"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c5908cc-0f89-4631-9e91-626c4653743c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8d64d85-4ef2-40af-8b91-b53fc171c88a"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"FunctionsClasses","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4380952712113784}},"nbformat":4,"nbformat_minor":0}
