{"cells":[{"cell_type":"markdown","source":["## Data modeling with MLlib"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d810881f-7491-4886-b5ef-2f9f1f7cbaec"}}},{"cell_type":"markdown","source":["In this notebook, MLlib functions and classes are used to develop a machine learning application for predicting a binary response variable. MLlib works on top of **Spark** technology, so all data manipulation, pre-processing and modeling here are implemented using **distributed computing**. Some codes are extracted from the previous notebook, \"Data Manipulation with PySpark\". Again, the code paradigm is procedural for demonstrating how to use Spark APIs for machine learning. Both of these notebooks aim to illustrate how to deal with **big data** through Python language. Even that Spark (raw) API slightly differs to PySpark, codes here can be easily extended to other approaches for dealing with large amounts of data.\n\nMLlib has a comprehensive API for all steps of model development: from [data pre-processing](https://spark.apache.org/docs/latest/ml-features.html) ([extraction](https://spark.apache.org/docs/latest/ml-features.html#feature-extractors), [transformation](https://spark.apache.org/docs/latest/ml-features.html#feature-transformers) and [selection](https://spark.apache.org/docs/latest/ml-features.html#feature-selectors) of features) to [model training](https://spark.apache.org/docs/latest/ml-classification-regression.html) and [tuning](https://spark.apache.org/docs/latest/ml-tuning.html), besides of model evaluation. The construction of [pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) is very straightforward with MLlib, as estimators and transformers are sequentially declared.\n\nThis notebook covers most of these topics: first, data is read and split into training and test sets; then, several data pre-processing steps are implemented, using direct calculations through SQL or making use of classes from MLlib. Finally, model estimation takes place: default and optimized models are trained based on logistic regression and GBM algorithms."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a37b601b-8ba9-4ed7-a0a7-91bccefa508a"}}},{"cell_type":"markdown","source":["--------------"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6318fb2-6cc9-4d4a-9ba2-231fdcadb21f"}}},{"cell_type":"markdown","source":["## Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e31a51a-c718-4446-8796-7f0df7db1384"}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n\nfrom pyspark.sql import functions as func\nfrom pyspark.sql.types import TimestampType\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import LogisticRegression, GBTClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce3f684a-e7e1-4121-ade8-1de2a4034b10"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Settings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"803e6063-3032-48d5-a500-122c383beec9"}}},{"cell_type":"code","source":["# Declare whether outcomes should be exported:\nexport = False\n\n# Number of unique values above which log transformation is applied:\nthres_to_log = 100\n\n# Share of missings above which features are disregarded:\nthres_missings = 0.95\n\n# Share of data above which categories are kept to one-hot encoding:\nthres_cat_repr = 0.01"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b75e8f4-4f30-454b-9df4-d529f35da265"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Importing data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c8bcded-b3ef-4637-8f8e-0142a0788f3e"}}},{"cell_type":"code","source":["df = spark.read.format('csv').\\\n           options(header='true', delimiter = ',', inferSchema='true').\\\n           load(\"/FileStore/shared_uploads/matheusf.rosso@gmail.com/fraud_data_sample.csv\")\n\n# Sorting the dataframe by date:\ndf = df.sort(func.col('epoch').asc())\n\n# Converting epoch into datetime:\ndf = df.withColumn('epoch', (func.col('epoch')/func.lit(1000)))\ndf = df.withColumn('datetime', func.date_format(df.epoch.cast(dataType=TimestampType()), \"yyyy-MM-dd HH:mm:ss\"))\n\nfirst_date = df.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['first_date']\nlast_date = df.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['last_date']\n\nprint(f'Type of df: {type(df)}.')\nprint(f'Shape of df: ({df.count()}, {len(df.columns)}).')\nprint(f'Number of unique instances: {df.select(\"order_id\").distinct().count()}.')\nprint(f'Time interval: ({first_date}, {last_date}).')\n\n# Support variables:\ndrop_vars = ['y', 'order_amount', 'store_id', 'order_id', 'status', 'epoch', 'datetime', 'weight']\n\n# df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87036667-7011-4e58-bfb1-792477656792"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of df: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of df: (8361, 1429).\nNumber of unique instances: 8361.\nTime interval: (2021-05-17 15:01:00, 2021-06-27 23:55:01).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of df: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of df: (8361, 1429).\nNumber of unique instances: 8361.\nTime interval: (2021-05-17 15:01:00, 2021-06-27 23:55:01).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Train-test split"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c8bd686-1579-441e-bbfa-db0da9a81199"}}},{"cell_type":"code","source":["# Number of instances by date:\norders_by_date = df.withColumn('date', func.to_date(func.col('datetime'))).\\\n                    select('date', 'order_id').groupBy('date').agg(func.count('order_id').alias('freq'))\norders_by_date = orders_by_date.toPandas().sort_values('date', ascending=True)\n\n# Accumulated number of instances by date:\norders_by_date['acum'] = np.cumsum(orders_by_date.freq)\norders_by_date['acum_share'] = [a/orders_by_date['acum'].max() for a in orders_by_date['acum']]\n\n# Date with 75% of volume of data:\nlast_date_train = orders_by_date.iloc[np.argmin(abs(orders_by_date['acum_share'] - 0.75))]['date']\n\n# Train-test split:\ndf_test = df.filter(func.col('datetime') > last_date_train)\ndf_train = df.filter(func.col('datetime') <= last_date_train)\n\n# Instances identification for training and test data:\ntrain_orders, test_orders = ([r['order_id'] for r in df_train.select('order_id').collect()],\n                             [r['order_id'] for r in df_test.select('order_id').collect()])\n\nfirst_date_train = df_train.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['first_date']\nlast_date_train = df_train.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['last_date']\nfirst_date_test = df_test.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['first_date']\nlast_date_test = df_test.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['last_date']\n\nprint(f'Shape of df_train: ({df_train.count()}, {len(df_train.columns)}).')\nprint(f'Number of unique instances (training data): {df_train.select(\"order_id\").distinct().count()}.')\nprint(f'Time interval (training data): ({first_date_train}, {last_date_train}).\\n')\n\nprint(f'Shape of df_test: ({df_test.count()}, {len(df_test.columns)}).')\nprint(f'Number of unique instances (test data): {df_test.select(\"order_id\").distinct().count()}.')\nprint(f'Time interval (test data): ({first_date_test}, {last_date_test}).')\n\n# df_train.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1703a9fc-7627-4d91-b9f9-ddc1e0bc447e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Shape of df_train: (6313, 1429).\nNumber of unique instances (training data): 6313.\nTime interval (training data): (2021-05-17 15:01:00, 2021-06-09 23:56:06).\n\nShape of df_test: (2048, 1429).\nNumber of unique instances (test data): 2048.\nTime interval (test data): (2021-06-10 00:01:08, 2021-06-27 23:55:01).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Shape of df_train: (6313, 1429).\nNumber of unique instances (training data): 6313.\nTime interval (training data): (2021-05-17 15:01:00, 2021-06-09 23:56:06).\n\nShape of df_test: (2048, 1429).\nNumber of unique instances (test data): 2048.\nTime interval (test data): (2021-06-10 00:01:08, 2021-06-27 23:55:01).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Data pre-processing"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7303bb4-9e8d-4f5c-8774-44779b2d536b"}}},{"cell_type":"code","source":["# Creating a temporary view table (training data):\ndf_train.createOrReplaceTempView(\"training_data\")\n\n# Creating a temporary view table (test data):\ndf_test.createOrReplaceTempView(\"test_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"761b4369-2e24-4356-8283-bb999c21e217"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Data types of features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2ae7d3d-fcce-490a-b36b-92225eca065c"}}},{"cell_type":"code","source":["# Lists with categorical and numerical variables:\ncat_vars = [t[0] for t in df_train.dtypes if (t[1]=='string') & (t[0] not in drop_vars)]\nnum_vars = [t[0] for t in df_train.dtypes if (t[1]!='string') & (t[0] not in drop_vars)]\n\n# Number of unique values for a sample of data:\nunique_values_sample = df_train.select(num_vars).sample(fraction=0.05).toPandas().nunique()\n\n# Name of numerical variables with sufficient variation for log transformation:\nunique_values_sample = pd.DataFrame(data={\n  'feature': unique_values_sample.index, 'num_uniques': unique_values_sample.values\n})\nto_log = list(unique_values_sample[unique_values_sample.num_uniques>thres_to_log]['feature'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"684fd4e4-21c0-4288-a6d2-b5a5ca0c4062"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Assessing missing values"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fcf6759-e00e-4d96-ae44-467f768ce289"}}},{"cell_type":"markdown","source":["#### Training data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c0476d0-0a04-4640-9537-c559a3b11176"}}},{"cell_type":"code","source":["# Dataframe with frequency of missings by feature:\nmissings_df_train = df_train.select([func.count(func.when(func.col(c).isNull(), c)).alias(c) for c in df_train.columns])\n\n# Converting the dataframe into pandas for ready use:\nmissings_df_train = missings_df_train.toPandas().T.reset_index(drop=False)\nmissings_df_train.columns = ['feature', 'missings']\n\n# Share of observations with missing value for each feature:\nnum_obs_train = df_train.count()\nmissings_df_train['share'] = missings_df_train['missings'].apply(lambda x: x/num_obs_train)\n\n# List of variables with missings:\nvars_missings_train = list(missings_df_train[missings_df_train.missings > 0]['feature'])\n\nprint(f'Number of features with missings: {sum(missings_df_train[\"missings\"] > 0)} ({np.nanmean(missings_df_train[\"missings\"] > 0)*100:.2f}%).')\nprint(f'Average number of missings: {missings_df_train[\"missings\"].mean():.0f} ({(missings_df_train[\"missings\"].mean()/num_obs_train)*100:.2f}%).')\n# missings_df_train.sample(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72265198-34c1-4652-b943-67048fe59ee7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of features with missings: 291 (20.36%).\nAverage number of missings: 353 (5.59%).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of features with missings: 291 (20.36%).\nAverage number of missings: 353 (5.59%).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"435324d7-a02b-4716-b3fe-1c421ddc0f50"}}},{"cell_type":"code","source":["# Dataframe with frequency of missings by feature:\nmissings_df_test = df_test.select([func.count(func.when(func.col(c).isNull(), c)).alias(c) for c in df_test.columns])\n\n# Converting the dataframe into pandas for ready use:\nmissings_df_test = missings_df_test.toPandas().T.reset_index(drop=False)\nmissings_df_test.columns = ['feature', 'missings']\n\n# Share of observations with missing value for each feature:\nnum_obs_test = df_test.count()\nmissings_df_test['share'] = missings_df_test['missings'].apply(lambda x: x/num_obs_test)\n\nprint(f'Number of features with missings: {sum(missings_df_test[\"missings\"] > 0)} ({np.nanmean(missings_df_test[\"missings\"] > 0)*100:.2f}%).')\nprint(f'Average number of missings: {missings_df_test[\"missings\"].mean():.0f} ({(missings_df_test[\"missings\"].mean()/num_obs_test)*100:.2f}%).')\n# missings_df_test.sample(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6cc882b-0a8e-44e8-aa04-61e3748add06"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of features with missings: 164 (11.48%).\nAverage number of missings: 102 (4.97%).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of features with missings: 164 (11.48%).\nAverage number of missings: 102 (4.97%).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Early selection of features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e186ace5-511b-4acc-a6f0-72e01505d21f"}}},{"cell_type":"code","source":["# Filtering variables with excessive number of missings:\nexcessive_missings_train = list(missings_df_train[missings_df_train.share > thres_missings]['feature'])\n\n# Variance of each numerical variable:\nvariance_train = df_train.select([func.variance(func.col(c)).alias(c) for c in num_vars])\n\n# Converting the dataframe into pandas for ready use:\nvariance_train = variance_train.toPandas().T.reset_index(drop=False)\nvariance_train.columns = ['feature', 'variance']\n\n# Filtering variables with no variance:\nno_variance_train = list(variance_train[variance_train.variance.apply(lambda x: round(x, 6))==0]['feature'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06e333fb-7776-4ddc-9633-e1e65053ddb2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["initial_vars = df_train.select([c for c in df_train.columns if c not in drop_vars]).columns\nprint(f'Initial number of features: {len(initial_vars)}.')\nprint(f'{len(excessive_missings_train)} features were dropped for excessive number of missings!')\nprint(f'{len(no_variance_train)} features were dropped for having no variance!')\nprint(f'{len(initial_vars) - len(excessive_missings_train) - len(no_variance_train)} remaining features')\n\ndisregard_vars = []\ndisregard_vars.extend(excessive_missings_train)\ndisregard_vars.extend(no_variance_train)\n\n# Dropping variables with exceissive number of missings or no variance:\nnum_vars = [c for c in num_vars if c not in disregard_vars]\nto_log = [c for c in to_log if c not in disregard_vars]\ncat_vars = [c for c in cat_vars if c not in disregard_vars]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3110f6bd-b798-44bb-af12-0883339464e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Initial number of features: 1421.\n3 features were dropped for excessive number of missings!\n361 features were dropped for having no variance!\n1057 remaining features\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Initial number of features: 1421.\n3 features were dropped for excessive number of missings!\n361 features were dropped for having no variance!\n1057 remaining features\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Logarithmic transformation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb9c25e9-4b0e-433a-9eb0-ffa632f8eec2"}}},{"cell_type":"code","source":["# Query clause for log transforming numerical data:\nlog_transf = \"CASE WHEN `{feat}` IS NOT NULL THEN ln(`{feat}`+0.0001) ELSE 0 END AS `L#{feat}`\"\nlog_transf = ', '.join([log_transf.format(feat=c) for c in to_log])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b85c3da-1df1-4c26-8611-b405c5606a74"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Standard scale transformation (first alternative)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7da09e3b-53f5-4fd6-9413-46722fcd3cb4"}}},{"cell_type":"markdown","source":["This first approach to generate data standard scaled considers the following order of operations: logarithmic transformation, standard scaling, missing values treatment and transformation of categorical variables. The first three procedures are implemented through SQL, and only the last one uses MLlib API."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3160478-544b-4171-a61d-487f350e3748"}}},{"cell_type":"code","source":["# # Clause for extracting numerical variables:\n# num_vars_clause = \"`{feat}`\"\n# num_vars_clause = ', '.join([num_vars_clause.format(feat=c) for c in num_vars if c not in to_log])\n\n# # Clause for extracting numerical variables that should be log-transformed:\n# log_vars_clause = \"CASE WHEN `{feat}` IS NOT NULL THEN ln(`{feat}`+0.0001) ELSE `{feat}` END AS `L#{feat}`\"\n# log_vars_clause = ', '.join([log_vars_clause.format(feat=c) for c in to_log])\n\n# # Querying numerical variables from the training data:\n# training_num_data = spark.sql(f'SELECT order_id, {log_vars_clause}, {num_vars_clause} FROM training_data')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab496859-bfa4-4060-9430-a6a0b7a89aa5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# # Mean and standard deviation of each numerical variable:\n# mean_train = training_num_data.select([func.mean(func.col(c)).alias(c) for c in [c for c in training_num_data.columns if c!='order_id']])\n# std_train = training_num_data.select([func.stddev(func.col(c)).alias(c) for c in [c for c in training_num_data.columns if c!='order_id']])\n\n# # Converting dataframes into dictionaries for ready use:\n# mean_train = mean_train.toPandas().T.reset_index(drop=False)\n# mean_train.columns = ['feature', '_mean']\n# mean_train['_mean'] = mean_train['_mean'].apply(lambda x: x*-1)\n# mean_train['sign'] = mean_train['_mean'].apply(lambda x: '-' if x < 0 else '+')\n# sign_train = dict(zip(mean_train['feature'], mean_train['sign']))\n# mean_train = dict(zip(mean_train['feature'], [abs(x) for x in mean_train['_mean']]))\n\n# std_train = std_train.toPandas().T.reset_index(drop=False)\n# std_train.columns = ['feature', 'std']\n# std_train = dict(zip(std_train['feature'], std_train['std']))\n\n# # Query clause for log transforming and standardizing numerical data:\n# log_transf = \"CASE WHEN `{feat}` IS NOT NULL THEN (ln(`{feat}`+0.0001){sign}{avg})/{std} ELSE 0 END AS `L#{feat}`\"\n# log_transf = ', '.join([log_transf.format(feat=c, avg=round(mean_train[f'L#{c}'], 4), sign=sign_train[f'L#{c}'], std=round(std_train[f'L#{c}'], 4)) for\n#                         c in to_log])\n\n# # Query clause to standardize numerical variables and impute missings (except from those that had already been treated during log transformation):\n# impute_missings_num = \"CASE WHEN `{feat}` IS NULL THEN 0 ELSE (`{feat}`{sign}{avg})/{std} END AS `{feat}`\"\n# impute_missings_num = ', '.join([impute_missings_num.format(feat=c, avg=round(mean_train[c], 4), sign=sign_train[c], std=round(std_train[c], 4)) for\n#                                  c in num_vars if c not in to_log])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8edb073f-2085-4b4a-be33-76bd08ab97fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Treating missing values"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e24b19a1-c1e1-4096-a670-48a31314f46e"}}},{"cell_type":"markdown","source":["#### Categorical variables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7564d11e-d5ea-4fd3-b119-4dbc4ab0f622"}}},{"cell_type":"code","source":["repr_categories = {}\n\n# Loop over categorical variables:\nfor c in cat_vars:\n  # Categories whose share is larger than 0.01:\n  repr_categories[c] = [r[c] for r in df_train.groupBy(c).\\\n                                               agg(func.countDistinct(func.col('order_id'))).\\\n                                               withColumn('share', (func.col('count(order_id)')/func.lit(num_obs_train))).\\\n                                               filter(func.col('share')>thres_cat_repr).select(c).collect()]\n  \n  # Handling missings and unique categories:\n  repr_categories[c] = [c for c in repr_categories[c] if pd.isna(c)==False]\n  \n  if len(repr_categories[c]) <= 1:\n    repr_categories.pop(c)\n    cat_vars = [v for v in cat_vars if v!=c]\n  \n# Query clause to impute missing values of categorical variables and treat their residual categories:\nimpute_missings_cat = \"CASE WHEN `{feat}` IN {repr_cat} THEN `{feat}` WHEN `{feat}` IS NULL THEN 'missing_value' ELSE 'residual' END AS `{feat}`\"\nimpute_missings_cat = ', '.join([impute_missings_cat.format(feat=c, repr_cat=tuple(repr_categories[c])) for c in cat_vars])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7563fd4e-4823-4027-959c-fe88785f2451"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Numerical variables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1b96a59-c8f9-423a-b722-0abd9352f2e7"}}},{"cell_type":"code","source":["# Query clause for creating binary variable indicating missing values:\nmissing_vars = \"CASE WHEN `{feat}` IS NULL THEN 1 ELSE 0 END AS `NA#{feat}`\"\nmissing_vars = ', '.join([missing_vars.format(feat=c) for c in num_vars if c in vars_missings_train])\n\n# Query clause to impute missing values of numerical variables (except from those that had already been treated during log transformation):\nimpute_missings_num = \"CASE WHEN `{feat}` IS NULL THEN 0 ELSE `{feat}` END AS `{feat}`\"\nimpute_missings_num = ', '.join([impute_missings_num.format(feat=c) for c in num_vars if c not in to_log])\n\n# Query clause with support variables:\ndrop_vars_query = ', '.join(drop_vars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4672b478-b156-4d38-b561-683744808b99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Training data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9aa20e3d-71d5-4ae0-ab82-3739f33238ac"}}},{"cell_type":"code","source":["# Concatenating the query and running it to produce the transformed dataframe:\nquery = f'SELECT {drop_vars_query}, {log_transf}, {missing_vars}, {impute_missings_num}, {impute_missings_cat} FROM training_data'\ndf_train = spark.sql(query)\n\nfirst_date_train = df_train.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['first_date']\nlast_date_train = df_train.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['last_date']\n\nprint(f'Shape of df_train: ({df_train.count()}, {len(df_train.columns)}).')\nprint(f'Number of unique instances (training data): {df_train.select(\"order_id\").distinct().count()}.')\nprint(f'Time interval (training data): ({first_date_train}, {last_date_train}).')\n\n# df_train.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b48c2361-3928-4b46-b29c-ee3e01604ef3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Shape of df_train: (6313, 1330).\nNumber of unique instances (training data): 6313.\nTime interval (training data): (2021-05-17 15:01:00, 2021-06-09 23:56:06).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Shape of df_train: (6313, 1330).\nNumber of unique instances (training data): 6313.\nTime interval (training data): (2021-05-17 15:01:00, 2021-06-09 23:56:06).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### Test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf5a1a14-416d-4c24-8c7f-c54b88b3ff42"}}},{"cell_type":"code","source":["# Concatenating the query and running it to produce the transformed dataframe:\nquery = f'SELECT {drop_vars_query}, {log_transf}, {missing_vars}, {impute_missings_num}, {impute_missings_cat} FROM test_data'\ndf_test = spark.sql(query)\n\nfirst_date_test = df_test.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['first_date']\nlast_date_test = df_test.agg(func.min('datetime').alias('first_date'), func.max('datetime').alias('last_date')).collect()[0]['last_date']\n\nprint(f'Shape of df_test: ({df_test.count()}, {len(df_test.columns)}).')\nprint(f'Number of unique instances (test data): {df_test.select(\"order_id\").distinct().count()}.')\nprint(f'Time interval (test data): ({first_date_test}, {last_date_test}).')\n\n# df_test.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4af46f05-7321-4947-b3a2-991b47b0b28b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Shape of df_test: (2048, 1330).\nNumber of unique instances (test data): 2048.\nTime interval (test data): (2021-06-10 00:01:08, 2021-06-27 23:55:01).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Shape of df_test: (2048, 1330).\nNumber of unique instances (test data): 2048.\nTime interval (test data): (2021-06-10 00:01:08, 2021-06-27 23:55:01).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Categorical features transformation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25ec724a-dfe9-4d53-9c64-ba286fb19cf3"}}},{"cell_type":"markdown","source":["The transformation of categorical variables relies on two classes of MLlib API: [StringIndexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer) creates an estimator that, once fitted on training data, converts strings to label, while [OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder) works similarly, but converting categories into binary variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43102e21-65db-40d8-8b27-2a31a9da8a8b"}}},{"cell_type":"code","source":["# Creating the object that will convert categories into labels:\nstring_indexer = StringIndexer(inputCols=cat_vars, outputCols=[x + \"_Index\" for x in cat_vars])\n\n# Creating the object that will convert labels into binary variables:\nencoder = OneHotEncoder(inputCols=string_indexer.getOutputCols(), outputCols=[x + \"_OHE\" for x in cat_vars])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b540907-1422-4a75-8d5e-3c2815148efc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Visualizing the dataframe where categories are converted into labels:\nstringIndexerModel = string_indexer.fit(df_train)\n# display(stringIndexerModel.transform(df_train).take(3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec933fa3-aea3-4969-bd78-67e76876bfa6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Datasets consistency"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bf21970-cec6-48dd-8787-cf158d288a90"}}},{"cell_type":"code","source":["if len(df_train.columns)!=len(df_test.columns) | len(df_train.columns)!=sum([train==test for train, test in zip(df_train.columns, df_test.columns)]):\n  print('Training and test dataframes are inconsistent with each other!')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e0d783a-153b-4c4c-a214-b2dcf2f21e61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Vector assembler"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73b9efe3-c251-4792-85ca-04fd50f91d94"}}},{"cell_type":"markdown","source":["MLlib algorithms may require the creation of a single vector with all values of features for each data point. This can be done through the [VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) class."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a1f7e18-4d03-47fb-aeee-35b1d5dc08e3"}}},{"cell_type":"code","source":["features = [c for c in df_train.columns if (c not in drop_vars) & (c not in cat_vars)]\nfeatures.extend([c+'_OHE' for c in cat_vars])\n\n# Creating the object that will create a features vector out of all features columns:\nvec_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17cddf4d-7ac4-4f6d-9473-e7f518d94f80"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Standard scaling (second alternative)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b6baa1-3eef-40a8-b958-520e6a4e7416"}}},{"cell_type":"markdown","source":["The second approach for standard scaling numerical data uses the [StandardScaler](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) class of MLlib API. Here, all variables are standardized after logarithmic transformation, missing values treatment and one-hot encoding of categorical variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d507fb3a-5cab-45c7-a540-1b7f98cb86c3"}}},{"cell_type":"code","source":["# Creating the object that standard scales the inputs:\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9dac9a46-7ac2-47a8-8566-67bfeadc1a71"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Data modeling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"632974e7-55ad-4f0f-ad9e-4c0a6bb95b82"}}},{"cell_type":"markdown","source":["### Default models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1ea52d0-29d5-4bcc-9b1c-ef2aa03188b5"}}},{"cell_type":"markdown","source":["#### Logistic regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0125bb8-1831-4a61-b329-3e3332818f16"}}},{"cell_type":"markdown","source":["Model estimation with MLlib, as done here training a [logistic regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression) model, depends on the creation of a [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html) that sequentially implements all necessary data processing operations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19f5d8d5-a85f-4902-9e26-1348bbf1773e"}}},{"cell_type":"code","source":["# Creating the object of logistic regression model:\nlr_model = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"y\", regParam=1.0)\n\n# Creating the pipeline that sequentially fits and transforms the data:\npipeline_lr = Pipeline(stages=[string_indexer, encoder, vec_assembler, scaler, lr_model])\nprint(f'Type of pipeline_lr: {type(pipeline_lr)}.')\n\n# Training the model by running through all steps of the pipeline:\npipeline_model_lr = pipeline_lr.fit(df_train)\nprint(f'Type of pipeline_model_lr: {type(pipeline_model_lr)}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9c63bdd-29f0-4f3f-8d78-01b0ff4567fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of pipeline_lr: <class 'pyspark.ml.pipeline.Pipeline'>.\nType of pipeline_model_lr: <class 'pyspark.ml.pipeline.PipelineModel'>.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of pipeline_lr: <class 'pyspark.ml.pipeline.Pipeline'>.\nType of pipeline_model_lr: <class 'pyspark.ml.pipeline.PipelineModel'>.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Predictions on test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95f4b434-94eb-43b7-8ee6-4445f507d4e1"}}},{"cell_type":"code","source":["# Applying the pipeline over test data and generating predictions:\ntest_pred_lr = pipeline_model_lr.transform(df_test)\nprint(f'Type of test_pred_lr: {type(test_pred_lr)}.')\nprint(f'Shape of test_pred_lr: ({test_pred_lr.count(), len(test_pred_lr.columns)}).')\n\n# display(test_pred_lr.select('features', 'y', 'prediction', 'probability').take(3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b6bc8e6-5607-4885-a72e-9136f161f848"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of test_pred_lr: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of test_pred_lr: ((2048, 1349)).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of test_pred_lr: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of test_pred_lr: ((2048, 1349)).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Model evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"264f1f70-785b-43bb-91a5-b6377a7757fd"}}},{"cell_type":"markdown","source":["The [BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html) class allows the calculation of either ROC-AUC or the area under the precision-recall curve."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc5af5c9-7c61-476d-808c-209d45085446"}}},{"cell_type":"code","source":["# Creating the object that evaluates model performance:\nroc_auc_eval = BinaryClassificationEvaluator(labelCol='y', metricName=\"areaUnderROC\")\n\nprint(f\"Test ROC-AUC: {roc_auc_eval.evaluate(test_pred_lr):.4f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"196bfb07-a838-4cd1-95bb-a39eea981c1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Test ROC-AUC: 0.9040\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Test ROC-AUC: 0.9040\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["#### GBM"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98863f43-9f2c-46bf-883b-9bb27f003f98"}}},{"cell_type":"markdown","source":["Another learning algorithm available with MLlib is [GBM](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"432495c8-e8b8-4322-9f6e-9bf11832e2ef"}}},{"cell_type":"code","source":["# Creating the object of GBM:\ngbm_model = GBTClassifier(featuresCol=\"features\", labelCol=\"y\", maxDepth=3, stepSize=0.01, subsamplingRate=0.75)\n\n# Creating the pipeline that sequentially fits and transforms the data:\npipeline_gbm = Pipeline(stages=[string_indexer, encoder, vec_assembler, gbm_model])\nprint(f'Type of pipeline_gbm: {type(pipeline_gbm)}.')\n\n# Training the model by running through all steps of the pipeline:\npipeline_model_gbm = pipeline_gbm.fit(df_train)\nprint(f'Type of pipeline_model_gbm: {type(pipeline_model_gbm)}.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ca53980-b316-47d0-8d62-3786e5578b36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of pipeline_gbm: <class 'pyspark.ml.pipeline.Pipeline'>.\nType of pipeline_model_gbm: <class 'pyspark.ml.pipeline.PipelineModel'>.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of pipeline_gbm: <class 'pyspark.ml.pipeline.Pipeline'>.\nType of pipeline_model_gbm: <class 'pyspark.ml.pipeline.PipelineModel'>.\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Predictions on test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f3b5b59-ccd4-4c00-afbd-e39841d6e096"}}},{"cell_type":"code","source":["# Applying the pipeline over test data and generating predictions:\ntest_pred_gbm = pipeline_model_gbm.transform(df_test)\nprint(f'Type of test_pred_gbm: {type(test_pred_gbm)}.')\nprint(f'Shape of test_pred_gbm: ({test_pred_gbm.count(), len(test_pred_gbm.columns)}).')\n\n# display(test_pred_gbm.select('features', 'y', 'prediction', 'probability').take(3))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b3d0ee9-6e5b-4e07-ad9d-6aefc9af88ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Type of test_pred_gbm: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of test_pred_gbm: ((2048, 1348)).\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Type of test_pred_gbm: <class 'pyspark.sql.dataframe.DataFrame'>.\nShape of test_pred_gbm: ((2048, 1348)).\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Model evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5602efb5-e3fa-4d3d-8ed0-b1f69e44adb7"}}},{"cell_type":"code","source":["print(f\"Test ROC-AUC: {roc_auc_eval.evaluate(test_pred_gbm):.4f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf280342-820d-4e73-a690-0809d3172a9e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Test ROC-AUC: 0.8674\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Test ROC-AUC: 0.8674\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Optimizing hyper-parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a76bee11-2a1f-4dc8-8e52-d26be861ef33"}}},{"cell_type":"markdown","source":["The optimization of hyper-parameters is available under MLlib through two classes: [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html), which creates a grid with different combinations of values of hyper-parameters, and [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator), which implements K-folds CV estimation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19422339-562d-4bff-80f4-b57c8ba57279"}}},{"cell_type":"markdown","source":["#### Logistic regression"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5c3e3ce-e683-423f-987f-a51f5f2b9a91"}}},{"cell_type":"code","source":["# Creating the object that defines a grid of hyper-parameters values:\nparamGrid_lr = (ParamGridBuilder()\n               .addGrid(lr_model.regParam, [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10.0])\n               .build())\n\n# Creating the object for running K-folds CV:\ncv = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr, evaluator=roc_auc_eval, numFolds=3, parallelism=4)\n\n# Running K-folds CV estimation:\ncv_lr = cv.fit(df_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"944eab0d-fc36-4ac4-9c77-c1d3604ceee9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:838: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:838: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Evaluating the final model on test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b57f16bb-2fae-414a-9a65-d6b4be69239d"}}},{"cell_type":"code","source":["# Predictions on test data from the best model:\ntest_pred_lr = cv_lr.transform(df_test)\n\n# Evaluating model performance:\nprint(f\"Test ROC-AUC: {roc_auc_eval.evaluate(test_pred_lr):.4f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09ceaebb-ecc8-4658-a355-053975bf8610"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### GBM"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cdc22c5-9d0b-4ff8-9139-b890cb9a5d98"}}},{"cell_type":"code","source":["# Creating the object that defines a grid of hyper-parameters values:\nparamGrid_gbm = (ParamGridBuilder()\n                .addGrid(gbm_model.maxDepth, [1, 3, 5])\n                .addGrid(gbm_model.stepSize, [0.001, 0.01, 0.1])\n                .addGrid(gbm_model.subsamplingRate, [0.5, 0.75, 1.0])\n                .build())\n\n# Creating the object for running K-folds CV:\ncv_gbm = CrossValidator(estimator=pipeline_gbm, estimatorParamMaps=paramGrid_gbm, evaluator=roc_auc_eval, numFolds=3, parallelism = 4)\n \n# Running K-folds CV estimation:\ncv_gbm = cv_gbm.fit(df_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4782ea09-4af1-4b1f-8d93-728d0acea83e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Evaluating the final model on test data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e65411ae-103c-44d4-b453-0e4fd1d1ed3e"}}},{"cell_type":"code","source":["# Predictions on test data from the best model:\ntest_pred_gbm = cv_gbm.transform(df_test)\n\n# Evaluating model performance:\nprint(f\"Test ROC-AUC: {roc_auc_eval.evaluate(test_pred_gbm):.4f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7bb5671-eaa7-4a85-9c23-0996c1e594a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37423262-11ad-465b-993b-03314bb992f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Modeling with MLlib","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4302853533603689}},"nbformat":4,"nbformat_minor":0}
