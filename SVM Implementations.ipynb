{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM Implementations.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNhSG1FT5NabZVOaQKk2z9s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7rbt6qO5tBQ2"},"source":["## SVM implementations"]},{"cell_type":"markdown","source":["**Support vector methods** explicitly estimate decision boundaries in the input space to separate (perfectly or not) classes, or additionally, to perform regression. The first of such methods, *maximal margin classifier*, tries to generate a hyperplane that perfectly separates two classes. *Support vector classifier*, in its turn, allows overlap of classes, while still fitting a linear decision boundary. *Support vector machines* consist of a generalization\n","which not only allows misclassification of training data points, but also produce non-linear decision boundaries in the original input space.\n","\n","The explicit estimation of decision boundaries by SVMs depends on the definition of a hyperplane, which is given by the set of points $\\{x \\in \\mathbb{R}^p | f(x) = x^T\\beta + \\beta_0 = 0\\}$. For a binary classification task, where $Y \\in \\{-1, 1\\}$ is the target variable, if a given instance $x$ falls above the hyperplane, i.e., $f(x) > 0$, then $x$ is assigned to class $Y = 1$, and vice-versa. Consequently, the classification rule induced by $f(x)$ is $G(x) = sign(x^T\\beta + \\beta_0)$.\n","\n","**Linearly separable case:** the perfect separation can be summarized by the following property of the *margin* for all data points available: $y_if(x_i) > 0$ $\\forall$ $i \\in \\{1, 2, ..., N\\}$. Indeed, maximizing the margin $M \\in \\mathbb{I}_+$ that separates the set of points for which $Y = 1$ from those that $Y = -1$ is the way how to define the perfect separating hyperplane $f(x)$:\n","\\begin{equation}\n","    \\displaystyle \\max_{\\beta, \\beta_0} M\n","\\end{equation}\n","\\begin{equation}\n","  \\displaystyle \\mbox{subject to } ||\\beta|| = 1\n","\\end{equation}\n","\n","\\begin{equation}\n","    \\displaystyle y_i(x_i^T\\beta + \\beta_0) \\geq M \\forall i \\in \\{1, 2, ..., N\\}\n","\\end{equation}\n","\n","Where $y_i(x_i^T\\beta + \\beta_0)$ is the *sign distance* between training instance $x_i$ and the hyperplane. More precisely, $2M$ is the margin between the two classes, since $M$ is actually the distance between the closest data points from $Y = 1$ and $Y = -1$ to the hyperplane. This optimization problem can be redefined as:\n","\\begin{equation}\n","    \\displaystyle \\min_{\\beta, \\beta_0} ||\\beta||\n","\\end{equation}\n","\\begin{equation}\n","    \\displaystyle \\mbox{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1 \\forall i \\in \\{1, 2, ..., N\\}\n","\\end{equation}\n","\n","Where $M$ is taken to be equal to $1/||\\beta||$. The estimated function $\\hat{f}(x) = x^T\\hat{\\beta} + \\hat{\\beta}_0$ that follows from this problem is named the **maximal margin classifier**.\n","\n","The **linearly non-separable case** demands the change of the main constraint of the first optimization problem to $y_i(x_i^T\\beta + \\beta_0) \\geq M(1 - \\xi_i)$, where $\\{\\xi_i\\}_{i=1}^N$ are the *slack variables* that allow misclassification of some data points, in the sense that some observations can fall into the wrong side of the hyperplane when $\\xi_i > 1$. Additionally, for observations with $0 < \\xi_i < 1$, they will belong to the right side of the hyperplane, but inside the margin. A new constraint under which $\\sum_i \\xi_i \\leq K$ limits the number of training data points that are allowed to be misclassified. The parameters $\\hat{\\beta}$ that satisfy the modified version of the optimization problem above configure the **support vector classifier**. This method does not only accomplish the task of classification in the non-separable case, but also it is more robust to overfitting than maximal margin classifier, even when perfect separation would be possible.\n","\n","Support vector methods are highly intuitive, but its mathematical formulation are somewhat complex. Given that optimization problems are their ground basis, and since there are a lot of ways to express a same optimization problem, it can be cumbersome to express all formulations. [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) present a complete derivation of SVMs, including one that justifies the term *support vectors*: only observations in the margin or in the incorrect side of it effectively contribute to the construction of the hyperplane.\n","\n","**Support vector machines** have the same structure of support vector classifiers. The main difference concerns the use of basis functions to enlarge the features space in order to produce linear decision boundaries in that enlarged space, but non-linear boundaries in the original space, which may help increasing predictive performance. The hyperplane equation is now given by $f(x) = h(x)^T\\beta + \\beta_0$, where $h(x)$ is a vector of basis functions constructed upon the original inputs. The main formulation that leads to the support vector classifier implies the following:\n","\\begin{equation}\n","  \\displaystyle \\hat{f}(x) = x^T\\hat{\\beta} + \\hat{\\beta}_0 = \\sum_{i=1}^N \\hat{\\alpha}_iy_ix^Tx_i + \\hat{\\beta}_0\n","\\end{equation}\n","\n","Consequently:\n","\\begin{equation}\n","  \\displaystyle \\hat{f}(x) = \\sum_{i=1}^N \\hat{\\alpha}_iy_i<x, x_i> + \\hat{\\beta}_0\n","\\end{equation}\n","\n","Where $<x, x_i>$ stands for the inner product between vectors $x$ and $x_i$, while $\\{\\hat{\\alpha}_i\\}$ are Lagrange multipliers of the optimization problem of support vector classifiers. What SVMs do is to replace $x$ and $x_i$ by basis functions $h(x)$ and $h(x_i)$. In practice, SVMs use a more general *kernel function* $K(x, x_i)$ instead of $<x, x_i>$, in such a way that $K(x, x_i) = <h(x), h(x_i)>$, and so $x$ is implicitly replaced by $h(x)$. Some of the main alternatives of kernel functions are:\n","\\begin{equation}\n","    \\displaystyle K(x, x') = (1 + <x, x'>)^d\n","\\end{equation}\n","\\begin{equation}\n","    \\displaystyle K(x, x') = \\exp(-\\gamma||x - x'||^2)\n","\\end{equation}\n","\\begin{equation}\n","    \\displaystyle K(x, x') = \\tanh(\\kappa_1<x, x'> + \\kappa_2)\n","\\end{equation}\n","\n","The $d$-th degree polynomial, the radial basis, and the neural network kernels, respectively. So, for SVMs the separating hyperplane is given by:\n","\n","\\begin{equation}\n","  \\displaystyle \\hat{f}(x) = \\sum_{i=1}^N \\hat{\\alpha}_iy_iK(x, x_i) + \\hat{\\beta}_0\n","\\end{equation}\n","\n","Note that support vector classifiers are a particular case of SVMs when the kernel is linear, i.e., $K(x, x_i) = <x, x_i>$.\n","\n","The ultimate version of the optimization problem solved for fitting an SVM model, that is assumed by the [article](https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2) that brings the codes presented and discussed next, is such that the intrinsic regularization of SVMs is showed up:\n","\\begin{equation}\n","  \\displaystyle \\min_{\\beta, \\beta_0} \\sum_{i=1}^N \\max[0, 1 - y_if(x_i)] + \\frac{\\lambda}{2}||\\beta||^2\n","\\end{equation}\n","\n","Where $L(y, f) = \\max[0, 1 - yf(x)]$ is the *hinge loss function*, while $||\\beta||$ consists of the penalty term. The larger $\\lambda$, the regularization parameter, the more coefficients of the hyperplane will be shrunken towards zero; the higher the regularization, the more training data points can be misclassified, i.e., the larger the margin.\n","\n","Finally, still regarding the theoretical ground of support vector methods, they do not explicitly produce estimates for class probabilities. Some additional algorithms may perform this estimation by regressing true labels against predicted labels from a SVM model, which are used as inputs to an auxiliary estimation method, a logistic regression (*Platt scaling*) or a piecewise regression (*isotonic regression*), for instance.\n","\n","The main [article](https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2) whose codes are reproduced here implements SVMs by solving the last minimization problem presented above through **stochastic gradient descent (SGD)**, a widely used optimization algorithm that is also applied for fitting neural networks, for instance. The pipeline of the machine learning application developed by the author starts by removing both highly correlated features and those whose relationship with the target variable is not statistically significant. Then, all input variables are normalized using min-max scaling, which is crucial since weights of variables are calculated, similarly to logistic regression, neural networks and KNNs, and very different scales could mislead the calculation of the relevance of each input in the hyperplane construction.\n","\n","After introducing train-test split, the author trains the SVM model by calculating the coefficients of the separating hyperplane by using the SGD algorithm. The *sgd* function is constructed upon the function *compute_cost*, which, using weights, inputs and output variables, returns the main cost function defined above for a given regularization parameter (*actually, instead of $\\lambda$, the regularization parameter $C = 1/\\lambda$ is considered in the code, where this parameter multiplies the Hinge loss instead of the norm of $\\beta$*). Most importantly, *sgd* function uses *calculate_cost_gradient* function, which calculates the gradient of the cost function for updating weights using the *gradient descent update rule*. So, *sgd* function iterates over a given number of epochs and, inside of each epoch and after shuffling the data, it iterates over each training instance, i.e., the mini-batch gradient descent with $S = 1$ is implemented. At each iteration, weights are updated using the gradient of cost function evaluated at the mini-batch. After some number of epochs, a termination criterium is evaluated by checking if the current iteration reduces the previous cost function by some predefined percentage of reference."],"metadata":{"id":"R9wtqOk8i4ys"}},{"cell_type":"markdown","metadata":{"id":"tWi-Yyretplg"},"source":["**References**\n","<br>\n","[SVM From Scratch](https://towardsdatascience.com/svm-implementation-from-scratch-python-2db2fc52e5c2).\n","<br>\n","[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf).\n","<br>\n","[Support Vector Machine Algorithm for Data Scientists](https://www.analyticsvidhya.com/blog/2021/07/svm-support-vector-machine-algorithm/)."]},{"cell_type":"markdown","metadata":{"id":"YjtnwOcFtdtn"},"source":["----------------"]},{"cell_type":"markdown","metadata":{"id":"-xp0Wq1nAb73"},"source":["This notebook first imports all relevant libraries, and then presents implementations of linear regression estimation with OLS and gradient descent, besides of polynomial regression, ridge and lasso regularized models. All implementations follow from this [article](https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076) ([Github](https://github.com/lukenew2/mlscratch) page of reference)."]},{"cell_type":"markdown","metadata":{"id":"1jKqXHmYthkK"},"source":["**Summary:**\n","1. [Libraries](#libraries)<a href='#libraries'></a>.\n","2. [Preparing the data](#data_prep)<a href='#data_prep'></a>.\n","3. [Inner functioning of SVM](#inner_functioning)<a href='#inner_functioning'></a>.\n","4. [Training the model](#model_training)<a href='#model_training'></a>."]},{"cell_type":"markdown","metadata":{"id":"NPwnjKEUtk3U"},"source":["<a id='libraries'></a>"]},{"cell_type":"markdown","metadata":{"id":"Kd3Ii0l1tmT1"},"source":["## Libraries"]},{"cell_type":"code","metadata":{"id":"-tNt5lArtoMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640221556689,"user_tz":180,"elapsed":31042,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"4e7efea5-5804-4bbc-af15-e81f7b9b32bb"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bx0RX4AsUtKe","executionInfo":{"status":"ok","timestamp":1640221560209,"user_tz":180,"elapsed":933,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"3994d519-38c3-4c1d-8a55-0bfcdd5b8a57"},"source":["cd \"/content/gdrive/MyDrive/Studies/svm/Codes\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Studies/svm/Codes\n"]}]},{"cell_type":"code","metadata":{"id":"xOnfCCeLtr2A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640221564358,"user_tz":180,"elapsed":1630,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"554e17bd-19c6-4940-d2c2-2088ffa69f54"},"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split as tts\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ojy3GLAotqii"},"source":["<a id='data_prep'></a>"]},{"cell_type":"markdown","metadata":{"id":"bg_iUVd-twyu"},"source":["## Preparing the data"]},{"cell_type":"markdown","source":["<a id='data_transf'></a>"],"metadata":{"id":"htlPBZYf2KU7"}},{"cell_type":"markdown","source":["### Reading and transforming the data"],"metadata":{"id":"Q8Rr1te_2PNk"}},{"cell_type":"code","source":["# Importing the data:\n","data = pd.read_csv('../Datasets/data.csv')\n","data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n","\n","# Treating the target variable:\n","diag_map = {'M': 1.0, 'B': -1.0}\n","data['diagnosis'] = data['diagnosis'].map(diag_map)\n","\n","# Input and output variables:\n","Y = data.loc[:, 'diagnosis']\n","X = data.iloc[:, 1:]\n","\n","# Scaling the data:\n","X_normalized = MinMaxScaler().fit_transform(X.values)\n","X = pd.DataFrame(X_normalized)\n","X.insert(loc=len(X.columns), column='intercept', value=1)\n","\n","# Train-test split:\n","X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)"],"metadata":{"id":"pXvxMu5Che20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='feat_selection'></a>"],"metadata":{"id":"bMa53oAc0wTt"}},{"cell_type":"markdown","source":["### Features selection"],"metadata":{"id":"FYkJNJ2y0yui"}},{"cell_type":"code","source":["# Function that removes highly correlated input variables:\n","def remove_correlated_features(X):\n","    corr_threshold = 0.9\n","    corr = X.corr()\n","    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n","    for i in range(corr.shape[0]):\n","        for j in range(i + 1, corr.shape[0]):\n","            if corr.iloc[i, j] >= corr_threshold:\n","                drop_columns[j] = True\n","    columns_dropped = X.columns[drop_columns]\n","    X.drop(columns_dropped, axis=1, inplace=True)\n","    return columns_dropped\n","\n","# Function that removes input variables with just a few correlation with the output variable:\n","def remove_less_significant_features(X, Y):\n","    sl = 0.05\n","    regression_ols = None\n","    columns_dropped = np.array([])\n","    for itr in range(0, len(X.columns)):\n","        regression_ols = sm.OLS(Y, X).fit()\n","        max_col = regression_ols.pvalues.idxmax()\n","        max_val = regression_ols.pvalues.max()\n","        if max_val > sl:\n","            X.drop(max_col, axis='columns', inplace=True)\n","            columns_dropped = np.append(columns_dropped, [max_col])\n","        else:\n","            break\n","    regression_ols.summary()\n","    return columns_dropped"],"metadata":{"id":"UxWufY4D00N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='inner_functioning'></a>"],"metadata":{"id":"nVcilZKOhLcs"}},{"cell_type":"markdown","source":["## Inner functioning of SVM"],"metadata":{"id":"bFOB2GjRhP1Y"}},{"cell_type":"markdown","source":["<a id='cost_function'></a>"],"metadata":{"id":"xX3_Mgiv1IPV"}},{"cell_type":"markdown","source":["### Cost function"],"metadata":{"id":"ohmvXY_c1Khb"}},{"cell_type":"code","source":["# Function that calculates the cost function of SVMs:\n","def compute_cost(W, X, Y):\n","    # calculate hinge loss\n","    N = X.shape[0]\n","    distances = 1 - Y * (np.dot(X, W))\n","    distances[distances < 0] = 0  # equivalent to max(0, distance)\n","    hinge_loss = regularization_strength * (np.sum(distances) / N)\n","\n","    # calculate cost\n","    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n","    return cost\n","\n","# Function that calculates the gradient vector of cost function with respect to coefficients:\n","def calculate_cost_gradient(W, X_batch, Y_batch):\n","    # if only one example is passed (eg. in case of SGD)\n","    if type(Y_batch) == np.float64:\n","        Y_batch = np.array([Y_batch])\n","        X_batch = np.array([X_batch])  # gives multidimensional array\n","\n","    distance = 1 - (Y_batch * np.dot(X_batch, W))\n","    dw = np.zeros(len(W))\n","\n","    for ind, d in enumerate(distance):\n","        if max(0, d) == 0:\n","            di = W\n","        else:\n","            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n","        dw += di\n","\n","    dw = dw/len(Y_batch)  # average\n","    return dw"],"metadata":{"id":"jwoc1y5dhd6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='sgd'></a>"],"metadata":{"id":"V_1WOy-E1OeL"}},{"cell_type":"markdown","source":["### Stochastic gradient descent"],"metadata":{"id":"7Iq1BVps1SbP"}},{"cell_type":"code","source":["# Function that implements SGD for minimizing the cost function of SVMs:\n","def sgd(features, outputs):\n","    max_epochs = 5000\n","    weights = np.zeros(features.shape[1])\n","    nth = 0\n","    prev_cost = float(\"inf\")\n","    cost_threshold = 0.01  # in percent\n","    # stochastic gradient descent\n","    for epoch in range(1, max_epochs):\n","        # shuffle to prevent repeating update cycles\n","        X, Y = shuffle(features, outputs)\n","        for ind, x in enumerate(X):\n","            ascent = calculate_cost_gradient(weights, x, Y[ind])\n","            weights = weights - (learning_rate * ascent)\n","\n","        # convergence check on 2^nth epoch\n","        if epoch == 2 ** nth or epoch == max_epochs - 1:\n","            cost = compute_cost(weights, features, outputs)\n","            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n","            # stoppage criterion\n","            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n","                return weights\n","            prev_cost = cost\n","            nth += 1\n","    return weights"],"metadata":{"id":"qa-GXzwT1R7C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='model_training'></a>"],"metadata":{"id":"AEMA637QhY84"}},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"j59EeUUIhcNU"}},{"cell_type":"markdown","source":["<a id='svm_algorithm'></a>"],"metadata":{"id":"LQQI2dh41rVW"}},{"cell_type":"markdown","source":["### SVM algorithm"],"metadata":{"id":"S1JlyGZm1wKB"}},{"cell_type":"code","source":["# Algorithm for fitting an SVM model:\n","def init():\n","    print(\"reading dataset...\")\n","    # read data in pandas (pd) data frame\n","    data = pd.read_csv('../Datasets/data.csv')\n","\n","    # drop last column (extra column added by pd)\n","    # and unnecessary first column (id)\n","    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n","\n","    print(\"applying feature engineering...\")\n","    # convert categorical labels to numbers\n","    diag_map = {'M': 1.0, 'B': -1.0}\n","    data['diagnosis'] = data['diagnosis'].map(diag_map)\n","\n","    # put features & outputs in different data frames\n","    Y = data.loc[:, 'diagnosis']\n","    X = data.iloc[:, 1:]\n","\n","    # filter features\n","    remove_correlated_features(X)\n","    remove_less_significant_features(X, Y)\n","\n","    # normalize data for better convergence and to prevent overflow\n","    X_normalized = MinMaxScaler().fit_transform(X.values)\n","    X = pd.DataFrame(X_normalized)\n","\n","    # insert 1 in every row for intercept b\n","    X.insert(loc=len(X.columns), column='intercept', value=1)\n","\n","    # split data into train and test set\n","    print(\"splitting dataset into train and test sets...\")\n","    X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n","\n","    # train the model\n","    print(\"training started...\")\n","    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n","    print(\"training finished.\")\n","    print(\"weights are: {}\".format(W))\n","\n","    # testing the model\n","    print(\"testing the model...\")\n","    y_train_predicted = np.array([])\n","    for i in range(X_train.shape[0]):\n","        yp = np.sign(np.dot(X_train.to_numpy()[i], W))\n","        y_train_predicted = np.append(y_train_predicted, yp)\n","\n","    y_test_predicted = np.array([])\n","    for i in range(X_test.shape[0]):\n","        yp = np.sign(np.dot(X_test.to_numpy()[i], W))\n","        y_test_predicted = np.append(y_test_predicted, yp)\n","\n","    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n","    print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n","    print(\"precision on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))"],"metadata":{"id":"u_dqtJUS1jDJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id='fitting_model'></a>"],"metadata":{"id":"FYKvUt7q13CN"}},{"cell_type":"markdown","source":["### Fitting the model"],"metadata":{"id":"NtynMLsb15tI"}},{"cell_type":"code","source":["# Hyper-parameters:\n","regularization_strength = 10000\n","learning_rate = 0.000001\n","\n","# Fitting the model:\n","init()"],"metadata":{"id":"G8HoPx_BhdZw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640222282590,"user_tz":180,"elapsed":19319,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"49624091-f554-4ca1-e31e-0648274750b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reading dataset...\n","applying feature engineering...\n","splitting dataset into train and test sets...\n","training started...\n","Epoch is: 1 and Cost is: 7265.461796282826\n","Epoch is: 2 and Cost is: 6548.324667994216\n","Epoch is: 4 and Cost is: 5454.715412780056\n","Epoch is: 8 and Cost is: 3963.303287448077\n","Epoch is: 16 and Cost is: 2640.61157366003\n","Epoch is: 32 and Cost is: 2044.4096653757088\n","Epoch is: 64 and Cost is: 1590.1977865307224\n","Epoch is: 128 and Cost is: 1325.3659890065799\n","Epoch is: 256 and Cost is: 1161.805355679864\n","Epoch is: 512 and Cost is: 1074.162147916526\n","Epoch is: 1024 and Cost is: 1048.5991553237839\n","Epoch is: 2048 and Cost is: 1044.5797119244573\n","training finished.\n","weights are: [ 3.5559056  11.04953495 -2.28574834 -7.89109122 10.1562053  -1.27033218\n"," -6.44637726  2.25265913 -3.88768916  3.26020627  4.9645179   4.8243594\n"," -4.7149502 ]\n","testing the model...\n","accuracy on test dataset: 0.9912280701754386\n","recall on test dataset: 0.9767441860465116\n","precision on test dataset: 0.9767441860465116\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"H83kwi4X1hWZ"},"execution_count":null,"outputs":[]}]}