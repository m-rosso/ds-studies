{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost estimation\n",
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to understand and implement **XGBoost library for developing boosted models**, which may be either composed of an ensemble of linear models or an ensemble of trees. All main pieces of code present here were extracted from [XGBoost official documentation](https://xgboost.readthedocs.io/en/latest/index.html). Once the library has been [installed](https://xgboost.readthedocs.io/en/latest/install.html), the documentation provides a [simple tutorial](https://xgboost.readthedocs.io/en/latest/get_started.html) of how to train a GBM model using XGBoost API. In this notebook, section [train-test estimation](#train_test)<a href='#train_test'></a> applies those basic codes for preparing and training an XGBoost model.\n",
    "<br>\n",
    "<br>\n",
    "The use of XGBoost through [Python](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) is somewhat similar to that for [LightGBM](https://lightgbm.readthedocs.io/en/latest/). So, for instance, in the first place, one should create a proper [data object](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.core) for feeding the model object. This data object is created by applying the callable class *DMatrix* over a conventional data object, such as dataframe or nd-array. This *DMatrix* object contains both input and output variables.\n",
    "<br>\n",
    "<br>\n",
    "Another similarity with LightGBM is the definition of a dictionary containing all relevant **parameters** for GBM estimation. This gives more control over the structure of the model under construction, since all parameters are gathered in the same object. Once the data object and the parameters dictionary have been created, the model is trained by declaring them in the initialization of the [training object](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train), *train*. Another crucial parameter declared during this initialization, this time outside the dictionary of parameters, is *num_boosted_rounds*, which works as the number of trees in the GBM ensemble. A customized cost function can be used by declaring it in the argument *obj* when initializing the object of model training - however, one can choose among standard cost functions through the *objective* key in the parameters dictionary.\n",
    "<br>\n",
    "<br>\n",
    "**Early stopping** is an estimation strategy that helps defining the number of boosting iterations. Its use is straightforward: performance metrics should be defined in the *eval_metric* key of parameters dictionary, while arguments *evals* and *early_stopping_rounds* should be declared in the training object. While *eval_metric* is given by the string or a list of strings with names of performance metrics, *evals* receives a list with tuples, where the first element is a *DMatrix* object and the second a string with its name, and *early_stopping_rounds* is the number of iterations allowed for not showing any performance improvement.\n",
    "<br>\n",
    "<br>\n",
    "The class *cv* consists of an extension of *train* that implements [K-folds cross-validation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv) together with XGBoost model. Its initialization arguments are basically the same as those for *train*, having some extra arguments for setting up the K-folds estimation, such as the number of folds, whether data should be shuffled or not previous to split, and whether stratified (representative) samples should be produced when data is shuffled.\n",
    "<br>\n",
    "<br>\n",
    "GBMs are very flexible estimation methods that usually outperform most alternative learning algorithms. This comes at the cost of requiring the proper definition of a huge collection of parameters. XGBoost documents [here](https://xgboost.readthedocs.io/en/latest/parameter.html#xgboost-parameters) all available parameters for optimizing model performance. In addition to [general parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#global-configuration), the lists of parameters for [tree booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster), for [linear booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-linear-booster-booster-gblinear) and for [DART booster](https://xgboost.readthedocs.io/en/latest/parameter.html#additional-parameters-for-dart-booster-booster-dart) correspond to the most relevant documentation of XGBoost. Linear booster and DART booster are alternative implementations of the boosting principle, since GBMs are generally taken to be ensembles of trees.\n",
    "<br>\n",
    "<br>\n",
    "This tutorial restricts itself to present and discuss the construction of boosted trees, leaving for further research the implementation of boosted linear models and DART models. Those **hyper-parameters** of GBM that are particularly helpful for improving model performance are the following:\n",
    "* Subsample paramter ($subsample \\leq 1$): when defined to be smaller than one, stochastic GBM is implemented, meaning that only a random subset of training data is used for fitting the tree at each boosting iteration.\n",
    "* Learning rate ($eta \\in [0,1]$): when defined to be smaller than one, each member of the ensemble will have its contribution to the prediction shrinked by a factor of $eta$.\n",
    "* Maximum depth ($max\\_depth \\in \\mathbb{I}$): controls the quantity of vertical growths of each tree. The larger $max\\_depth$, the more complex each member of the ensemble will be.\n",
    "* Number of boosting iterations ($num\\_boost\\_round \\in \\mathbb{I}_+$): number of members of the ensemble of trees. *Note that this parameter should be declared when initializing the training object.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional parameters** can also contribute for fine tuning the model performance. They mostly relate to the construction of each tree that compose the boosted model. For instance, $gamma$ ($min\\_split\\_loss$) controls the minimum reduction in cost function for a split to be considered. $grow\\_policy$ directly dictates how the tree should grow as splits go on. Parameters such as $colsample\\_bytree$, $colsample\\_bylevel$, $colsample\\_bynode$ allow the random selection of a subset of features for producing splits, similar to the development of random forests. Parameters $lambda$ and $alpha$ make possible to introduce further regularization to the model. Parameter $tree\\_method$ gives an even larger degree of control over the construction of the boosted model, since it makes possible the choice regarding which optimization algorithm should be used for generating trees.\n",
    "<br>\n",
    "<br>\n",
    "XGBoost documentation presents some discussion about how to [fine tune](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) hyper-parameters in order to reduce overfitting, improve model performance and make estimations faster.\n",
    "<br>\n",
    "<br>\n",
    "Documentation regarding parameters also cover options for the [learning task](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters), such as the cost function to be used during training and the metric for evaluating the model at each boosting iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation also shows how to select [sub-models](https://xgboost.readthedocs.io/en/latest/python/model.html) from the entire ensemble of trees (for tree booster or DART booster) and how to develop [callback functions](https://xgboost.readthedocs.io/en/latest/python/callbacks.html) (and [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#callback-api)) for monitoring the model construction. Documentation for [prediction](https://xgboost.readthedocs.io/en/latest/prediction.html), [plotting](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting) model outcomes, and for using XGBoost under [sklearn interface](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) ends the main documentation, together with several official tutorials ([here](https://github.com/dmlc/xgboost/tree/master/demo) and [here](https://xgboost.readthedocs.io/en/latest/tutorials/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "1. [Libraries](#libraries)<a href='#libraries'></a>.\n",
    "2. [Functions and classes](#functions_classes)<a href='#functions_classes'></a>.\n",
    "3. [Settings](#settings)<a href='#settings'></a>.\n",
    "4. [Importing data](#imports)<a href='#imports'></a>.\n",
    "    * [Features and label](#feats_label)<a href='#feats_label'></a>.\n",
    "<br>\n",
    "<br>\n",
    "5. [Model estimation](#model_estimation)<a href='#model_estimation'></a>.\n",
    "    * [Train-test estimation](#train_test)<a href='#train_test'></a>.\n",
    "    * [Early stopping](#es)<a href='#es'></a>.\n",
    "    * [CV estimation](#cv)<a href='#cv'></a>.\n",
    "    * [Grid and random searches](#grid_random_searches)<a href='#grid_random_searches'></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='libraries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import re\n",
    "# pip install unidecode\n",
    "from unidecode import unidecode\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from scipy.stats import uniform, norm, randint\n",
    "\n",
    "# pip install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, auc, precision_recall_curve, brier_score_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='functions_classes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils import loading_data, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfolds\n",
    "from kfolds import Kfolds_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='settings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset_id:\n",
    "dataset_id = 2706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imports'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feats_label'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\u001b[1mDataset 2706:\u001b[0m\n",
      "Shape of df: (7217, 1286).\n",
      "Number of distinct instances: 7217.\n",
      "Time period: from 2020-12-31 to 2021-02-17.\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------')\n",
    "print(f'\\033[1mDataset {dataset_id}:\\033[0m')\n",
    "\n",
    "df_train = loading_data(path=f'../Datasets/dataset_{dataset_id}_train.csv',\n",
    "                        dtype={'order_id': str, 'store_id': int, 'epoch': str},\n",
    "                        id_var='order_id')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "# Accessory variables:\n",
    "drop_vars = ['y', 'order_id', 'epoch', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "\u001b[1mDataset 2706:\u001b[0m\n",
      "Shape of df: (7217, 1286).\n",
      "Number of distinct instances: 7217.\n",
      "Time period: from 2021-02-17 to 2021-03-31.\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----------------------------------------')\n",
    "print(f'\\033[1mDataset {dataset_id}:\\033[0m')\n",
    "\n",
    "df_test = loading_data(path=f'../Datasets/dataset_{dataset_id}_test.csv',\n",
    "                        dtype={'order_id': str, 'store_id': int, 'epoch': str},\n",
    "                        id_var='order_id')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_estimation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DMatrix in module xgboost.core:\n",
      "\n",
      "class DMatrix(builtins.object)\n",
      " |  DMatrix(data, label=None, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      " |  \n",
      " |  Data Matrix used in XGBoost.\n",
      " |  \n",
      " |  DMatrix is a internal data structure that used by XGBoost\n",
      " |  which is optimized for both memory efficiency and training speed.\n",
      " |  You can construct DMatrix from numpy.arrays\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, data, label=None, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/\n",
      " |             dt.Frame/cudf.DataFrame/cupy.array/dlpack\n",
      " |          Data source of DMatrix.\n",
      " |          When data is string or os.PathLike type, it represents the path\n",
      " |          libsvm format txt file, csv file (by specifying uri parameter\n",
      " |          'path_to_csv?format=csv'), or binary file that xgboost can read\n",
      " |          from.\n",
      " |      label : list, numpy 1-D array or cudf.DataFrame, optional\n",
      " |          Label of the training data.\n",
      " |      missing : float, optional\n",
      " |          Value in the input data which needs to be present as a missing\n",
      " |          value. If None, defaults to np.nan.\n",
      " |      weight : list, numpy 1-D array or cudf.DataFrame , optional\n",
      " |          Weight for each instance.\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each\n",
      " |              data point). This is because we only care about the relative\n",
      " |              ordering of data points within each group, so it doesn't make\n",
      " |              sense to assign weights to individual data points.\n",
      " |      \n",
      " |      silent : boolean, optional\n",
      " |          Whether print messages during construction\n",
      " |      feature_names : list, optional\n",
      " |          Set names for features.\n",
      " |      feature_types : list, optional\n",
      " |          Set types for features.\n",
      " |      nthread : integer, optional\n",
      " |          Number of threads to use for loading data when parallelization is\n",
      " |          applicable. If -1, uses maximum threads available on the system.\n",
      " |  \n",
      " |  get_base_margin(self)\n",
      " |      Get the base margin of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      base_margin : float\n",
      " |  \n",
      " |  get_float_info(self, field)\n",
      " |      Get float property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of float information of the data\n",
      " |  \n",
      " |  get_label(self)\n",
      " |      Get the label of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      label : array\n",
      " |  \n",
      " |  get_uint_info(self, field)\n",
      " |      Get unsigned integer property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of unsigned integer information of the data\n",
      " |  \n",
      " |  get_weight(self)\n",
      " |      Get the weight of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      weight : array\n",
      " |  \n",
      " |  num_col(self)\n",
      " |      Get the number of columns (features) in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of columns : int\n",
      " |  \n",
      " |  num_row(self)\n",
      " |      Get the number of rows in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of rows : int\n",
      " |  \n",
      " |  save_binary(self, fname, silent=True)\n",
      " |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      " |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Name of the output buffer file.\n",
      " |      silent : bool (optional; default: True)\n",
      " |          If set, the output is suppressed.\n",
      " |  \n",
      " |  set_base_margin(self, margin)\n",
      " |      Set base margin of booster to start from.\n",
      " |      \n",
      " |      This can be used to specify a prediction value of existing model to be\n",
      " |      base_margin However, remember margin is needed, instead of transformed\n",
      " |      prediction e.g. for logistic regression: need to put in value before\n",
      " |      logistic transformation see also example/demo.py\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      margin: array like\n",
      " |          Prediction margin of each datapoint\n",
      " |  \n",
      " |  set_float_info(self, field, data)\n",
      " |      Set float type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_float_info_npy2d(self, field, data)\n",
      " |      Set float type property into the DMatrix\n",
      " |         for numpy 2d array input\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_group(self, group)\n",
      " |      Set group size of DMatrix (used for ranking).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : array like\n",
      " |          Group size of each group\n",
      " |  \n",
      " |  set_interface_info(self, field, data)\n",
      " |      Set info type property into DMatrix.\n",
      " |  \n",
      " |  set_label(self, label)\n",
      " |      Set label of dmatrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      label: array like\n",
      " |          The label information to be set into DMatrix\n",
      " |  \n",
      " |  set_uint_info(self, field, data)\n",
      " |      Set uint type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_weight(self, weight)\n",
      " |      Set weight of each instance.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weight : array like\n",
      " |          Weight for each data point\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each\n",
      " |              data point). This is because we only care about the relative\n",
      " |              ordering of data points within each group, so it doesn't make\n",
      " |              sense to assign weights to individual data points.\n",
      " |  \n",
      " |  slice(self, rindex, allow_groups=False)\n",
      " |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rindex : list\n",
      " |          List of indices to be selected.\n",
      " |      allow_groups : boolean\n",
      " |          Allow slicing of a matrix with a groups attribute\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      res : DMatrix\n",
      " |          A new DMatrix containing only selected indices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  feature_names\n",
      " |      Get feature names (column labels).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list or None\n",
      " |  \n",
      " |  feature_types\n",
      " |      Get feature types (column types).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_types : list or None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.DMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training data object:\n",
    "dtrain = xgb.DMatrix(data=df_train.drop(drop_vars, axis=1),\n",
    "                     label=df_train['y'])\n",
    "\n",
    "# Creating the test data object:\n",
    "dtest = xgb.DMatrix(data=df_test.drop(drop_vars, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round: int\n",
      "        Number of boosting iterations.\n",
      "    evals: list of pairs (DMatrix, string)\n",
      "        List of validation sets for which metrics will evaluated during training.\n",
      "        Validation metrics will help us track the performance of the model.\n",
      "    obj : function\n",
      "        Customized objective function.\n",
      "    feval : function\n",
      "        Customized evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Validation metric needs to improve at least once in\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        The method returns the model from the last iteration (not the best one).\n",
      "        If there's more than one item in **evals**, the last entry will be used\n",
      "        for early stopping.\n",
      "        If there's more than one metric in the **eval_metric** parameter given in\n",
      "        **params**, the last metric will be used for early stopping.\n",
      "        If early stopping occurs, the model will have three additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "        (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "        ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "    evals_result: dict\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval : bool or int\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring dictionary with hyper-parameters:\n",
    "param = {'subsample': 0.75, 'eta': 0.1, 'max_depth': 3, 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model:\n",
    "xgb_model = xgb.train(params=param, dtrain=dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting scores for test data:\n",
    "score_pred_test = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerformance metrics (test data):\u001b[0m\n",
      "Test ROC-AUC: 0.9901764234161989.\n",
      "Test average precision score: 0.9625007990634594.\n",
      "Test Brier score: 0.004469397445861866.\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics:\n",
    "test_roc_auc = roc_auc_score(df_test['y'], score_pred_test)\n",
    "test_prec_avg = average_precision_score(df_test['y'], score_pred_test)\n",
    "test_brier = brier_score_loss(df_test['y'], score_pred_test)\n",
    "\n",
    "print('\\033[1mPerformance metrics (test data):\\033[0m')\n",
    "print(f'Test ROC-AUC: {test_roc_auc}.')\n",
    "print(f'Test average precision score: {test_prec_avg}.')\n",
    "print(f'Test Brier score: {test_brier}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevant attributes or methods of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the ensemble of trees using a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree</th>\n",
       "      <th>Node</th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Split</th>\n",
       "      <th>Yes</th>\n",
       "      <th>No</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Gain</th>\n",
       "      <th>Cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>feat_879</td>\n",
       "      <td>2.838081</td>\n",
       "      <td>0-1</td>\n",
       "      <td>0-2</td>\n",
       "      <td>0-1</td>\n",
       "      <td>571.748047</td>\n",
       "      <td>1360.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>feat_886</td>\n",
       "      <td>5.626141</td>\n",
       "      <td>0-3</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0-3</td>\n",
       "      <td>123.749512</td>\n",
       "      <td>1313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0-2</td>\n",
       "      <td>feat_1204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0-5</td>\n",
       "      <td>0-6</td>\n",
       "      <td>0-5</td>\n",
       "      <td>30.966598</td>\n",
       "      <td>47.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0-3</td>\n",
       "      <td>feat_175</td>\n",
       "      <td>2.996223</td>\n",
       "      <td>0-7</td>\n",
       "      <td>0-8</td>\n",
       "      <td>0-7</td>\n",
       "      <td>13.816406</td>\n",
       "      <td>1303.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>feat_120</td>\n",
       "      <td>-0.287084</td>\n",
       "      <td>0-9</td>\n",
       "      <td>0-10</td>\n",
       "      <td>0-9</td>\n",
       "      <td>2.566416</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0-5</td>\n",
       "      <td>feat_137</td>\n",
       "      <td>-0.076741</td>\n",
       "      <td>0-11</td>\n",
       "      <td>0-12</td>\n",
       "      <td>0-11</td>\n",
       "      <td>7.285355</td>\n",
       "      <td>44.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0-6</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0-7</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.197852</td>\n",
       "      <td>1302.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0-8</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0-9</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0-11</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0-12</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182123</td>\n",
       "      <td>43.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1-0</td>\n",
       "      <td>feat_879</td>\n",
       "      <td>2.838081</td>\n",
       "      <td>1-1</td>\n",
       "      <td>1-2</td>\n",
       "      <td>1-1</td>\n",
       "      <td>441.762207</td>\n",
       "      <td>1349.875370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-1</td>\n",
       "      <td>feat_886</td>\n",
       "      <td>5.626141</td>\n",
       "      <td>1-3</td>\n",
       "      <td>1-4</td>\n",
       "      <td>1-3</td>\n",
       "      <td>96.127441</td>\n",
       "      <td>1304.730220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1-2</td>\n",
       "      <td>feat_1204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1-5</td>\n",
       "      <td>1-6</td>\n",
       "      <td>1-5</td>\n",
       "      <td>20.016945</td>\n",
       "      <td>45.145187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1-3</td>\n",
       "      <td>feat_175</td>\n",
       "      <td>2.996223</td>\n",
       "      <td>1-7</td>\n",
       "      <td>1-8</td>\n",
       "      <td>1-7</td>\n",
       "      <td>15.104004</td>\n",
       "      <td>1295.786620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1-4</td>\n",
       "      <td>feat_109</td>\n",
       "      <td>0.560863</td>\n",
       "      <td>1-9</td>\n",
       "      <td>1-10</td>\n",
       "      <td>1-9</td>\n",
       "      <td>2.379816</td>\n",
       "      <td>8.943536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1-5</td>\n",
       "      <td>feat_137</td>\n",
       "      <td>-0.076741</td>\n",
       "      <td>1-11</td>\n",
       "      <td>1-12</td>\n",
       "      <td>1-11</td>\n",
       "      <td>8.615234</td>\n",
       "      <td>42.654926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1-6</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.105663</td>\n",
       "      <td>2.490260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1-7</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.180211</td>\n",
       "      <td>1294.539790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1-8</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105707</td>\n",
       "      <td>1.246880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1-9</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>1.989413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1-10</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.162279</td>\n",
       "      <td>6.954123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1-11</td>\n",
       "      <td>Leaf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.063912</td>\n",
       "      <td>1.249219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tree  Node    ID    Feature     Split   Yes    No Missing        Gain  \\\n",
       "0      0     0   0-0   feat_879  2.838081   0-1   0-2     0-1  571.748047   \n",
       "1      0     1   0-1   feat_886  5.626141   0-3   0-4     0-3  123.749512   \n",
       "2      0     2   0-2  feat_1204  1.000000   0-5   0-6     0-5   30.966598   \n",
       "3      0     3   0-3   feat_175  2.996223   0-7   0-8     0-7   13.816406   \n",
       "4      0     4   0-4   feat_120 -0.287084   0-9  0-10     0-9    2.566416   \n",
       "5      0     5   0-5   feat_137 -0.076741  0-11  0-12    0-11    7.285355   \n",
       "6      0     6   0-6       Leaf       NaN   NaN   NaN     NaN   -0.125000   \n",
       "7      0     7   0-7       Leaf       NaN   NaN   NaN     NaN   -0.197852   \n",
       "8      0     8   0-8       Leaf       NaN   NaN   NaN     NaN    0.100000   \n",
       "9      0     9   0-9       Leaf       NaN   NaN   NaN     NaN   -0.000000   \n",
       "10     0    10  0-10       Leaf       NaN   NaN   NaN     NaN    0.168421   \n",
       "11     0    11  0-11       Leaf       NaN   NaN   NaN     NaN   -0.050000   \n",
       "12     0    12  0-12       Leaf       NaN   NaN   NaN     NaN    0.182123   \n",
       "13     1     0   1-0   feat_879  2.838081   1-1   1-2     1-1  441.762207   \n",
       "14     1     1   1-1   feat_886  5.626141   1-3   1-4     1-3   96.127441   \n",
       "15     1     2   1-2  feat_1204  1.000000   1-5   1-6     1-5   20.016945   \n",
       "16     1     3   1-3   feat_175  2.996223   1-7   1-8     1-7   15.104004   \n",
       "17     1     4   1-4   feat_109  0.560863   1-9  1-10     1-9    2.379816   \n",
       "18     1     5   1-5   feat_137 -0.076741  1-11  1-12    1-11    8.615234   \n",
       "19     1     6   1-6       Leaf       NaN   NaN   NaN     NaN   -0.105663   \n",
       "20     1     7   1-7       Leaf       NaN   NaN   NaN     NaN   -0.180211   \n",
       "21     1     8   1-8       Leaf       NaN   NaN   NaN     NaN    0.105707   \n",
       "22     1     9   1-9       Leaf       NaN   NaN   NaN     NaN    0.025020   \n",
       "23     1    10  1-10       Leaf       NaN   NaN   NaN     NaN    0.162279   \n",
       "24     1    11  1-11       Leaf       NaN   NaN   NaN     NaN   -0.063912   \n",
       "\n",
       "          Cover  \n",
       "0   1360.750000  \n",
       "1   1313.000000  \n",
       "2     47.750000  \n",
       "3   1303.500000  \n",
       "4      9.500000  \n",
       "5     44.750000  \n",
       "6      3.000000  \n",
       "7   1302.500000  \n",
       "8      1.000000  \n",
       "9      1.000000  \n",
       "10     8.500000  \n",
       "11     1.000000  \n",
       "12    43.750000  \n",
       "13  1349.875370  \n",
       "14  1304.730220  \n",
       "15    45.145187  \n",
       "16  1295.786620  \n",
       "17     8.943536  \n",
       "18    42.654926  \n",
       "19     2.490260  \n",
       "20  1294.539790  \n",
       "21     1.246880  \n",
       "22     1.989413  \n",
       "23     6.954123  \n",
       "24     1.249219  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.trees_to_dataframe().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='es'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training data object:\n",
    "dtrain = xgb.DMatrix(data=df_train.drop(drop_vars, axis=1),\n",
    "                     label=df_train['y'])\n",
    "\n",
    "# Creating the validation data object:\n",
    "dval = xgb.DMatrix(data=df_test.drop(drop_vars, axis=1),\n",
    "                   label=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring dictionary with hyper-parameters:\n",
    "param = {'subsample': 0.75, 'eta': 0.1, 'max_depth': 3, 'objective': 'binary:logistic',\n",
    "         'eval_metric': ['logloss', 'auc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval_data-logloss:0.60059\tval_data-auc:0.96612\n",
      "Multiple eval metrics have been passed: 'val_data-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val_data-auc hasn't improved in 20 rounds.\n",
      "[1]\tval_data-logloss:0.52463\tval_data-auc:0.96614\n",
      "[2]\tval_data-logloss:0.46108\tval_data-auc:0.96616\n",
      "[3]\tval_data-logloss:0.40777\tval_data-auc:0.96611\n",
      "[4]\tval_data-logloss:0.36199\tval_data-auc:0.96611\n",
      "[5]\tval_data-logloss:0.32237\tval_data-auc:0.96611\n",
      "[6]\tval_data-logloss:0.28813\tval_data-auc:0.96611\n",
      "[7]\tval_data-logloss:0.25859\tval_data-auc:0.96744\n",
      "[8]\tval_data-logloss:0.23256\tval_data-auc:0.96746\n",
      "[9]\tval_data-logloss:0.20987\tval_data-auc:0.96744\n",
      "[10]\tval_data-logloss:0.18959\tval_data-auc:0.96745\n",
      "[11]\tval_data-logloss:0.17184\tval_data-auc:0.96742\n",
      "[12]\tval_data-logloss:0.15580\tval_data-auc:0.96742\n",
      "[13]\tval_data-logloss:0.14160\tval_data-auc:0.96735\n",
      "[14]\tval_data-logloss:0.12907\tval_data-auc:0.96714\n",
      "[15]\tval_data-logloss:0.11793\tval_data-auc:0.96716\n",
      "[16]\tval_data-logloss:0.10800\tval_data-auc:0.96696\n",
      "[17]\tval_data-logloss:0.09928\tval_data-auc:0.96696\n",
      "[18]\tval_data-logloss:0.09128\tval_data-auc:0.96700\n",
      "[19]\tval_data-logloss:0.08432\tval_data-auc:0.96700\n",
      "[20]\tval_data-logloss:0.07806\tval_data-auc:0.97095\n",
      "[21]\tval_data-logloss:0.07253\tval_data-auc:0.97088\n",
      "[22]\tval_data-logloss:0.06733\tval_data-auc:0.97085\n",
      "[23]\tval_data-logloss:0.06285\tval_data-auc:0.97337\n",
      "[24]\tval_data-logloss:0.05902\tval_data-auc:0.97696\n",
      "[25]\tval_data-logloss:0.05534\tval_data-auc:0.97701\n",
      "[26]\tval_data-logloss:0.05227\tval_data-auc:0.97677\n",
      "[27]\tval_data-logloss:0.04935\tval_data-auc:0.97677\n",
      "[28]\tval_data-logloss:0.04688\tval_data-auc:0.97750\n",
      "[29]\tval_data-logloss:0.04461\tval_data-auc:0.97744\n",
      "[30]\tval_data-logloss:0.04248\tval_data-auc:0.97735\n",
      "[31]\tval_data-logloss:0.04067\tval_data-auc:0.97754\n",
      "[32]\tval_data-logloss:0.03881\tval_data-auc:0.97751\n",
      "[33]\tval_data-logloss:0.03725\tval_data-auc:0.97866\n",
      "[34]\tval_data-logloss:0.03584\tval_data-auc:0.98920\n",
      "[35]\tval_data-logloss:0.03452\tval_data-auc:0.99087\n",
      "[36]\tval_data-logloss:0.03361\tval_data-auc:0.99052\n",
      "[37]\tval_data-logloss:0.03261\tval_data-auc:0.99203\n",
      "[38]\tval_data-logloss:0.03186\tval_data-auc:0.99135\n",
      "[39]\tval_data-logloss:0.03093\tval_data-auc:0.99248\n",
      "[40]\tval_data-logloss:0.03012\tval_data-auc:0.99311\n",
      "[41]\tval_data-logloss:0.02944\tval_data-auc:0.99414\n",
      "[42]\tval_data-logloss:0.02885\tval_data-auc:0.99370\n",
      "[43]\tval_data-logloss:0.02834\tval_data-auc:0.99319\n",
      "[44]\tval_data-logloss:0.02789\tval_data-auc:0.99309\n",
      "[45]\tval_data-logloss:0.02727\tval_data-auc:0.99364\n",
      "[46]\tval_data-logloss:0.02681\tval_data-auc:0.99357\n",
      "[47]\tval_data-logloss:0.02647\tval_data-auc:0.99334\n",
      "[48]\tval_data-logloss:0.02611\tval_data-auc:0.99318\n",
      "[49]\tval_data-logloss:0.02583\tval_data-auc:0.99331\n",
      "[50]\tval_data-logloss:0.02550\tval_data-auc:0.99361\n",
      "[51]\tval_data-logloss:0.02537\tval_data-auc:0.99339\n",
      "[52]\tval_data-logloss:0.02510\tval_data-auc:0.99329\n",
      "[53]\tval_data-logloss:0.02478\tval_data-auc:0.99333\n",
      "[54]\tval_data-logloss:0.02451\tval_data-auc:0.99345\n",
      "[55]\tval_data-logloss:0.02440\tval_data-auc:0.99340\n",
      "[56]\tval_data-logloss:0.02435\tval_data-auc:0.99299\n",
      "[57]\tval_data-logloss:0.02434\tval_data-auc:0.99232\n",
      "[58]\tval_data-logloss:0.02413\tval_data-auc:0.99266\n",
      "[59]\tval_data-logloss:0.02398\tval_data-auc:0.99250\n",
      "[60]\tval_data-logloss:0.02407\tval_data-auc:0.99216\n",
      "[61]\tval_data-logloss:0.02407\tval_data-auc:0.99182\n",
      "Stopping. Best iteration:\n",
      "[41]\tval_data-logloss:0.02944\tval_data-auc:0.99414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model:\n",
    "xgb_model = xgb.train(params=param, dtrain=dtrain, num_boost_round=100,\n",
    "                      evals=[(dval, 'val_data')], early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation ROC-AUC: 0.9941\n",
      "Best iteration: 41\n"
     ]
    }
   ],
   "source": [
    "print(f'Best validation ROC-AUC: {xgb_model.best_score:.4f}\\nBest iteration: {xgb_model.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module xgboost.core:\n",
      "\n",
      "predict(data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False, pred_interactions=False, validate_features=True, training=False) method of xgboost.core.Booster instance\n",
      "    Predict with data.\n",
      "    \n",
      "    .. note:: This function is not thread safe except for ``gbtree``\n",
      "              booster.\n",
      "    \n",
      "      For ``gbtree`` booster, the thread safety is guaranteed by locks.\n",
      "      For lock free prediction use ``inplace_predict`` instead.  Also, the\n",
      "      safety does not hold when used in conjunction with other methods.\n",
      "    \n",
      "      When using booster other than ``gbtree``, predict can only be called\n",
      "      from one thread.  If you want to run prediction using multiple\n",
      "      thread, call ``bst.copy()`` to make copies of model object and then\n",
      "      call ``predict()``.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : DMatrix\n",
      "        The dmatrix storing the input.\n",
      "    \n",
      "    output_margin : bool\n",
      "        Whether to output the raw untransformed margin value.\n",
      "    \n",
      "    ntree_limit : int\n",
      "        Limit number of trees in the prediction; defaults to 0 (use all\n",
      "        trees).\n",
      "    \n",
      "    pred_leaf : bool\n",
      "        When this option is on, the output will be a matrix of (nsample,\n",
      "        ntrees) with each record indicating the predicted leaf index of\n",
      "        each sample in each tree.  Note that the leaf index of a tree is\n",
      "        unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n",
      "    \n",
      "    pred_contribs : bool\n",
      "        When this is True the output will be a matrix of size (nsample,\n",
      "        nfeats + 1) with each record indicating the feature contributions\n",
      "        (SHAP values) for that prediction. The sum of all feature\n",
      "        contributions is equal to the raw untransformed margin value of the\n",
      "        prediction. Note the final column is the bias term.\n",
      "    \n",
      "    approx_contribs : bool\n",
      "        Approximate the contributions of each feature\n",
      "    \n",
      "    pred_interactions : bool\n",
      "        When this is True the output will be a matrix of size (nsample,\n",
      "        nfeats + 1, nfeats + 1) indicating the SHAP interaction values for\n",
      "        each pair of features. The sum of each row (or column) of the\n",
      "        interaction values equals the corresponding SHAP value (from\n",
      "        pred_contribs), and the sum of the entire matrix equals the raw\n",
      "        untransformed margin value of the prediction. Note the last row and\n",
      "        column correspond to the bias term.\n",
      "    \n",
      "    validate_features : bool\n",
      "        When this is True, validate that the Booster's and data's\n",
      "        feature_names are identical.  Otherwise, it is assumed that the\n",
      "        feature_names are the same.\n",
      "    \n",
      "    training : bool\n",
      "        Whether the prediction value is used for training.  This can effect\n",
      "        `dart` booster, which performs dropouts during training iterations.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    .. note:: Using ``predict()`` with DART booster\n",
      "    \n",
      "      If the booster object is DART type, ``predict()`` will not perform\n",
      "      dropouts, i.e. all the trees will be evaluated.  If you want to\n",
      "      obtain result with dropouts, provide `training=True`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    prediction : numpy array\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb_model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting scores for validation data using model as it was in the best iteration:\n",
    "score_pred_val = xgb_model.predict(dval, ntree_limit=xgb_model.best_iteration+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerformance metrics (validation data):\u001b[0m\n",
      "Test ROC-AUC: 0.9941393098711168.\n",
      "Test average precision score: 0.9626073602847985.\n",
      "Test Brier score: 0.004270969364380872.\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics:\n",
    "val_roc_auc = roc_auc_score(df_test['y'], score_pred_val)\n",
    "val_prec_avg = average_precision_score(df_test['y'], score_pred_val)\n",
    "val_brier = brier_score_loss(df_test['y'], score_pred_val)\n",
    "\n",
    "print('\\033[1mPerformance metrics (validation data):\\033[0m')\n",
    "print(f'Test ROC-AUC: {val_roc_auc}.')\n",
    "print(f'Test average precision score: {val_prec_avg}.')\n",
    "print(f'Test Brier score: {val_brier}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training data object:\n",
    "dtrain = xgb.DMatrix(data=df_train.drop(drop_vars, axis=1),\n",
    "                     label=df_train['y'])\n",
    "\n",
    "# Creating the test data object:\n",
    "dtest = xgb.DMatrix(data=df_test.drop(drop_vars, axis=1),\n",
    "                    label=df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cv in module xgboost.training:\n",
      "\n",
      "cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "    Cross-validation with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params : dict\n",
      "        Booster params.\n",
      "    dtrain : DMatrix\n",
      "        Data to be trained.\n",
      "    num_boost_round : int\n",
      "        Number of boosting iterations.\n",
      "    nfold : int\n",
      "        Number of folds in CV.\n",
      "    stratified : bool\n",
      "        Perform stratified sampling.\n",
      "    folds : a KFold or StratifiedKFold instance or list of fold indices\n",
      "        Sklearn KFolds or StratifiedKFolds object.\n",
      "        Alternatively may explicitly pass sample indices for each fold.\n",
      "        For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
      "        Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
      "        as the training samples for the ``n`` th fold and ``out`` is a list of\n",
      "        indices to be used as the testing samples for the ``n`` th fold.\n",
      "    metrics : string or list of strings\n",
      "        Evaluation metrics to be watched in CV.\n",
      "    obj : function\n",
      "        Custom objective function.\n",
      "    feval : function\n",
      "        Custom evaluation function.\n",
      "    maximize : bool\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds: int\n",
      "        Activates early stopping. Cross-Validation metric (average of validation\n",
      "        metric computed over CV folds) needs to improve at least once in\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        The last entry in the evaluation history will represent the best iteration.\n",
      "        If there's more than one metric in the **eval_metric** parameter given in\n",
      "        **params**, the last metric will be used for early stopping.\n",
      "    fpreproc : function\n",
      "        Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "        transformed versions of those.\n",
      "    as_pandas : bool, default True\n",
      "        Return pd.DataFrame when pandas is installed.\n",
      "        If False or pandas is not installed, return np.ndarray\n",
      "    verbose_eval : bool, int, or None, default None\n",
      "        Whether to display the progress. If None, progress will be displayed\n",
      "        when np.ndarray is returned. If True, progress will be displayed at\n",
      "        boosting stage. If an integer is given, progress will be displayed\n",
      "        at every given `verbose_eval` boosting stage.\n",
      "    show_stdv : bool, default True\n",
      "        Whether to display the standard deviation in progress.\n",
      "        Results are not affected, and always contains std.\n",
      "    seed : int\n",
      "        Seed used to generate the folds (passed to numpy.random.seed).\n",
      "    callbacks : list of callback functions\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "        Example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "    shuffle : bool\n",
      "        Shuffle data before creating folds.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    evaluation history : list(string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring dictionary with hyper-parameters:\n",
    "param = {'subsample': 0.75, 'eta': 0.1, 'max_depth': 3, 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model:\n",
    "xgb_model = xgb.cv(params=param, dtrain=dtrain, num_boost_round=100,\n",
    "                   nfold=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>0.001357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007413</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.009284</td>\n",
       "      <td>0.001708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.001530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.001708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.002208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.002208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0          0.007690         0.000294         0.009977        0.001357\n",
       "1          0.007413         0.000197         0.009284        0.001708\n",
       "2          0.007621         0.000197         0.008868        0.001530\n",
       "3          0.007136         0.000260         0.008314        0.002036\n",
       "4          0.007482         0.000171         0.008591        0.001708\n",
       "5          0.006997         0.000393         0.008314        0.002036\n",
       "6          0.006997         0.000260         0.008314        0.002036\n",
       "7          0.006997         0.000260         0.008314        0.002036\n",
       "8          0.006720         0.000596         0.008452        0.002208\n",
       "9          0.006651         0.000340         0.008452        0.002208"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grid_random_searches'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid and random searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGrid estimation progress:\u001b[0m [----------------------------------------------] 100%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\u001b[1mTrain-test estimation outcomes:\u001b[0m\n",
      "\n",
      "\n",
      "Outcomes from K-folds CV estimation:\n",
      "   Number of data folds: 3.\n",
      "   Number of samples for random search: 10.\n",
      "   Estimation method: xgboost.\n",
      "   Metric for choosing best hyper-parameter: roc_auc.\n",
      "   Best hyper-parameters: {'subsample': 0.6958157064067576, 'eta': 0.03823028413810232, 'max_depth': 4, 'num_boost_round': 500}.\n",
      "   CV performance metric associated with best hyper-parameters: 0.9799.\n",
      "\n",
      "\n",
      "Performance metrics evaluated at test data:\n",
      "   test_roc_auc = 0.9889\n",
      "   test_prec_avg = 0.9601\n",
      "   test_brier = 0.0046\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "------------------------------------\n",
      "\u001b[1mRunning time:\u001b[0m 6.3 minutes.\n",
      "Start time: 2021-06-13, 18:38:03\n",
      "End time: 2021-06-13, 18:44:21\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Grid of hyper-parameters:\n",
    "grid_param = {'subsample': uniform(0.5, 0.5),\n",
    "              'eta': uniform(0.0001, 0.1),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'num_boost_round': [100, 250, 500]}\n",
    "\n",
    "# Creating K-folds CV object:\n",
    "kfolds = Kfolds_fit(task = 'binary:logistic', method = 'xgboost', num_folds = 3, metric = 'roc_auc',\n",
    "                    random_search = True, n_samples = 10,\n",
    "                    grid_param = grid_param,\n",
    "                    default_param = {'subsample': 0.75,\n",
    "                                     'eta': 0.01,\n",
    "                                     'max_depth': 10,\n",
    "                                     'num_boost_round': 100})\n",
    "\n",
    "# Running K-folds CV:\n",
    "kfolds.fit(train_inputs = df_train.drop(drop_vars, axis=1), train_output = df_train['y'],\n",
    "           test_inputs = df_test.drop(drop_vars, axis=1), test_output = df_test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPerformance metrics (test data):\u001b[0m\n",
      "Test ROC-AUC: 0.9889312408852977.\n",
      "Test average precision score: 0.9601162681393539.\n",
      "Test Brier score: 0.0046171217817736225.\n",
      "\n",
      "\n",
      "\u001b[1mBest hyper-parameters:\u001b[0m\n",
      "{'subsample': 0.6958157064067576, 'eta': 0.03823028413810232, 'max_depth': 4, 'num_boost_round': 500}\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics:\n",
    "test_roc_auc = kfolds.performance_metrics['test_roc_auc']\n",
    "test_prec_avg = kfolds.performance_metrics['test_prec_avg']\n",
    "test_brier = kfolds.performance_metrics['test_brier']\n",
    "\n",
    "print('\\033[1mPerformance metrics (test data):\\033[0m')\n",
    "print(f'Test ROC-AUC: {test_roc_auc}.')\n",
    "print(f'Test average precision score: {test_prec_avg}.')\n",
    "print(f'Test Brier score: {test_brier}.')\n",
    "print('\\n')\n",
    "\n",
    "print('\\033[1mBest hyper-parameters:\\033[0m')\n",
    "print(kfolds.best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
