{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid vs. random searches\n",
    "## Experiments design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests aim to oppose two alternative approaches for the definition of hyper-parameters of statistical learning methods: **grid search** and **random search**. Both explore subsets of the hyper-parameters space in order to tune them, thus improving predictive performance of models. The main difference between them, however, relies on the fact that grid search executes a guided look over hyper-parameter values, while random search explores values in a stochastic manner. Consequently, the first needs pre-selection of appropriate values, while the second requires only the definition of appropriate ranges of values.\n",
    "<br>\n",
    "<br>\n",
    "This notebook will describe experiments that will be conducted so that the impacts on performance metrics of both approaches can be assessed. Then, python scripts will be constructed to implement tests, while a second notebook will provide the analysis and discussion of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search and random search are different, but related approaches for choosing values of hyper-parameters $\\theta \\in \\mathbb{R}^L$ from an arbitrarily extensive grid of values. **Grid search** considers the following sets of values for hyper-parameters in vector $\\theta$:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\theta_1 \\in \\Theta_1 = \\{\\theta_{11}, \\theta_{12}, ..., \\theta_{1s_1}\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\theta_2 \\in \\Theta_2 = \\{\\theta_{21}, \\theta_{22}, ..., \\theta_{2s_2}\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\vdots\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\theta_L \\in \\Theta_L = \\{\\theta_{L1}, \\theta_{L2}, ..., \\theta_{Ls_L}\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<br>\n",
    "Then, a complete grid $\\Theta = \\Theta_1x\\Theta_2x...\\Theta_L$ covering all combinations among elements of $\\Theta_1$, $\\Theta_2$, ..., $\\Theta_L$ is sequentially tested.\n",
    "<br>\n",
    "<br>\n",
    "While grid search looks across a discrete set of combinations, **random search** can be implemented over a continuum of values, which drastically increases the range of possible values to be tested. Now, $\\Theta_l$ may represent intervals, instead of only sets. For instance, $\\theta_l' \\in \\Theta_l' = [\\theta_{0l'}, \\theta_{1l'}]$ and $\\theta_l' \\in \\Theta_l' = \\{\\theta_{l''1}, \\theta_{l''2}, ..., \\theta_{l''s_{l''}}\\}$. Still, a potentially continuous grid of values is given by the Cartesian product $\\Theta = \\Theta_1x\\Theta_2x...\\Theta_L$.\n",
    "<br>\n",
    "<br>\n",
    "Another difference between grid and random searches points to the deterministic aspect of grid search against the stochastic character of random search. All values in $\\Theta$ are tested during a grid search, while only a random sample of size $S$ is collected from $\\Theta$ and then tested during a random search.\n",
    "<br>\n",
    "<br>\n",
    "**Pros and cons:**\n",
    "* If there are only a few distinct hyper-parameters, and assuming that appropriate values for them have already been defined, then grid search may be more assertive and lead to good values more rapidly.\n",
    "* Since random search depends on an adequate definition of supports for hyper-parameters and of a sufficiently large number of samples, it may imply unstable choices that generalize poorly on future batches of data.\n",
    "* While grid search is more appropriate for discrete hyper-parameters, those which are continuous are handled better by random search.\n",
    "* Grid search requires previous inquirements or theoretical knowledge for the definition of appropriate values for hyper-parameters.\n",
    "* Random search may require a number substantially smaller of random samples in order to look into relatively large neighborhoods of appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** this study aims to find empirical evidences of which approach suits better for the hyper-parameters definition, taking as references performance metrics evaluated on test data, besides running time and computational complexity of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments design**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different learning methods are going to be used during tests: **logistic regression** and **Gradient Boosting Model (GBM)**. While the first is a simple method that constructs a linear decision boundary on the features space, the second is pretty more flexible, though containing a few more hyper-parameters to be set. Therefore, grid and random searches can be opposed based on learning methods that require different levels of parameter tuning.\n",
    "<br>\n",
    "<br>\n",
    "**Hyper-parameters:** starting with logistic regression, the only hyper-parameter whose value needs to be chosen is the *regularization parameter*, $\\lambda$. This hyper-parameter defines the strength of L1 regularization for shrinkage and features selection. GBM, in its turn, has several distinct hyper-parameters to be set, and only the four main of them will be explored here: *subsample rate* $\\eta$, share of the training data to be sampled for each estimation in the ensemble; *max depth* $J$, the maximum number of splits in each tree composing the ensemble; *learning rate* $v$, the shrinkage parameter for the estimators; and the *number of estimators* $M$, the total number of distinct estimators in the ensemble.\n",
    "<br>\n",
    "<br>\n",
    "**Grid search - logistic regression:** for the *regularization parameter*, the following set of values will be successively tested in a grid search:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\{0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.25, 0.3, 0.5, 0.75, 1, 3, 10\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "**Random search - logistic regression:** likewise, random search for this hyper-parameter will be constructed upon an interval whose limits are given by minimum and maximum values of the set above: $[0.0001, 10]$. Then, 14 randomly picked values will be extracted from a *uniform distribution* whose support is the interval $[0.0001, 10]$.\n",
    "<br>\n",
    "<br>\n",
    "**Grid search - GBM:** for all four hyper-parameters $\\{\\eta, J, v, M\\}$, appropriate values were pre-selected based on the literature and previous inquirements:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\eta \\in \\{0.75\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle J \\in \\{1, 3, 5\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle v \\in \\{0.0001, 0.01, 0.1\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle M \\in \\{250, 500\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "Then, all combinations of them will be tested on, implying in a grid search over the following set which make up 27 different alternatives $(\\eta, J, v, M)$:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{equation}\n",
    "    \\displaystyle \\{0.75\\}x\\{1, 3, 5\\}x\\{0.0001, 0.01, 0.1\\}x\\{100, 250, 500\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "**Random search - GBM:** random search will make use of the same range of values, but randomly extracted from a continuum of values, for learning rate, or a large set of discrete alternatives, for maximum depth and for number of estimators. So, random search will extract 20 random combinations $(\\eta, J, v, M)$ from $\\{0.75\\}x[1,2,3,4,5\\}x[0.0001, 0.1]x[100,500]$.\n",
    "<br>\n",
    "<br>\n",
    "**Datasets:** tests will be constructed upon 30 different datasets of distinct lengths all conducting to a binary classification task.\n",
    "<br>\n",
    "<br>\n",
    "**Validation:** data will be split on train and test main folds, with K-folds CV being applied to training data in order to choose hyper-parameter values, while test data will be used to evaluate performance metrics that should help opposing grid and random searches.\n",
    "<br>\n",
    "<br>\n",
    "**Guidelines for conclusion:** ROC-AUC, average precision score and Brier score will be evaluated on test data, revealing which approach (grid or random search) has the best expected performance for the learning tasks. Distribution of such metrics will be opposed for both approaches, and also running time will be used to weight performance considering computational costs involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
