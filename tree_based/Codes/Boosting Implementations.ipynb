{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boosting Implementations.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOcpJDus+0tO9GA33dCWVd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7rbt6qO5tBQ2"},"source":["## Boosting implementations"]},{"cell_type":"markdown","source":["**Boosting** is a general learning approach that combines multiple weak learners constructed in a way that bias is reduced at each iteration. So, the main aspect of boosting fitting procedures is the progressive learning by the mistakes the ensemble is doing while each of its components is fitted. There are a wide range of algorithms that follow the boosting principle, but two of them are currently the most used: AdaBoost and gradient boosted trees.\n","\n","Differently from random forests, which try to reduce variance among the trees in the ensemble, boosting methods focus on controling the bias. While random forests produce estimators with low bias, but in a way that the overall variance is minimized, boosting works with individual estimators that have low variance, and the process of constructing the entire ensemble seeks to minimize the bias. Consequently, boosting takes a collection of simple models and combines them so that a complex final model can be produced.\n","\n","This notebook, in addition to the previous theoretical discussion, brings some codes implementing boosted models from scratch. The main reference is this [article](https://towardsdatascience.com/statistical-machine-learning-gradient-boosting-adaboost-from-scratch-8c4b5a9db9ed), but [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) is a crucial reading for understanding the inner functioning of this (and many other) learning algorithms. As found in this book, one can understand the boosting principle through **AdaBoost**, an algorithm for binary classification which explicitly reproduces the idea under which, given an estimator in the ensemble, incorrectly predicted instances get more weight in the construction of the following estimator. The following algorithm is an alternative for implementing AdaBoost, and its steps illustrate the boosting principle:\n","1. Define initial weights $w_i = 1/N$ for each data point $i \\in \\{1, 2, ..., N\\}$.\n","2. For each step $m \\in \\{1, 2, ..., M\\}$:\n","  1. Fit a classifier $G_m(x)$ using data points weighted by $w_i$.\n","  2. Calculate the following weighted classification error:\n","  \\begin{equation}\n","      \\displaystyle err_m = \\frac{\\sum_{i=1}^Nw_iI(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N w_i}\n","  \\end{equation}\n","\n","  3. Calculate the following quantity:\n","  \\begin{equation}\n","      \\alpha_m = \\log((1 - err_m)/err_m)\n","  \\end{equation}\n","\n","  Note that the higher $err_m$, the lower will be $\\alpha_m$.\n","  4. Redefine the weights:\n","  \\begin{equation}\n","      \\displaystyle w_i \\leftarrow w_i.\\exp(\\alpha_mI(y_i \\neq G_m(x_i)))\n","  \\end{equation}\n","\n","  Note that more weight is put to misclassified observations.\n","\n","3. Define the output from the algorithm by:\n","  \\begin{equation}\n","      \\displaystyle g(x) = sign\\Big(\\sum_{m=1}^M\\alpha_mG_m(x)\\Big)\n","  \\end{equation}\n","  The AdaBoost.M1 algorithm should be adapted if instead of a class prediction, the objective is to compute class-probabilities.\n","\n","Although very similar to AdaBoost, **gradient boosted trees** algorithm is another implementation that explicitly reproduces a fundamental principal of boosting: each estimator is constructed using information on the amount of error the current ensemble has made. The following algorithm implements a boosting model on a regression setting:\n","1. Define the initial guess:\n","\\begin{equation}\n","    \\displaystyle f_0(x) = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_{i=1}^N L(y_i, \\gamma)\n","\\end{equation}\n","2. For each step $m \\in \\{1, 2, ..., M\\}$:\n","  1. For each observation, compute:\n","  \\begin{equation}\n","      \\displaystyle r_{mi} = -\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\Big|_{f = f_{m-1}}\n","  \\end{equation}\n","  Note that $f(.)$ refers to the current ensemble.\n","\n","  2. Estimate a regression tree, where the response variable is given by $r_{mi}$, providing regions $R_{mj}$, with $j \\in \\{1, 2, ..., J_m\\}$.\n","  3. For each $j \\in \\{1, 2, ..., J_m\\}$, calculate:\n","  \\begin{equation}\n","      \\displaystyle \\gamma_{mj} = \\underset{\\gamma}{\\mathrm{argmin}} \\sum_{x_i \\in R_{mj}} L(y_i, f_{m-1}(x_i) + \\gamma)\n","  \\end{equation}\n","  4. Update the gradient boosting estimate:\n","  \\begin{equation}\n","      \\displaystyle f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m} \\gamma_{mj}I(x \\in R_{mj})\n","  \\end{equation}\n","\n","3. Define the final estimator by $\\hat{f}(x) = f_M(x)$.\n","\n","The gradient tree boosting algorithm applied for classification should be adapted properly. Mainly, steps 2a-2d are repeated for each class $k \\in \\{1, 2, ..., K\\}$, producing final trees $f_{kM}(x)$. The notation $r_{mi}$ follows from the definition of *generalized*, or *pseudo residuals*, since the gradient from squared-error loss, $(y_i - f(x_i))^2$, is equal to the current residual $y_i - f(x_i)$, where $f(x) = f_{m-1}(x)$.\n","\n","Since gradient boosted trees are the most popular implementation of boosting models, it is important to highlight the most important hyper-parameters to tune when modeling with GBM from scikit-learn, LightGBM, XGBoost or any other library. First, the overall depth of trees in the ensemble ($J_m = J$ $\\forall m$) regulates how weak are the elementary learners. Second, we can introduce a learning rate $v$ that weights each component of the ensemble $f_m(x)$. Here, the number of trees $M$ can lead to overfitting, so it is more relevant than for random forests to tune $M$. Finally, we can implement stochastic gradient boosting, where only a fraction $\\eta \\leq 1$ of training data is used for fitting each component in the ensemble.\n","\n","Another approach to the boosting principle considers its models as implementing a **forward stagewise additive modeling**:\n","1. Define $f_0(x) = 0$.\n","2. For each step $m \\in \\{1, 2, ..., M\\}$:\n","  1. Compute:\n","  \\begin{equation}\n","      \\displaystyle (\\beta_m, \\gamma_m) = \\underset{\\beta, \\gamma}{\\mathrm{argmin}} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i; \\gamma))\n","  \\end{equation}\n","\n","  2. Update $f_m(x) = f_{m-1}(x) + \\beta_m b(x; \\gamma)$.\n","\n","It is specially clear how gradient boosted trees relate with forward stagewise additive modeling. Additionally, AdaBoost converges to the preocedure above when its loss function is given by the exponential loss, instead of the misclassification error.\n","\n","The [article](https://towardsdatascience.com/statistical-machine-learning-gradient-boosting-adaboost-from-scratch-8c4b5a9db9ed) mentioned above brings a different, but very consistent and well-constructed, theoretical foundation for both AdaBoost and gradient boosted trees. Even so, it presents solutions that resemble the algorithms found in [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). So, for instance, its solution to AdaBoost optimization problem indicates that a new component of the ensemble should minimize the weights defined to the misclassified observations. When it comes to the gradient boosting model for a regression task, a new component of the ensemble should minimize the sum of squared differences between the actual value of the outcome variable and the pseudo-residual previously defined.\n","\n","The **gradient boosting implementation from scratch** that can be found below first initializes parameters (learning rate - *alpha* - and loss function) and then iterates the following until a termination criterium is met: a decision tree is fitted and predicted values are calculated; then, the alpha value for a given estimator is optimized (here, a value is found so the loss function can be reduced) and the outcome variable is finally updated by calculating pseudo-residuals.\n","\n","The **AdaBoost implementation from scratch** reproduced here initializes weights (and the ensemble prediction), and then iterates over the following: a weak decision tree is fitted and predictions are made; the learning rate (*alpha* parameter) is defined through its optimal definition, so the ensemble predictions and weights can be updated. The iteration is ended when the termination criterium is met."],"metadata":{"id":"XQxSOtpjpW6o"}},{"cell_type":"markdown","metadata":{"id":"tWi-Yyretplg"},"source":["**References**\n","<br>\n","[Statistical Machine Learning: Gradient Boosting & AdaBoost from Scratch](https://towardsdatascience.com/statistical-machine-learning-gradient-boosting-adaboost-from-scratch-8c4b5a9db9ed).\n","<br>\n","[Gradient Boosted Decision Trees Explained with a Real-Life Example and Some Python Code](https://towardsdatascience.com/gradient-boosted-decision-trees-explained-with-a-real-life-example-and-some-python-code-77cee4ccf5e?gi=25ec6e2c8c4a)\n","<br>\n","[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)."]},{"cell_type":"markdown","metadata":{"id":"YjtnwOcFtdtn"},"source":["----------------"]},{"cell_type":"markdown","metadata":{"id":"-xp0Wq1nAb73"},"source":["This notebook first imports all relevant libraries, and then presents an implementation and its demonstration."]},{"cell_type":"markdown","metadata":{"id":"1jKqXHmYthkK"},"source":["**Summary:**\n","1. [Libraries](#libraries)<a href='#libraries'></a>.\n","2. [First implementation](#first_implementation)<a href='#first_implementation'></a>."]},{"cell_type":"markdown","metadata":{"id":"NPwnjKEUtk3U"},"source":["<a id='libraries'></a>"]},{"cell_type":"markdown","metadata":{"id":"Kd3Ii0l1tmT1"},"source":["## Libraries"]},{"cell_type":"code","metadata":{"id":"-tNt5lArtoMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639247169665,"user_tz":180,"elapsed":23606,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"2f4a7b1a-0016-40ef-f6ce-b19f85980539"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bx0RX4AsUtKe","executionInfo":{"status":"ok","timestamp":1639247175246,"user_tz":180,"elapsed":659,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"fb909714-f17e-4ba9-855e-a144efdc502c"},"source":["cd \"/content/gdrive/MyDrive/Studies/tree_based/Codes\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Studies/tree_based/Codes\n"]}]},{"cell_type":"code","metadata":{"id":"xOnfCCeLtr2A","executionInfo":{"status":"ok","timestamp":1639247179189,"user_tz":180,"elapsed":1566,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}}},"source":["import pandas as pd\n","import numpy as np\n","np.random.seed(123456789)\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojy3GLAotqii"},"source":["<a id='first_implementation'></a>"]},{"cell_type":"markdown","metadata":{"id":"bg_iUVd-twyu"},"source":["## First implementation"]},{"cell_type":"markdown","metadata":{"id":"aq_xm3Dy9_91"},"source":["This first implementation follows from this [article](https://towardsdatascience.com/statistical-machine-learning-gradient-boosting-adaboost-from-scratch-8c4b5a9db9ed) ([Github](https://github.com/atrothman/Gradient-Boosting-AdaBoost-Simulation) page of reference)."]},{"cell_type":"markdown","metadata":{"id":"mCg8OqJT7j35"},"source":["<a id='dataset'></a>"]},{"cell_type":"markdown","metadata":{"id":"1p0-BGhc7qD1"},"source":["### Simulated dataset"]},{"cell_type":"code","source":["def simulate_df(n=100, seed=123456, binary_flag=False):\n","    np.random.seed(seed)\n","    \n","    ## specify dataframe\n","    df = pd.DataFrame()\n","\n","    ## specify variables L1 through L6\n","    L1_split = 0.52\n","    L2_split = 0.23\n","    L3_split = 0.38\n","    df['L1'] = np.random.choice([0, 1], size=n, replace=True, p=[L1_split, (1-L1_split)])\n","    df['L2'] = np.random.choice([0, 1], size=n, replace=True, p=[L2_split, (1-L2_split)])\n","    df['L3'] = np.random.choice([0, 1], size=n, replace=True, p=[L3_split, (1-L3_split)])\n","    df['L4'] = np.random.normal(0, 1, df.shape[0])\n","    df['L5'] = np.random.normal(0, 0.75, df.shape[0])\n","    df['L6'] = np.random.normal(0, 2, df.shape[0])\n","    \n","    theta_0 = 5.5\n","    theta_1 = 1.28\n","    theta_2 = 0.42\n","    theta_3 = 2.32\n","    theta_4 = -3.15\n","    theta_5 = 3.12\n","    theta_6 = -4.29\n","    theta_7 = -1.23\n","    theta_8 = -10.18\n","    theta_9 = 2.21\n","    theta_10 = 10.3\n","    \n","    if(binary_flag):\n","        Z = theta_0 + (theta_1*df['L1']) + (theta_2*df['L2']) + (theta_3*df['L3']) + (theta_4*df['L4']) + (theta_5*df['L5']) + (theta_6*df['L6']) + (theta_7*df['L2']*df['L4']) + (theta_8*df['L3']*df['L6']) + (theta_9*df['L5']*df['L5']) + (theta_10*np.sin(df['L5']))\n","        p = 1 / (1 + np.exp(-Z))\n","        df['Y'] = np.random.binomial(1, p)\n","        df.loc[df['Y']==0, 'Y'] = -1\n","    else:\n","        df['Y'] = theta_0 + (theta_1*df['L1']) + (theta_2*df['L2']) + (theta_3*df['L3']) + (theta_4*df['L4']) + (theta_5*df['L5']) + (theta_6*df['L6']) + (theta_7*df['L2']*df['L4']) + (theta_8*df['L3']*df['L6']) + (theta_9*df['L5']*df['L5']) + (theta_10*np.sin(df['L5'])) + np.random.normal(0, 0.1, df.shape[0])\n","\n","    return(df)"],"metadata":{"id":"jp9ZlkZej-mL","executionInfo":{"status":"ok","timestamp":1639247198978,"user_tz":180,"elapsed":330,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OovjVbH8fXd4"},"source":["<a id='gradient_boosting'></a>"]},{"cell_type":"markdown","metadata":{"id":"9GOPvSJxfck_"},"source":["### Gradient boosting"]},{"cell_type":"code","source":["# Creating the simulated dataset for a regression task:\n","df = simulate_df(n=1000, seed=123456)\n","df['Y_original'] = df['Y']\n","\n","# Initializing parameters:\n","alpha = 100000\n","current_loss = sum((df['Y'])**2)\n","\n","# Fitting a gradient boosting model:\n","while(current_loss > 1):\n","    # Creating and fitting a weak learner:\n","    model = DecisionTreeRegressor(random_state=0, max_depth=3)\n","    model.fit(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']], df['Y'])\n","\n","    # Predicted outcomes:\n","    df['Y_hat'] = model.predict(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']])\n","    df['Y_hat_squared'] = df['Y_hat']**2\n","    df['Y_hat_scaled'] = np.sqrt(df['Y_hat_squared'] / df['Y_hat_squared'].sum()) * np.sign(df['Y_hat'])\n","\n","    # Optimizing the learning rate:\n","    loss_not_lowered_flag = True\n","    while(loss_not_lowered_flag):\n","        # Calculating the loss function (sum of squared residuals):\n","        new_loss = sum((df['Y'] - (alpha*df['Y_hat_scaled']))**2)\n","\n","        # Checking whether the current learning rate reduces the loss function:\n","        if(new_loss < current_loss):\n","            loss_not_lowered_flag = False\n","            current_loss = new_loss\n","            print('Current Loss: ' + str(current_loss))\n","\n","            # Updating the outcome variable for training models (pseudo-residuals):\n","            df['Y'] = df['Y'] - (alpha*df['Y_hat_scaled'])\n","        else:\n","            # Redefining the learning rate:\n","            alpha = 0.99*alpha\n","\n","    del model\n","print('model converged')"],"metadata":{"id":"XaAgHHdTkIGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639247316681,"user_tz":180,"elapsed":11709,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"d5f9982b-28bc-44d5-ab3a-064a1e71a8e9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Loss: 710652.9617096438\n","Current Loss: 704697.9593626335\n","Current Loss: 687852.7483979535\n","Current Loss: 681537.2980095611\n","Current Loss: 665478.7445945848\n","Current Loss: 659586.5364438506\n","Current Loss: 640014.5760520245\n","Current Loss: 638497.2551319164\n","Current Loss: 618245.6441627133\n","Current Loss: 618203.3564164563\n","Current Loss: 616505.1593726957\n","Current Loss: 598494.8103541062\n","Current Loss: 587837.59235525\n","Current Loss: 579409.3972249583\n","Current Loss: 574747.8100380449\n","Current Loss: 561192.4950622892\n","Current Loss: 543560.0029280061\n","Current Loss: 543207.4725473331\n","Current Loss: 536458.6003377557\n","Current Loss: 526373.448554166\n","Current Loss: 506577.88892480097\n","Current Loss: 493605.09762763977\n","Current Loss: 479834.3592400863\n","Current Loss: 478361.1843650924\n","Current Loss: 464437.13313002617\n","Current Loss: 463945.9471018524\n","Current Loss: 453269.89872302604\n","Current Loss: 449970.7331680531\n","Current Loss: 446826.93110305385\n","Current Loss: 436211.3148086319\n","Current Loss: 436021.46326745563\n","Current Loss: 423511.6916293486\n","Current Loss: 411058.6856678279\n","Current Loss: 410399.246293951\n","Current Loss: 409161.57963915204\n","Current Loss: 398231.4612537325\n","Current Loss: 394070.8494466457\n","Current Loss: 386273.19315253076\n","Current Loss: 382645.03853645327\n","Current Loss: 373851.2621279121\n","Current Loss: 370918.25261364016\n","Current Loss: 365538.64709001663\n","Current Loss: 355426.5495258091\n","Current Loss: 353083.7109897996\n","Current Loss: 352337.38687507226\n","Current Loss: 342215.1607720542\n","Current Loss: 340771.3481804764\n","Current Loss: 331529.5292991758\n","Current Loss: 327673.2149673333\n","Current Loss: 320907.0485950164\n","Current Loss: 314874.1510807905\n","Current Loss: 313616.1198052119\n","Current Loss: 313531.88689777825\n","Current Loss: 311651.2943574396\n","Current Loss: 310433.5649730562\n","Current Loss: 310222.6002992376\n","Current Loss: 306928.7658299219\n","Current Loss: 301386.7144059076\n","Current Loss: 298308.7521602127\n","Current Loss: 293059.20494377427\n","Current Loss: 292466.57448309846\n","Current Loss: 284608.24038666283\n","Current Loss: 277469.15530773083\n","Current Loss: 276542.9267533318\n","Current Loss: 269425.6800820969\n","Current Loss: 268612.0820581564\n","Current Loss: 261958.9235820618\n","Current Loss: 260480.89538931882\n","Current Loss: 256816.60963684108\n","Current Loss: 252983.379244311\n","Current Loss: 247979.0688441867\n","Current Loss: 245338.6618156127\n","Current Loss: 240078.2853218784\n","Current Loss: 237414.25394981078\n","Current Loss: 232659.59117179541\n","Current Loss: 230423.48660529967\n","Current Loss: 228302.78215420092\n","Current Loss: 224274.45731226788\n","Current Loss: 222165.40571310822\n","Current Loss: 218033.3619121518\n","Current Loss: 215639.4941242133\n","Current Loss: 211328.58788784247\n","Current Loss: 209125.54177934193\n","Current Loss: 205657.31868663972\n","Current Loss: 203698.7795988305\n","Current Loss: 200848.22148443855\n","Current Loss: 197525.11754255492\n","Current Loss: 195061.11691341383\n","Current Loss: 192058.1237438817\n","Current Loss: 189864.9018662016\n","Current Loss: 187078.0263543998\n","Current Loss: 184777.6081868406\n","Current Loss: 181773.6190082045\n","Current Loss: 179336.9559763192\n","Current Loss: 177216.29475042337\n","Current Loss: 175092.73263268924\n","Current Loss: 174797.27449985975\n","Current Loss: 174679.19557337643\n","Current Loss: 171480.42329184673\n","Current Loss: 168657.69374264928\n","Current Loss: 167254.63146783784\n","Current Loss: 164680.3277783426\n","Current Loss: 163216.79891745394\n","Current Loss: 162748.92105322258\n","Current Loss: 162377.0971561211\n","Current Loss: 158955.1413815093\n","Current Loss: 158319.5633607206\n","Current Loss: 155422.9868248941\n","Current Loss: 154105.15968249168\n","Current Loss: 151749.40903463683\n","Current Loss: 150827.50374134033\n","Current Loss: 148514.55484743096\n","Current Loss: 147642.92925730042\n","Current Loss: 145407.3063518313\n","Current Loss: 144583.7015801173\n","Current Loss: 142422.6325978037\n","Current Loss: 141644.88429433928\n","Current Loss: 138850.9681363416\n","Current Loss: 138557.22249771375\n","Current Loss: 134319.70139431147\n","Current Loss: 133310.9262369934\n","Current Loss: 131012.70292141875\n","Current Loss: 129557.36628831645\n","Current Loss: 127168.47135486187\n","Current Loss: 126093.07309790463\n","Current Loss: 123656.42984533425\n","Current Loss: 122577.07357609911\n","Current Loss: 120371.78847452076\n","Current Loss: 115098.35531781898\n","Current Loss: 114381.9439760198\n","Current Loss: 113337.22575410115\n","Current Loss: 111842.33098848611\n","Current Loss: 110014.24215881863\n","Current Loss: 107898.36054707257\n","Current Loss: 107101.28315505605\n","Current Loss: 105463.20985496567\n","Current Loss: 104210.09772877254\n","Current Loss: 102461.8261153691\n","Current Loss: 101428.060740672\n","Current Loss: 100438.94616654342\n","Current Loss: 99343.58130976884\n","Current Loss: 99306.81865181339\n","Current Loss: 99044.1213664782\n","Current Loss: 98963.69346985153\n","Current Loss: 98519.28898989895\n","Current Loss: 96934.27648627422\n","Current Loss: 96502.64209414912\n","Current Loss: 94792.03530764049\n","Current Loss: 94297.38982154184\n","Current Loss: 94282.10461872483\n","Current Loss: 94208.11940557158\n","Current Loss: 92576.9511035443\n","Current Loss: 92205.50603737844\n","Current Loss: 92172.29090058814\n","Current Loss: 91900.48532338192\n","Current Loss: 91605.25643975144\n","Current Loss: 91504.6635472081\n","Current Loss: 91409.70467952843\n","Current Loss: 91357.71622669595\n","Current Loss: 91318.25092279271\n","Current Loss: 91265.76367777244\n","Current Loss: 90942.42559062885\n","Current Loss: 89345.90332492666\n","Current Loss: 88885.3183092127\n","Current Loss: 87261.30019163978\n","Current Loss: 87020.92575410465\n","Current Loss: 85633.939997039\n","Current Loss: 85351.6205022619\n","Current Loss: 84142.86736871346\n","Current Loss: 83866.67652595219\n","Current Loss: 82692.01493686049\n","Current Loss: 82179.63830494838\n","Current Loss: 81150.47153545014\n","Current Loss: 80550.11252766357\n","Current Loss: 79700.81038180886\n","Current Loss: 79267.13333679215\n","Current Loss: 78071.3410548957\n","Current Loss: 77480.91764109387\n","Current Loss: 76543.97316741977\n","Current Loss: 75646.6914020297\n","Current Loss: 72646.08112075843\n","Current Loss: 71887.84071169709\n","Current Loss: 71520.4156783724\n","Current Loss: 70521.13365457585\n","Current Loss: 69673.3820155201\n","Current Loss: 69660.1090704374\n","Current Loss: 69605.06590518181\n","Current Loss: 68641.80660152879\n","Current Loss: 68578.17142153686\n","Current Loss: 68372.26760187683\n","Current Loss: 68189.01619172283\n","Current Loss: 67722.89148872878\n","Current Loss: 67568.48340932815\n","Current Loss: 67252.45350136008\n","Current Loss: 67162.78141627627\n","Current Loss: 64312.691048224464\n","Current Loss: 63651.70921901017\n","Current Loss: 63129.86460157964\n","Current Loss: 62518.13810643302\n","Current Loss: 62029.7239773365\n","Current Loss: 61414.683424763316\n","Current Loss: 61029.435162366855\n","Current Loss: 59294.44157865298\n","Current Loss: 59222.35597679243\n","Current Loss: 58940.346553974145\n","Current Loss: 58892.22778892308\n","Current Loss: 56773.55514965254\n","Current Loss: 55996.128607917984\n","Current Loss: 55378.40838071989\n","Current Loss: 54765.945374877774\n","Current Loss: 54133.95350013983\n","Current Loss: 53667.602294433134\n","Current Loss: 53295.07772343505\n","Current Loss: 52581.37048561288\n","Current Loss: 52179.52853536977\n","Current Loss: 51658.84748925105\n","Current Loss: 51401.67201693482\n","Current Loss: 51148.17202376718\n","Current Loss: 51125.21043390114\n","Current Loss: 50497.57997691235\n","Current Loss: 50377.53129592718\n","Current Loss: 50177.04179117521\n","Current Loss: 49568.090029949046\n","Current Loss: 49248.530396047296\n","Current Loss: 49001.08315060103\n","Current Loss: 48877.02226544973\n","Current Loss: 48517.30801265154\n","Current Loss: 48282.32415267394\n","Current Loss: 47828.836663326365\n","Current Loss: 47712.86745662133\n","Current Loss: 47202.33775925966\n","Current Loss: 46882.10873253601\n","Current Loss: 46669.59131369778\n","Current Loss: 46260.125662473736\n","Current Loss: 45776.5849219904\n","Current Loss: 45341.277105797744\n","Current Loss: 44964.07946382367\n","Current Loss: 44631.51287676686\n","Current Loss: 44488.33723415961\n","Current Loss: 44311.42904541334\n","Current Loss: 44110.261346376334\n","Current Loss: 43657.074442376725\n","Current Loss: 43240.37528840839\n","Current Loss: 42915.047609885725\n","Current Loss: 42764.34679305463\n","Current Loss: 42463.398036570725\n","Current Loss: 42289.95225066034\n","Current Loss: 42159.26429845126\n","Current Loss: 41463.603286011334\n","Current Loss: 41081.97071568808\n","Current Loss: 40699.611802192\n","Current Loss: 40304.072605985835\n","Current Loss: 40109.93616185001\n","Current Loss: 40052.72514000545\n","Current Loss: 39261.60159030109\n","Current Loss: 38877.91187323202\n","Current Loss: 38608.85624434977\n","Current Loss: 38488.151437929846\n","Current Loss: 37986.520951524704\n","Current Loss: 37746.46388044371\n","Current Loss: 37499.45542711279\n","Current Loss: 37392.0077794173\n","Current Loss: 37065.386881171304\n","Current Loss: 36973.68174730712\n","Current Loss: 36885.797641090234\n","Current Loss: 36613.47205657728\n","Current Loss: 36273.78061841705\n","Current Loss: 35592.689029032044\n","Current Loss: 35434.21428590552\n","Current Loss: 35028.82514617004\n","Current Loss: 34883.621118734496\n","Current Loss: 34636.77709174019\n","Current Loss: 34424.97350993458\n","Current Loss: 33582.415921678235\n","Current Loss: 33483.847406923436\n","Current Loss: 33474.05371073091\n","Current Loss: 32444.304678982007\n","Current Loss: 32211.069971585137\n","Current Loss: 32188.349520988108\n","Current Loss: 31874.021888305375\n","Current Loss: 31536.53654646376\n","Current Loss: 31282.083505735663\n","Current Loss: 31104.52653276269\n","Current Loss: 30841.352309742673\n","Current Loss: 30648.83442128708\n","Current Loss: 30577.390507255357\n","Current Loss: 30276.08752336584\n","Current Loss: 30187.735729412863\n","Current Loss: 29972.717394429634\n","Current Loss: 29793.340421400197\n","Current Loss: 29638.538757562543\n","Current Loss: 29510.269755945043\n","Current Loss: 29379.54765695085\n","Current Loss: 29226.997996940438\n","Current Loss: 29207.37915679438\n","Current Loss: 28989.57215881042\n","Current Loss: 28978.05429115853\n","Current Loss: 28766.830405711356\n","Current Loss: 28751.721974928707\n","Current Loss: 28552.8251222933\n","Current Loss: 28539.8607166283\n","Current Loss: 28347.26798784958\n","Current Loss: 28330.87833483665\n","Current Loss: 28149.771263709386\n","Current Loss: 28135.465654131905\n","Current Loss: 27960.07108076931\n","Current Loss: 27840.093386617744\n","Current Loss: 27689.936156891083\n","Current Loss: 27597.109406686744\n","Current Loss: 27359.00738808411\n","Current Loss: 27353.17355162869\n","Current Loss: 26953.94943511547\n","Current Loss: 26934.816764300813\n","Current Loss: 26922.27908939022\n","Current Loss: 26763.574792923635\n","Current Loss: 26746.083586104935\n","Current Loss: 26611.107038911483\n","Current Loss: 26457.1060308197\n","Current Loss: 26388.592607739545\n","Current Loss: 26250.99463574648\n","Current Loss: 26227.162257327374\n","Current Loss: 26090.56535349523\n","Current Loss: 26045.7916457055\n","Current Loss: 25952.412880100244\n","Current Loss: 25850.985172171928\n","Current Loss: 25771.01907593502\n","Current Loss: 25662.831863547908\n","Current Loss: 25436.92465046538\n","Current Loss: 25434.514729304155\n","Current Loss: 25277.194178270533\n","Current Loss: 25210.110174393812\n","Current Loss: 24989.34913548675\n","Current Loss: 24800.9081688594\n","Current Loss: 24762.81269826664\n","Current Loss: 24715.237364343968\n","Current Loss: 24680.42708539343\n","Current Loss: 24656.24267736519\n","Current Loss: 24646.236309319018\n","Current Loss: 24643.11151925268\n","Current Loss: 24508.640062731538\n","Current Loss: 24494.820225277654\n","Current Loss: 24463.63894749109\n","Current Loss: 24354.873268544485\n","Current Loss: 24331.429723456524\n","Current Loss: 24218.240756731073\n","Current Loss: 24115.120912447197\n","Current Loss: 24070.758116616187\n","Current Loss: 24054.51607348078\n","Current Loss: 24040.845693547803\n","Current Loss: 23955.58236301177\n","Current Loss: 23928.75616419118\n","Current Loss: 23805.548462325954\n","Current Loss: 23747.317672037836\n","Current Loss: 23665.743271413285\n","Current Loss: 22352.92722625014\n","Current Loss: 22348.05667948252\n","Current Loss: 20695.50663199406\n","Current Loss: 20659.067876716384\n","Current Loss: 20481.806664130858\n","Current Loss: 20467.064378678482\n","Current Loss: 20284.621462521034\n","Current Loss: 19624.747254300666\n","Current Loss: 19572.538723135072\n","Current Loss: 19141.506666481615\n","Current Loss: 18951.646972067087\n","Current Loss: 18424.618030416936\n","Current Loss: 18366.473520496384\n","Current Loss: 17736.35687689949\n","Current Loss: 17706.174817571675\n","Current Loss: 17227.188897283904\n","Current Loss: 16865.422704167486\n","Current Loss: 16810.439841116404\n","Current Loss: 16506.59628888761\n","Current Loss: 16356.263782615226\n","Current Loss: 16212.426885328772\n","Current Loss: 16197.412861415229\n","Current Loss: 15727.971459044324\n","Current Loss: 14787.315792399677\n","Current Loss: 14759.192694834383\n","Current Loss: 14179.167662125172\n","Current Loss: 14147.872470447675\n","Current Loss: 13400.540488267334\n","Current Loss: 13023.605467058947\n","Current Loss: 12870.465654882053\n","Current Loss: 12854.34418776058\n","Current Loss: 12424.12782618417\n","Current Loss: 12408.28892964267\n","Current Loss: 11942.034666442127\n","Current Loss: 11775.80673368244\n","Current Loss: 11765.392224843697\n","Current Loss: 11468.375260653227\n","Current Loss: 11366.113542270145\n","Current Loss: 11314.818525395378\n","Current Loss: 11050.520586118779\n","Current Loss: 11045.891915136272\n","Current Loss: 10781.551651313246\n","Current Loss: 10512.095765238555\n","Current Loss: 10490.220887459747\n","Current Loss: 10283.342588807049\n","Current Loss: 10194.11381289736\n","Current Loss: 9884.246356028636\n","Current Loss: 9816.7363491745\n","Current Loss: 9665.590324569033\n","Current Loss: 9531.28003788034\n","Current Loss: 9325.581542054751\n","Current Loss: 9316.581672286695\n","Current Loss: 9105.728428706761\n","Current Loss: 8982.780644960025\n","Current Loss: 8978.794020576534\n","Current Loss: 8852.417861059419\n","Current Loss: 8676.07532326812\n","Current Loss: 8477.910523606179\n","Current Loss: 8357.290210728801\n","Current Loss: 8051.078781420778\n","Current Loss: 8041.100419235019\n","Current Loss: 8037.488754879519\n","Current Loss: 7918.075818838189\n","Current Loss: 7808.243424021927\n","Current Loss: 7598.118703790367\n","Current Loss: 7587.721566887873\n","Current Loss: 7447.029028803662\n","Current Loss: 7420.603579306644\n","Current Loss: 7418.476039227734\n","Current Loss: 7245.012278047563\n","Current Loss: 7222.890887448771\n","Current Loss: 6992.394541022848\n","Current Loss: 6924.093974143412\n","Current Loss: 6742.994456886047\n","Current Loss: 6707.489335856501\n","Current Loss: 6522.931348433135\n","Current Loss: 6398.929771941126\n","Current Loss: 6392.384519079965\n","Current Loss: 6280.182250622128\n","Current Loss: 6272.733119545911\n","Current Loss: 6119.164765618925\n","Current Loss: 6087.946225950682\n","Current Loss: 6013.309701305039\n","Current Loss: 5897.6819751686335\n","Current Loss: 5879.525747866762\n","Current Loss: 5807.609080444945\n","Current Loss: 5723.1499825958645\n","Current Loss: 5675.256674153057\n","Current Loss: 5457.518275283146\n","Current Loss: 5436.213682689816\n","Current Loss: 5311.249297748571\n","Current Loss: 5240.6926752560685\n","Current Loss: 5240.594799920705\n","Current Loss: 5124.940027797096\n","Current Loss: 5114.747318529489\n","Current Loss: 5078.439626033909\n","Current Loss: 5031.220314235308\n","Current Loss: 5025.625586095462\n","Current Loss: 4959.8774007849415\n","Current Loss: 4946.936603811554\n","Current Loss: 4841.272963908474\n","Current Loss: 4717.160110698885\n","Current Loss: 4713.64372450146\n","Current Loss: 4705.630903214459\n","Current Loss: 4589.178496885959\n","Current Loss: 4529.733381578208\n","Current Loss: 4523.610513891809\n","Current Loss: 4458.007716696059\n","Current Loss: 4327.340975478822\n","Current Loss: 4306.286745502507\n","Current Loss: 4196.474261378052\n","Current Loss: 4190.808203183797\n","Current Loss: 4029.6335811835684\n","Current Loss: 3948.646088488688\n","Current Loss: 3897.9648629841104\n","Current Loss: 3847.7955959656597\n","Current Loss: 3686.241285154349\n","Current Loss: 3684.8628756019534\n","Current Loss: 3564.3549031475786\n","Current Loss: 3564.326977903518\n","Current Loss: 3531.043442455367\n","Current Loss: 3375.9059738377164\n","Current Loss: 3374.7475347612244\n","Current Loss: 3301.0551188285235\n","Current Loss: 3151.187298994411\n","Current Loss: 3071.3511761460663\n","Current Loss: 2929.425270885255\n","Current Loss: 2820.168313012372\n","Current Loss: 2768.36797755195\n","Current Loss: 2695.303259785554\n","Current Loss: 2693.388711476951\n","Current Loss: 2692.3087822360594\n","Current Loss: 2619.872340046552\n","Current Loss: 2618.0416662972243\n","Current Loss: 2615.557499589133\n","Current Loss: 2588.267348658975\n","Current Loss: 2587.55927642036\n","Current Loss: 2553.7823991148434\n","Current Loss: 2481.9929480389847\n","Current Loss: 2437.994543796499\n","Current Loss: 2289.271949706313\n","Current Loss: 2219.8105188727322\n","Current Loss: 2156.844078965813\n","Current Loss: 2079.0391469241013\n","Current Loss: 2046.9737258393034\n","Current Loss: 2046.7259230608454\n","Current Loss: 2022.673509934146\n","Current Loss: 2012.1913599885665\n","Current Loss: 1962.9911622465017\n","Current Loss: 1934.7535213410142\n","Current Loss: 1910.6401640289337\n","Current Loss: 1909.9427011747734\n","Current Loss: 1865.1286186340524\n","Current Loss: 1816.3562029726522\n","Current Loss: 1765.4506239884365\n","Current Loss: 1765.1699362900044\n","Current Loss: 1756.0994214248421\n","Current Loss: 1749.775955133134\n","Current Loss: 1735.0460095850574\n","Current Loss: 1667.381615766717\n","Current Loss: 1661.5515224601627\n","Current Loss: 1635.6762900752651\n","Current Loss: 1570.4350904415314\n","Current Loss: 1529.8691876773198\n","Current Loss: 1529.438882645848\n","Current Loss: 1505.9481106019787\n","Current Loss: 1478.12594995517\n","Current Loss: 1433.2437316046667\n","Current Loss: 1397.0363711616617\n","Current Loss: 1345.0853844159847\n","Current Loss: 1304.1998705733574\n","Current Loss: 1279.2927537362764\n","Current Loss: 1273.7979731600778\n","Current Loss: 1261.6860409375474\n","Current Loss: 1248.5340331561135\n","Current Loss: 1248.342031559255\n","Current Loss: 1221.9482706336303\n","Current Loss: 1192.7073681416211\n","Current Loss: 1165.2619848354732\n","Current Loss: 1133.0029509182395\n","Current Loss: 1109.603403164435\n","Current Loss: 1078.500883053012\n","Current Loss: 1034.2261244892356\n","Current Loss: 991.5185587272853\n","Current Loss: 971.5319987166882\n","Current Loss: 936.1937504235074\n","Current Loss: 935.8287002445311\n","Current Loss: 908.0926914241586\n","Current Loss: 890.0007223086847\n","Current Loss: 875.4080182509991\n","Current Loss: 875.1473875758774\n","Current Loss: 862.2151120386798\n","Current Loss: 861.918783441569\n","Current Loss: 846.0317224109482\n","Current Loss: 830.4360781294058\n","Current Loss: 811.1718902752671\n","Current Loss: 795.0431537753811\n","Current Loss: 780.8245888048224\n","Current Loss: 763.3236610820214\n","Current Loss: 736.5326487554405\n","Current Loss: 734.0988246381181\n","Current Loss: 702.4028892503857\n","Current Loss: 701.3515339670611\n","Current Loss: 701.1634781191033\n","Current Loss: 694.4217652014518\n","Current Loss: 691.8657951267196\n","Current Loss: 682.7614075280591\n","Current Loss: 671.5924989116041\n","Current Loss: 669.0691637400186\n","Current Loss: 648.7002254709872\n","Current Loss: 641.2230791145142\n","Current Loss: 621.1235092204942\n","Current Loss: 600.995770407721\n","Current Loss: 597.2913172845277\n","Current Loss: 593.3311729603246\n","Current Loss: 570.9518521194567\n","Current Loss: 551.7980460767653\n","Current Loss: 551.6542750008228\n","Current Loss: 541.2681006778633\n","Current Loss: 527.9515808169798\n","Current Loss: 506.434883292059\n","Current Loss: 502.91197332522955\n","Current Loss: 502.85201804625785\n","Current Loss: 498.3249804813236\n","Current Loss: 498.2849584020974\n","Current Loss: 495.1646727654088\n","Current Loss: 488.1010464194109\n","Current Loss: 474.9993944334738\n","Current Loss: 464.93670839866974\n","Current Loss: 462.2177274799404\n","Current Loss: 448.0130698431935\n","Current Loss: 440.4867767161721\n","Current Loss: 427.8509275132591\n","Current Loss: 422.0129401903701\n","Current Loss: 411.12468710970796\n","Current Loss: 394.5155197560521\n","Current Loss: 390.0730584192416\n","Current Loss: 382.6958944383419\n","Current Loss: 378.5485528412515\n","Current Loss: 378.530268926802\n","Current Loss: 377.88735300846236\n","Current Loss: 377.249650714201\n","Current Loss: 367.52962036190183\n","Current Loss: 361.5537663668381\n","Current Loss: 355.6948698978757\n","Current Loss: 352.5831877659762\n","Current Loss: 348.1191035057779\n","Current Loss: 348.0972777505766\n","Current Loss: 341.9345919522252\n","Current Loss: 336.1549723308474\n","Current Loss: 330.89931225050196\n","Current Loss: 327.9165318008729\n","Current Loss: 325.4961282354473\n","Current Loss: 320.38739635670566\n","Current Loss: 314.6081979067304\n","Current Loss: 306.21844815737535\n","Current Loss: 296.5443044574772\n","Current Loss: 286.35856855770203\n","Current Loss: 282.07900998069636\n","Current Loss: 273.0140929859045\n","Current Loss: 268.11596911625514\n","Current Loss: 263.09512440188627\n","Current Loss: 254.918311090377\n","Current Loss: 249.86885180463653\n","Current Loss: 245.2197279457582\n","Current Loss: 244.05713598522226\n","Current Loss: 237.85596717396072\n","Current Loss: 232.1929372871823\n","Current Loss: 228.77960471137226\n","Current Loss: 219.60690247080421\n","Current Loss: 215.06267246703408\n","Current Loss: 209.8838783517349\n","Current Loss: 207.43060576761405\n","Current Loss: 205.90271831219152\n","Current Loss: 199.86443030002545\n","Current Loss: 197.43353150588572\n","Current Loss: 195.04884349968088\n","Current Loss: 189.879583033737\n","Current Loss: 183.98511991857544\n","Current Loss: 178.1429086624801\n","Current Loss: 173.0725898690052\n","Current Loss: 168.54825997385512\n","Current Loss: 167.1689757163179\n","Current Loss: 163.10587384783602\n","Current Loss: 160.77830589959086\n","Current Loss: 158.00542507277348\n","Current Loss: 153.93097639871934\n","Current Loss: 150.45338093687013\n","Current Loss: 146.1535836271802\n","Current Loss: 142.9433202669166\n","Current Loss: 140.61049189726654\n","Current Loss: 135.59485230210402\n","Current Loss: 133.61095719425245\n","Current Loss: 132.02506961049374\n","Current Loss: 130.15782513668807\n","Current Loss: 120.66625270067671\n","Current Loss: 114.84356531676626\n","Current Loss: 112.766290424286\n","Current Loss: 109.98422007143446\n","Current Loss: 107.40871108901118\n","Current Loss: 103.50286571442193\n","Current Loss: 102.45687444828813\n","Current Loss: 98.47052388413505\n","Current Loss: 96.55178613669737\n","Current Loss: 95.66630107422733\n","Current Loss: 93.73802075274934\n","Current Loss: 91.30805760514373\n","Current Loss: 91.1782461498765\n","Current Loss: 89.05023244227311\n","Current Loss: 88.55345389141789\n","Current Loss: 87.34480517177866\n","Current Loss: 85.44187326490834\n","Current Loss: 82.02086597900603\n","Current Loss: 80.84469631039853\n","Current Loss: 78.77937964201561\n","Current Loss: 77.97837119623226\n","Current Loss: 74.57836276036228\n","Current Loss: 74.17200274758865\n","Current Loss: 73.7877428497724\n","Current Loss: 72.52539854153058\n","Current Loss: 71.19707622168369\n","Current Loss: 71.16862622513439\n","Current Loss: 70.76022189227967\n","Current Loss: 70.73939425281552\n","Current Loss: 69.5334190379801\n","Current Loss: 69.51251223864479\n","Current Loss: 67.75740040020122\n","Current Loss: 66.36238191859412\n","Current Loss: 65.92065248561822\n","Current Loss: 62.2427704334768\n","Current Loss: 61.701296881900284\n","Current Loss: 61.008657410389176\n","Current Loss: 60.96068760979576\n","Current Loss: 60.1681301533212\n","Current Loss: 58.91722740034993\n","Current Loss: 58.90937335590432\n","Current Loss: 56.471792072883794\n","Current Loss: 54.64061807673324\n","Current Loss: 53.423975954875715\n","Current Loss: 52.53737021773846\n","Current Loss: 51.6168070416724\n","Current Loss: 50.47907446454869\n","Current Loss: 49.47603439760068\n","Current Loss: 49.47541380680256\n","Current Loss: 49.44198741867151\n","Current Loss: 49.44084586343901\n","Current Loss: 47.969016098024184\n","Current Loss: 46.889056219915034\n","Current Loss: 45.84865453392591\n","Current Loss: 45.2312741571196\n","Current Loss: 44.39847466090476\n","Current Loss: 42.89403281314086\n","Current Loss: 42.72713493034938\n","Current Loss: 41.85695106648214\n","Current Loss: 40.229332440821466\n","Current Loss: 39.602280201478756\n","Current Loss: 38.92518850966124\n","Current Loss: 38.19985632514089\n","Current Loss: 37.966407044916004\n","Current Loss: 37.38727887474763\n","Current Loss: 36.55320340177225\n","Current Loss: 36.28951432267778\n","Current Loss: 35.3899153303207\n","Current Loss: 34.4495403792929\n","Current Loss: 33.74955499579802\n","Current Loss: 33.24827911592818\n","Current Loss: 32.897267269672064\n","Current Loss: 32.20535064418893\n","Current Loss: 31.55178210518906\n","Current Loss: 30.844049463431748\n","Current Loss: 30.188892654937618\n","Current Loss: 29.344888351945446\n","Current Loss: 29.341333577277375\n","Current Loss: 29.103530340504165\n","Current Loss: 29.01706819451759\n","Current Loss: 28.938828922674105\n","Current Loss: 27.532012406530058\n","Current Loss: 26.404503624496115\n","Current Loss: 26.174980949760716\n","Current Loss: 25.41581578584453\n","Current Loss: 24.66695046932595\n","Current Loss: 24.36737774403541\n","Current Loss: 23.679600966987607\n","Current Loss: 23.576835014975096\n","Current Loss: 22.806132169364613\n","Current Loss: 22.258395124301135\n","Current Loss: 21.902910311232482\n","Current Loss: 21.35594330439167\n","Current Loss: 20.555716145393145\n","Current Loss: 20.407805647037016\n","Current Loss: 20.211788819077977\n","Current Loss: 20.12033871042214\n","Current Loss: 20.002511554878637\n","Current Loss: 19.999835759101963\n","Current Loss: 19.231809240545395\n","Current Loss: 18.993625934349783\n","Current Loss: 18.32881490797595\n","Current Loss: 17.800445474950507\n","Current Loss: 17.174908653586485\n","Current Loss: 16.642888252791757\n","Current Loss: 16.347086216031094\n","Current Loss: 16.174616123282053\n","Current Loss: 15.788180590780621\n","Current Loss: 15.271712812339755\n","Current Loss: 14.974877793914276\n","Current Loss: 14.93202412040131\n","Current Loss: 14.396245020509081\n","Current Loss: 13.83847061859633\n","Current Loss: 13.571054673620376\n","Current Loss: 13.56760222389885\n","Current Loss: 13.540905091408742\n","Current Loss: 13.539817015427582\n","Current Loss: 13.379541110678714\n","Current Loss: 13.333228387740471\n","Current Loss: 13.110542161660552\n","Current Loss: 12.854377543290218\n","Current Loss: 12.446127087332602\n","Current Loss: 12.151429644149454\n","Current Loss: 11.71946388213816\n","Current Loss: 11.518914705633978\n","Current Loss: 11.332174928909982\n","Current Loss: 10.874827050183784\n","Current Loss: 10.62022300287987\n","Current Loss: 10.434660017640141\n","Current Loss: 10.15038469787735\n","Current Loss: 9.95223468589514\n","Current Loss: 9.639654725335644\n","Current Loss: 9.544505853687914\n","Current Loss: 9.242595847850732\n","Current Loss: 8.739404150138725\n","Current Loss: 8.65163296026137\n","Current Loss: 8.528697794937644\n","Current Loss: 8.27321867937959\n","Current Loss: 8.270853963033197\n","Current Loss: 8.267155146417542\n","Current Loss: 8.266291577838706\n","Current Loss: 8.263932111695711\n","Current Loss: 8.261118213975875\n","Current Loss: 8.260456018820435\n","Current Loss: 8.257732252408795\n","Current Loss: 8.257116690399124\n","Current Loss: 8.254479916637582\n","Current Loss: 8.253908741495414\n","Current Loss: 8.251355937185274\n","Current Loss: 8.250826999278644\n","Current Loss: 8.248355252294617\n","Current Loss: 8.247866494667894\n","Current Loss: 8.245235426163683\n","Current Loss: 8.245020078569087\n","Current Loss: 8.242381616579546\n","Current Loss: 8.242284711940941\n","Current Loss: 8.242155943198807\n","Current Loss: 8.239628350987239\n","Current Loss: 8.239444987393398\n","Current Loss: 8.237076064565436\n","Current Loss: 8.236709367119563\n","Current Loss: 8.194228834532728\n","Current Loss: 8.141858100569513\n","Current Loss: 7.945139908658069\n","Current Loss: 7.574974089392247\n","Current Loss: 7.55321943094531\n","Current Loss: 7.363170008864346\n","Current Loss: 7.146464402422062\n","Current Loss: 6.9938492571456266\n","Current Loss: 6.836158204425578\n","Current Loss: 6.703160637862152\n","Current Loss: 6.620879427302436\n","Current Loss: 6.5071873183875155\n","Current Loss: 6.425745092540228\n","Current Loss: 6.265605617110951\n","Current Loss: 6.145804547707969\n","Current Loss: 6.120287805429069\n","Current Loss: 5.9446134675836895\n","Current Loss: 5.731580061563681\n","Current Loss: 5.540359058723754\n","Current Loss: 5.37533153982331\n","Current Loss: 5.145743665384942\n","Current Loss: 5.145445351284355\n","Current Loss: 5.104961277240693\n","Current Loss: 5.029171902267271\n","Current Loss: 4.961575705660889\n","Current Loss: 4.917014943734318\n","Current Loss: 4.864067368106958\n","Current Loss: 4.75731136556143\n","Current Loss: 4.636817290074549\n","Current Loss: 4.458776748455073\n","Current Loss: 4.362662055137279\n","Current Loss: 4.226024324477171\n","Current Loss: 4.043098643761584\n","Current Loss: 3.9055917231773187\n","Current Loss: 3.843336653309015\n","Current Loss: 3.7936280830219826\n","Current Loss: 3.792586848584885\n","Current Loss: 3.731380046175968\n","Current Loss: 3.702035316209382\n","Current Loss: 3.6781734700409143\n","Current Loss: 3.631203724501153\n","Current Loss: 3.595559015142952\n","Current Loss: 3.574808745867642\n","Current Loss: 3.5393267564339945\n","Current Loss: 3.4552928292685268\n","Current Loss: 3.426584788745609\n","Current Loss: 3.36053236424863\n","Current Loss: 3.289330462263582\n","Current Loss: 3.2067079922042323\n","Current Loss: 3.1743019344038603\n","Current Loss: 2.987138332318843\n","Current Loss: 2.8307391007440277\n","Current Loss: 2.7230145389477585\n","Current Loss: 2.7224763804038887\n","Current Loss: 2.636776636108374\n","Current Loss: 2.636142837520458\n","Current Loss: 2.5129263855075106\n","Current Loss: 2.505073060054369\n","Current Loss: 2.497949757344254\n","Current Loss: 2.476748032034824\n","Current Loss: 2.4368794357297836\n","Current Loss: 2.3892529985891957\n","Current Loss: 2.3229727791795267\n","Current Loss: 2.308656001405012\n","Current Loss: 2.2665223953689524\n","Current Loss: 2.1644264048847863\n","Current Loss: 2.1121344676937635\n","Current Loss: 2.069640124637328\n","Current Loss: 2.054853282579701\n","Current Loss: 1.9925649079726633\n","Current Loss: 1.9391686143333635\n","Current Loss: 1.8937259836794653\n","Current Loss: 1.8935605254047787\n","Current Loss: 1.8496576371288362\n","Current Loss: 1.8456570777074328\n","Current Loss: 1.8216040590068836\n","Current Loss: 1.7770137673273536\n","Current Loss: 1.776889130483244\n","Current Loss: 1.7712069794810414\n","Current Loss: 1.7711717563367744\n","Current Loss: 1.7339210803262275\n","Current Loss: 1.7131746809880284\n","Current Loss: 1.683609768040766\n","Current Loss: 1.6552191989656344\n","Current Loss: 1.625778443896978\n","Current Loss: 1.586969937395541\n","Current Loss: 1.5703543115504484\n","Current Loss: 1.5438035850539455\n","Current Loss: 1.5130050581074581\n","Current Loss: 1.5094616605934346\n","Current Loss: 1.4792664380911802\n","Current Loss: 1.4654000092896993\n","Current Loss: 1.4291284308527996\n","Current Loss: 1.4239900078897583\n","Current Loss: 1.4234442319121172\n","Current Loss: 1.4160225849392236\n","Current Loss: 1.4111520576834484\n","Current Loss: 1.3939388605917693\n","Current Loss: 1.3938724940514944\n","Current Loss: 1.3521315542033092\n","Current Loss: 1.3061782531872175\n","Current Loss: 1.259129161638485\n","Current Loss: 1.2582348165581945\n","Current Loss: 1.2186396065660468\n","Current Loss: 1.184148534500063\n","Current Loss: 1.1544718392549824\n","Current Loss: 1.1456436057658277\n","Current Loss: 1.142131712113265\n","Current Loss: 1.106796251717823\n","Current Loss: 1.102266614192472\n","Current Loss: 1.0868189820463288\n","Current Loss: 1.0605169774086485\n","Current Loss: 1.0387436691152268\n","Current Loss: 1.0017150421509573\n","Current Loss: 0.9935744613774612\n","model converged\n"]}]},{"cell_type":"markdown","metadata":{"id":"t2OnBP5Q7sfh"},"source":["<a id='ada_boost'></a>"]},{"cell_type":"markdown","metadata":{"id":"SFCy5u6a7vG7"},"source":["### AdaBoost"]},{"cell_type":"code","metadata":{"id":"asJLYJG7mXRw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639249824466,"user_tz":180,"elapsed":1313,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"343d88a3-bdb6-457c-de30-1f1db39a844a"},"source":["# Creating the simulated dataset for a binary classification task:\n","df = simulate_df(n=1000, seed=123456, binary_flag=True)\n","df = df.rename(columns={'Y':'Y_original'})\n","\n","# Initializing weights (same for all training instances):\n","df['w'] = df.shape[0]*[1/df.shape[0]]\n","\n","# Initializing ensemble predictions:\n","df['Y'] = 0\n","\n","# Fitting an AdaBoost model:\n","count = True\n","while(count):\n","    # Creating and fitting a weak learner (using weights for training instances):\n","    model = DecisionTreeClassifier(random_state=0, max_depth=2)\n","    model.fit(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']], df['Y_original'], sample_weight=df['w'])\n","\n","    # Predicted outcomes (individual learner):\n","    df['Y_hat'] = model.predict(df[['L1', 'L2', 'L3','L4', 'L5', 'L6']])\n","\n","    # Setting to zero the weights of correctly predicted observations:\n","    df.loc[df['Y_hat']==df['Y_original'], 'w'] = 0\n","\n","    # Defining the alpha parameter as given by its optimal definition:\n","    epsilon = sum(df['w'])\n","    alpha = 0.5*(np.log((1-epsilon)/epsilon))\n","    \n","    # Updating ensemble predictions:\n","    df['Y'] = df['Y'] + (alpha*df['Y_hat'])\n","    \n","    # Calculating loss function:\n","    current_loss = sum(np.exp(-df['Y_original']*df['Y']))\n","    print('Current Loss: ' + str(current_loss))\n","\n","    # Updating weights:\n","    psi = np.exp(-df['Y_original']*df['Y'])\n","    df['w'] = psi / current_loss\n","    \n","    # Predicted outcomes (ensemble):\n","    df['Y_final'] = 1\n","    df.loc[df['Y']<0, 'Y_final'] = -1\n","    \n","    # Termination criterium:\n","    if(df.loc[df['Y_original']!=df['Y_final'], :].shape[0] == 0):\n","        print('results converged, all datapoints correctly classified')\n","        break"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Loss: 659.1631057636724\n","Current Loss: 487.7655215433816\n","Current Loss: 385.43015602209204\n","Current Loss: 309.6683993321115\n","Current Loss: 255.5542037237089\n","Current Loss: 218.3147055733867\n","Current Loss: 198.66608204748306\n","Current Loss: 171.9175494217112\n","Current Loss: 151.6839290289567\n","Current Loss: 138.20003547506386\n","Current Loss: 131.45019790958239\n","Current Loss: 126.25445993343435\n","Current Loss: 122.78411083637391\n","Current Loss: 116.59354372531038\n","Current Loss: 108.28050371146264\n","Current Loss: 104.39126436345835\n","Current Loss: 101.02340988337336\n","Current Loss: 97.78481006804954\n","Current Loss: 95.31392819906642\n","Current Loss: 92.54905329566284\n","Current Loss: 87.00935770165631\n","Current Loss: 85.1447829237424\n","Current Loss: 83.34377736632857\n","Current Loss: 81.5389811738938\n","Current Loss: 77.92521488496631\n","Current Loss: 74.76450253594427\n","Current Loss: 71.78607286865692\n","Current Loss: 69.08618323068403\n","Current Loss: 67.14548022606846\n","Current Loss: 66.48569498215033\n","Current Loss: 65.60058832576865\n","Current Loss: 64.95562892117327\n","Current Loss: 64.08074748427363\n","Current Loss: 62.77251964431154\n","Current Loss: 61.57671989700741\n","Current Loss: 59.12390384549784\n","Current Loss: 57.80059735171777\n","Current Loss: 57.62259230392608\n","Current Loss: 57.110309947255224\n","Current Loss: 56.15816964029094\n","Current Loss: 55.207937027300154\n","Current Loss: 54.14920809338544\n","Current Loss: 53.409217406397815\n","Current Loss: 52.98460104214518\n","Current Loss: 52.27588072610328\n","Current Loss: 49.272173927586216\n","Current Loss: 48.40688142465532\n","Current Loss: 45.93216912152813\n","Current Loss: 43.8989745564378\n","Current Loss: 43.089781029819875\n","Current Loss: 41.174522006536826\n","Current Loss: 40.28032239412736\n","Current Loss: 39.58737141336605\n","Current Loss: 38.20825630397851\n","Current Loss: 37.39401636886241\n","Current Loss: 36.27185691815008\n","Current Loss: 35.38640212337222\n","Current Loss: 34.63786318569133\n","Current Loss: 33.62679434610299\n","Current Loss: 33.32168552500472\n","Current Loss: 32.96533992374049\n","Current Loss: 32.112424640966644\n","Current Loss: 31.80141131516977\n","Current Loss: 31.52772166810667\n","Current Loss: 31.315776391002135\n","Current Loss: 30.925649752994886\n","Current Loss: 30.38602671164647\n","Current Loss: 29.799764750765032\n","Current Loss: 29.036151279152392\n","Current Loss: 28.456973701202834\n","Current Loss: 28.22475882624438\n","Current Loss: 27.537102971746172\n","Current Loss: 27.169628161485914\n","Current Loss: 26.823610423455627\n","Current Loss: 26.707277738691033\n","Current Loss: 26.512481322812363\n","Current Loss: 26.35821689911592\n","Current Loss: 26.189925185238934\n","Current Loss: 25.713075439020663\n","Current Loss: 25.134039309907855\n","Current Loss: 24.094686292869145\n","Current Loss: 23.304834376269692\n","results converged, all datapoints correctly classified\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"rZrtMsTI50O0"},"execution_count":null,"outputs":[]}]}