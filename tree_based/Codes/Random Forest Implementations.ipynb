{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Random Forest Implementations.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9wNr8rzD+xL7EECvoLmp3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7rbt6qO5tBQ2"},"source":["## Random forest implementations"]},{"cell_type":"markdown","metadata":{"id":"8wopI9_PkPuO"},"source":["**Random forest** is one of the main supervised learning algorithms that derive from decision trees. The main purpose of gathering individual trees in the way random forests do is to reduce the variance of final estimates at the same time bias is kept under control. Decision trees are highly dependent on the training data, so overfitting is a major concern regarding this learning method. In order to make the final model relies less on the training data, random forests create an ensemble of a large number of *decorrelated trees*, implying that the average of individual predictions can have less variance than a simple bagging, which only attenuates the variability of estimates through the average of identically distributed variables (here, predictions).\n","\n","Supposing a random forest that should have $B$ trees, each one of them will be grown under the following procedure: a bootstrap sample (indexed by $b$) with size $N$ is drawn from the entire training data (sampling with replacement from all $N$ instances); a tree is then created using this bootstrap sample $b$, where for each split in the tree only a random sample of $m \\leq p$ input variables is considered. Given an input vector $x$, the final predictions is the average of predictions of individual trees (majority vote for classification task, average value for regression task and probability estimates).\n","\n","Fitting a random forest model involves defining (at least) 4 different **hyper-parameters**: minimum node size, $n$, and maximum depth, $J$, (as for a single decision tree), number of trees in the ensemble, $B$, and the number of variables for each split during the growth of each tree $m$. Given the power of this ensemble estimator, it has a relatively low number of hyper-parameters to tune (in comparison with gradient boosting, for instance). Regarding the definition of its hyper-parameters, all discussion related with $n$ and $J$ also applies here. There is some liberty when it comes to define $B$, since random forest can be quite resistant to a large collection of trees. Defining a low value for $m$, however, may be a problem if the learning problem is sparse enough (large collection of inputs $p$ with just a few relevant variables).\n","\n","The first implementation discussed here follows from this [article](https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a), which first discusses the theoretical definition of the algorithm and then presents an implementation from scratch. The approach of the author considers the creation of three classes: \"Node\", for declaring a node in a tree; \"DecisionTree\", for building a single decision tree; and \"RandomForest\", for fitting and gathering $B$ different decision trees into an ensemble of models.\n","\n","The **\"Node\" class** allows the initialization of a node by defining the following attributes: feature and value for the split, data (rows and columns) that goes to the left and right child nodes, the information gain from the split, and the value of response variable. Note that leaf nodes have null values for all attributes, except for the last one; and the opposite applies for decision nodes.\n","\n","The **\"DecisionTree\" class** is initialized by declaring the main hyper-parameters of a decision tree: minimum number of samples that follows a split and maximum tree depth. The usage of this class requires initialization and call of *fit* and *predict* methods. But under the hood several methods allows the construction and functioning of a decision tree. First, there is a function for calculating the entropy of a node (*_entropy*), which is used inside another method for calculating the information gain of a split (*_information_gain*), given by the difference between the entropy of the parent node and the weighted average of entropies from the left and right child nodes.\n","\n","A crucial method of DecisionTree is *_best_split*: it iterates over every available feature and over all possible values of them to find the pair of variable and threshold (or category for categorical inputs) that generates the best split by maximizing the information gain. This method returns a dictionary with the feature for the split, its threshold, the data that goes to the left and the right, and the information gain related with the split.\n","\n","Then, it comes to the most important method: *_build*, which keeps recursively searching for best splits until at least one of termination criteria is met (all defined by the hyper-parameters of decision trees). This search implies that nodes are constructed using the Node class in the chained manner that usually characterizes the implementation of decision trees. Given that one termination criterium is met, the node will not be further splitted, and only a value of the response variable is defined to it.\n","\n","Finally, *fit* method takes inputs and output and passes them to the _build method, which assignes the outcome of the method to the root node attribute. The *predict* method also takes inputs to return predicted values of the outcome variable, but uses the *_predict* method to walk the input vector through decision nodes until a leaf is found.\n","\n","The **RandomForest** class is initialized by declaring all hyper-parameters: minimum node size, maximum depth, and, here, only the number of trees regaring the ensemble construction. This class has three methods. First, *_sample* takes from the training data a bootstrap sample. The *fit* method sequentially creates decision trees by initializing DecisionTree class and by calling its fit method. Finally, the *predict* method iterates over the decision trees in the ensemble and predicts using them by taking the majority vote from all individual predictions.\n","\n","Some extensions are available to the implementation found here. First, it is possible to use, at each split, a random sample of $m$ variables from all $p$ available. Additionally, random forests can be evaluated or even constructed by using out-of-bag (OOB) error estimates, or any metric calculated under the OOB idea."]},{"cell_type":"markdown","metadata":{"id":"tWi-Yyretplg"},"source":["**References**\n","<br>\n","[Master Machine Learning: Random Forest From Scratch With Python](https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a).\n","<br>\n","[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)."]},{"cell_type":"markdown","metadata":{"id":"YjtnwOcFtdtn"},"source":["----------------"]},{"cell_type":"markdown","metadata":{"id":"-xp0Wq1nAb73"},"source":["This notebook first imports all relevant libraries, and then presents an implementation and its demonstration."]},{"cell_type":"markdown","metadata":{"id":"1jKqXHmYthkK"},"source":["**Summary:**\n","1. [Libraries](#libraries)<a href='#libraries'></a>.\n","2. [First implementation](#first_implementation)<a href='#first_implementation'></a>."]},{"cell_type":"markdown","metadata":{"id":"NPwnjKEUtk3U"},"source":["<a id='libraries'></a>"]},{"cell_type":"markdown","metadata":{"id":"Kd3Ii0l1tmT1"},"source":["## Libraries"]},{"cell_type":"code","metadata":{"id":"-tNt5lArtoMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638233737416,"user_tz":180,"elapsed":45497,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"999959b7-e364-4053-a005-4faac065db26"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bx0RX4AsUtKe","executionInfo":{"status":"ok","timestamp":1638233748226,"user_tz":180,"elapsed":425,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"d25344a7-6a94-4fc6-de19-f1529eb929cb"},"source":["cd \"/content/gdrive/MyDrive/Studies/tree_based/Codes\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Studies/tree_based/Codes\n"]}]},{"cell_type":"code","metadata":{"id":"xOnfCCeLtr2A"},"source":["import numpy as np\n","from collections import Counter\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojy3GLAotqii"},"source":["<a id='first_implementation'></a>"]},{"cell_type":"markdown","metadata":{"id":"bg_iUVd-twyu"},"source":["## First implementation"]},{"cell_type":"markdown","metadata":{"id":"aq_xm3Dy9_91"},"source":["This first implementation follows from this [article](https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a) ([Github](https://github.com/daradecic/BDS-articles/blob/main/014_MML_Random_Forests.ipynb) page of reference)."]},{"cell_type":"markdown","metadata":{"id":"mCg8OqJT7j35"},"source":["<a id='first_implementation_codes'></a>"]},{"cell_type":"markdown","metadata":{"id":"1p0-BGhc7qD1"},"source":["### Codes"]},{"cell_type":"markdown","metadata":{"id":"uElYjetBWpJZ"},"source":["#### Class for nodes"]},{"cell_type":"code","metadata":{"id":"_Bzht_1nU9C-"},"source":["class Node:\n","    '''\n","    Helper class which implements a single tree node.\n","    '''\n","    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n","        self.feature = feature\n","        self.threshold = threshold\n","        self.data_left = data_left\n","        self.data_right = data_right\n","        self.gain = gain\n","        self.value = value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3aFtJIAQYW0d"},"source":["#### Class for the decision tree"]},{"cell_type":"code","metadata":{"id":"FZEnAWktYRZo"},"source":["class DecisionTree:\n","    '''\n","    Class which implements a decision tree classifier algorithm.\n","    '''\n","    def __init__(self, min_samples_split=2, max_depth=5):\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        self.root = None\n","        \n","    @staticmethod\n","    def _entropy(s):\n","        '''\n","        Helper function, calculates entropy from an array of integer values.\n","        \n","        :param s: list\n","        :return: float, entropy value\n","        '''\n","        # Convert to integers to avoid runtime errors\n","        counts = np.bincount(np.array(s, dtype=np.int64))\n","        # Probabilities of each class label\n","        percentages = counts / len(s)\n","\n","        # Caclulate entropy\n","        entropy = 0\n","        for pct in percentages:\n","            if pct > 0:\n","                entropy += pct * np.log2(pct)\n","        return -entropy\n","    \n","    def _information_gain(self, parent, left_child, right_child):\n","        '''\n","        Helper function, calculates information gain from a parent and two child nodes.\n","        \n","        :param parent: list, the parent node\n","        :param left_child: list, left child of a parent\n","        :param right_child: list, right child of a parent\n","        :return: float, information gain\n","        '''\n","        num_left = len(left_child) / len(parent)\n","        num_right = len(right_child) / len(parent)\n","        \n","        # One-liner which implements the previously discussed formula\n","        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n","    \n","    def _best_split(self, X, y):\n","        '''\n","        Helper function, calculates the best split for given features and target\n","        \n","        :param X: np.array, features\n","        :param y: np.array or list, target\n","        :return: dict\n","        '''\n","        best_split = {}\n","        best_info_gain = -1\n","        n_rows, n_cols = X.shape\n","        \n","        # For every dataset feature\n","        for f_idx in range(n_cols):\n","            X_curr = X[:, f_idx]\n","            # For every unique value of that feature\n","            for threshold in np.unique(X_curr):\n","                # Construct a dataset and split it to the left and right parts\n","                # Left part includes records lower or equal to the threshold\n","                # Right part includes records higher than the threshold\n","                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n","                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n","                df_right = np.array([row for row in df if row[f_idx] > threshold])\n","\n","                # Do the calculation only if there's data in both subsets\n","                if len(df_left) > 0 and len(df_right) > 0:\n","                    # Obtain the value of the target variable for subsets\n","                    y = df[:, -1]\n","                    y_left = df_left[:, -1]\n","                    y_right = df_right[:, -1]\n","\n","                    # Caclulate the information gain and save the split parameters\n","                    # if the current split is better than the previous best\n","                    gain = self._information_gain(y, y_left, y_right)\n","                    if gain > best_info_gain:\n","                        best_split = {\n","                            'feature_index': f_idx,\n","                            'threshold': threshold,\n","                            'df_left': df_left,\n","                            'df_right': df_right,\n","                            'gain': gain\n","                        }\n","                        best_info_gain = gain\n","        return best_split\n","    \n","    def _build(self, X, y, depth=0):\n","        '''\n","        Helper recursive function, used to build a decision tree from the input data.\n","        \n","        :param X: np.array, features\n","        :param y: np.array or list, target\n","        :param depth: current depth of a tree, used as a stopping criteria\n","        :return: Node\n","        '''\n","        n_rows, n_cols = X.shape\n","        \n","        # Check to see if a node should be leaf node\n","        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n","            # Get the best split\n","            best = self._best_split(X, y)\n","            # If the split isn't pure\n","            if best['gain'] > 0:\n","                # Build a tree on the left\n","                left = self._build(\n","                    X=best['df_left'][:, :-1], \n","                    y=best['df_left'][:, -1], \n","                    depth=depth + 1\n","                )\n","                right = self._build(\n","                    X=best['df_right'][:, :-1], \n","                    y=best['df_right'][:, -1], \n","                    depth=depth + 1\n","                )\n","                return Node(\n","                    feature=best['feature_index'], \n","                    threshold=best['threshold'], \n","                    data_left=left, \n","                    data_right=right, \n","                    gain=best['gain']\n","                )\n","        # Leaf node - value is the most common target value \n","        return Node(\n","            value=Counter(y).most_common(1)[0][0]\n","        )\n","    \n","    def fit(self, X, y):\n","        '''\n","        Function used to train a decision tree classifier model.\n","        \n","        :param X: np.array, features\n","        :param y: np.array or list, target\n","        :return: None\n","        '''\n","        # Call a recursive function to build the tree\n","        self.root = self._build(X, y)\n","        \n","    def _predict(self, x, tree):\n","        '''\n","        Helper recursive function, used to predict a single instance (tree traversal).\n","        \n","        :param x: single observation\n","        :param tree: built tree\n","        :return: float, predicted class\n","        '''\n","        # Leaf node\n","        if tree.value != None:\n","            return tree.value\n","        feature_value = x[tree.feature]\n","        \n","        # Go to the left\n","        if feature_value <= tree.threshold:\n","            return self._predict(x=x, tree=tree.data_left)\n","        \n","        # Go to the right\n","        if feature_value > tree.threshold:\n","            return self._predict(x=x, tree=tree.data_right)\n","        \n","    def predict(self, X):\n","        '''\n","        Function used to classify new instances.\n","        \n","        :param X: np.array, features\n","        :return: np.array, predicted classes\n","        '''\n","        # Call the _predict() function for every observation\n","        return [self._predict(x, self.root) for x in X]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZ2MmjzbcU5Z"},"source":["#### Class for the random forest"]},{"cell_type":"code","metadata":{"id":"49i0QopqceEO"},"source":["class RandomForest:\n","    '''\n","    A class that implements Random Forest algorithm from scratch.\n","    '''\n","    def __init__(self, num_trees=25, min_samples_split=2, max_depth=5):\n","        self.num_trees = num_trees\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        # Will store individually trained decision trees\n","        self.decision_trees = []\n","        \n","    @staticmethod\n","    def _sample(X, y):\n","        '''\n","        Helper function used for boostrap sampling.\n","        \n","        :param X: np.array, features\n","        :param y: np.array, target\n","        :return: tuple (sample of features, sample of target)\n","        '''\n","        n_rows, n_cols = X.shape\n","        # Sample with replacement\n","        samples = np.random.choice(a=n_rows, size=n_rows, replace=True)\n","        return X[samples], y[samples]\n","        \n","    def fit(self, X, y):\n","        '''\n","        Trains a Random Forest classifier.\n","        \n","        :param X: np.array, features\n","        :param y: np.array, target\n","        :return: None\n","        '''\n","        # Reset\n","        if len(self.decision_trees) > 0:\n","            self.decision_trees = []\n","            \n","        # Build each tree of the forest\n","        num_built = 0\n","        while num_built < self.num_trees:\n","            try:\n","                clf = DecisionTree(\n","                    min_samples_split=self.min_samples_split,\n","                    max_depth=self.max_depth\n","                )\n","                # Obtain data sample\n","                _X, _y = self._sample(X, y)\n","                # Train\n","                clf.fit(_X, _y)\n","                # Save the classifier\n","                self.decision_trees.append(clf)\n","                num_built += 1\n","            except Exception as e:\n","                continue\n","    \n","    def predict(self, X):\n","        '''\n","        Predicts class labels for new data instances.\n","        \n","        :param X: np.array, new instances to predict\n","        :return: \n","        '''\n","        # Make predictions with every tree in the forest\n","        y = []\n","        for tree in self.decision_trees:\n","            y.append(tree.predict(X))\n","        \n","        # Reshape so we can find the most common value\n","        y = np.swapaxes(a=y, axis1=0, axis2=1)\n","        \n","        # Use majority voting for the final prediction\n","        predictions = []\n","        for preds in y:\n","            counter = Counter(preds)\n","            predictions.append(counter.most_common(1)[0][0])\n","        return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OovjVbH8fXd4"},"source":["<a id='first_implementation_codes'></a>"]},{"cell_type":"markdown","metadata":{"id":"9GOPvSJxfck_"},"source":["### Demonstration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MWGiivLfYNS","executionInfo":{"status":"ok","timestamp":1638236562262,"user_tz":180,"elapsed":2601,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"cba6d8a9-d2e3-4953-cfcb-b57a1e7874d0"},"source":["# Loading the data and definint input and outcome variables:\n","iris = load_iris()\n","X = iris['data']\n","y = iris['target']\n","\n","# Train-test split:\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Creating the model object:\n","model = RandomForest()\n","\n","# Training the model:\n","model.fit(X_train, y_train)\n","\n","# Generating predictions:\n","preds = model.predict(X_test)\n","\n","# Accuracy evaluated on test data:\n","print(f'Accuracy: {accuracy_score(y_test, preds)}.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0.\n"]}]},{"cell_type":"markdown","metadata":{"id":"Oof6lCGGg0mS"},"source":["#### Comparing performance with Scikit-learn implementation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E71LxWv9ZTGn","executionInfo":{"status":"ok","timestamp":1638236733214,"user_tz":180,"elapsed":1498,"user":{"displayName":"Matheus Rosso","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07497572953789637511"}},"outputId":"bdcacb16-1908-4044-d5b3-98f4765dc143"},"source":["# Creating the model object:\n","sk_model = RandomForestClassifier()\n","\n","# Training the model:\n","sk_model.fit(X_train, y_train)\n","\n","# Generating predictions:\n","sk_preds = sk_model.predict(X_test)\n","\n","# Accuracy evaluated on test data:\n","print(f'Accuracy: {accuracy_score(y_test, sk_preds)}.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 1.0.\n"]}]},{"cell_type":"code","metadata":{"id":"asJLYJG7mXRw"},"source":[""],"execution_count":null,"outputs":[]}]}